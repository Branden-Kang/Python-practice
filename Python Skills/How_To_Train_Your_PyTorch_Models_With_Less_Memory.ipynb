{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtH/fGOzuCCYE8xjqzrkcJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Referencce](https://levelup.gitconnected.com/how-to-optimize-memory-usage-for-training-llms-in-pytorch-b012f3008798)"
      ],
      "metadata": {
        "id": "m3L2gfQFwTyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Automatic Mixed-Precision Training"
      ],
      "metadata": {
        "id": "S_ToBWPMwabp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QDhOlyk4u83F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Assume your model and optimizer have been defined elsewhere.\n",
        "model = MyModel().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scaler = GradScaler()\n",
        "\n",
        "for data, target in data_loader:\n",
        "    optimizer.zero_grad()\n",
        "    # Enable mixed precision\n",
        "    with autocast():\n",
        "        output = model(data)\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "    # Scale the loss and backpropagate\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Lower-Precision Training"
      ],
      "metadata": {
        "id": "MICjKc1ewikJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_bf16_supported())  # should print True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXGf4EtKwhIr",
        "outputId": "fea377d3-c120-41e1-ec2b-eb87f3453dfa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Gradient Check-Pointing"
      ],
      "metadata": {
        "id": "feg1E7Z1wn14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "def checkpointed_segment(input_tensor):\n",
        "    # This function represents a portion of your model\n",
        "    # which will be recomputed during the backward pass.\n",
        "    # You can create a custom forward pass for this segment.\n",
        "    return model_segment(input_tensor)\n",
        "\n",
        "# Instead of a conventional forward pass, wrap the segment with checkpoint.\n",
        "output = checkpoint(checkpointed_segment, input_tensor)"
      ],
      "metadata": {
        "id": "F78yVApqwlNE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Tensor Sharding and Distributed Training"
      ],
      "metadata": {
        "id": "12_uqaecwxv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
        "\n",
        "# Initialize your model and ensure it is on the correct device.\n",
        "model = MyLargeModel().cuda()\n",
        "\n",
        "# Wrap the model in FSDP for sharded training across GPUs.\n",
        "fsdp_model = FSDP(model)"
      ],
      "metadata": {
        "id": "Qy7VZ48Swt2O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Efficient Data Loading"
      ],
      "metadata": {
        "id": "akJ0wrnxw89v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create your dataset instance and then the DataLoader with pinned memory enabled.\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=4,      # Adjust based on your CPU capabilities\n",
        "    pin_memory=True     # Enables faster host-to-device transfers\n",
        ")"
      ],
      "metadata": {
        "id": "P06s7s59w7IQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Use In-Place Operations"
      ],
      "metadata": {
        "id": "VFL03KxgxBzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.randn(100, 100, device='cuda')\n",
        "y = torch.randn(100, 100, device='cuda')\n",
        "\n",
        "# Using in-place addition\n",
        "x.add_(y)  # Here x is modified directly instead of creating a new tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2PgOKwAw_gu",
        "outputId": "8da41eae-2e8e-4516-834d-5a99adcdc179"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-3.3012,  0.4687,  1.3011,  ...,  0.4248, -0.6974,  0.3895],\n",
              "        [-0.1602, -1.4334, -0.1336,  ..., -1.0948, -1.0826,  1.3182],\n",
              "        [ 1.1904,  2.6363,  0.8543,  ...,  1.7735,  0.4995,  1.3008],\n",
              "        ...,\n",
              "        [-2.7956, -2.2139, -1.1450,  ...,  0.8583, -0.9636,  0.0385],\n",
              "        [ 1.0686,  1.5596, -0.9039,  ..., -0.6360, -1.5732,  2.1149],\n",
              "        [ 0.3814,  1.7975,  2.0598,  ...,  0.2565, -0.1484, -2.2323]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Activation and Parameter Offloading"
      ],
      "metadata": {
        "id": "ZSpn_7EExFKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def offload_activation(tensor):\n",
        "    # Move tensor to CPU to save GPU memory\n",
        "    return tensor.cpu()\n",
        "\n",
        "def process_batch(data):\n",
        "    # Offload some activations explicitly\n",
        "    intermediate = model.layer1(data)\n",
        "    intermediate = offload_activation(intermediate)\n",
        "    intermediate = intermediate.cuda()  # Move back when needed\n",
        "    output = model.layer2(intermediate)\n",
        "    return output"
      ],
      "metadata": {
        "id": "GGsZl4mtxDnw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Using a Leaner Optimizer"
      ],
      "metadata": {
        "id": "-w-OZN5exIfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instead of this\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "\n",
        "# use this\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "num_steps = NUM_EPOCHS * len(train_loader)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, T_max=num_steps)"
      ],
      "metadata": {
        "id": "s80TATDsxG14"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Beyond the Basics"
      ],
      "metadata": {
        "id": "F6bZYxuSxOMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1) Memory Profiling and Cache Management"
      ],
      "metadata": {
        "id": "XkHDAO-txQpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# print a detailed report of current GPU memory usage and fragmentation\n",
        "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "\n",
        "# free up cached memory thatâ€™s no longer needed by PyTorch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulCUDnCvxKZ5",
        "outputId": "9583df3f-2bdd-4dda-942a-0482070f750c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  80896 B   |  84480 B   |  96256 B   |  15360 B   |\n",
            "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from small pool |  80896 B   |  84480 B   |  96256 B   |  15360 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  80896 B   |  84480 B   |  96256 B   |  15360 B   |\n",
            "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from small pool |  80896 B   |  84480 B   |  96256 B   |  15360 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  80000 B   |  81239 B   |  82282 B   |   2282 B   |\n",
            "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from small pool |  80000 B   |  81239 B   |  82282 B   |   2282 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |\n",
            "|       from large pool |      0 KiB |      0 KiB |      0 KiB |      0 B   |\n",
            "|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1969 KiB |   2047 KiB |   4070 KiB |   2101 KiB |\n",
            "|       from large pool |      0 KiB |      0 KiB |      0 KiB |      0 KiB |\n",
            "|       from small pool |   1969 KiB |   2047 KiB |   4070 KiB |   2101 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |       2    |       9    |      31    |      29    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       2    |       9    |      31    |      29    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |       2    |       9    |      31    |      29    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       2    |       9    |      31    |      29    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       1    |       1    |       1    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       1    |       1    |       1    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       1    |       4    |      10    |       9    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       1    |       4    |      10    |       9    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2) JIT Compilation with TorchScript"
      ],
      "metadata": {
        "id": "SSazCKVpxVAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Suppose `model` is an instance of your PyTorch network.\n",
        "scripted_model = torch.jit.script(model)\n",
        "\n",
        "# Now, you can run the scripted model just like before.\n",
        "output = scripted_model(input_tensor)"
      ],
      "metadata": {
        "id": "g0lyl074xS8H"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}
