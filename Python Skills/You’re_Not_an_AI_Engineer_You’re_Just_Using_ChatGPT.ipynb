{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIRO9OUigQFRp0sfGM385D"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://pub.towardsai.net/youre-not-an-ai-engineer-you-re-just-using-chatgpt-7044f58ba4fb)"
      ],
      "metadata": {
        "id": "X9YbNWyj8zNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer 1: You Use APIs, They Build Them"
      ],
      "metadata": {
        "id": "QPgIzm5085AC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9lq-DMsg8wBV"
      },
      "outputs": [],
      "source": [
        "# AI User:\n",
        "import openai\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Write code\"}]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AI Engineer:\n",
        "# Design custom architecture\n",
        "class CustomTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer(src, src_mask)\n",
        "        return output\n",
        "\n",
        "# Implement custom training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch.src, batch.src_mask)\n",
        "        loss = criterion(output, batch.tgt)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step()\n",
        "\n",
        "    # Validation and checkpointing\n",
        "    val_loss = evaluate(model, val_loader)\n",
        "    if val_loss < best_val_loss:\n",
        "        save_checkpoint(model, optimizer, epoch)# Implement custom training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch.src, batch.src_mask)\n",
        "        loss = criterion(output, batch.tgt)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step()\n",
        "\n",
        "    # Validation and checkpointing\n",
        "    val_loss = evaluate(model, val_loader)\n",
        "    if val_loss < best_val_loss:\n",
        "        save_checkpoint(model, optimizer, epoch)"
      ],
      "metadata": {
        "id": "9J2b_Q_q87n3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer 2: You Prompt, They Train\n",
        "\n",
        "AI User perspective:\n",
        "- “I’ll just describe what I want and AI does it!”\n",
        "- Treats AI like a magic box\n",
        "- No understanding of how it works internally\n",
        "\n",
        "AI Engineer reality:\n",
        "- Curates and cleans training datasets (months of work)\n",
        "- Selects model architecture based on task requirements\n",
        "- Implements custom loss functions for specific domains\n",
        "- Tunes hyperparameters across thousands of experiments\n",
        "- Handles class imbalance, data augmentation, regularization\n",
        "- Debugs why validation loss plateaus at epoch 47\n",
        "- Implements distributed training across multiple GPUs"
      ],
      "metadata": {
        "id": "eVGnXhC89C9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer 3: You See Magic, They See Mathematics\n",
        "When you use ChatGPT:\n",
        "- “Wow, it just knows things!”\n",
        "\n",
        "What AI engineers see:\n",
        "- Transformer architecture with multi-head attention\n",
        "- Softmax probability distributions over vocabulary\n",
        "Context window limitations (token limits)\n",
        "- Temperature sampling affecting output randomness\n",
        "- Beam search vs greedy decoding trade-offs\n",
        "- Attention mechanism computational complexity O(n²)\n",
        "- Why certain prompts trigger specific neural pathway activations"
      ],
      "metadata": {
        "id": "nPO0XvC89S7e"
      }
    }
  ]
}
