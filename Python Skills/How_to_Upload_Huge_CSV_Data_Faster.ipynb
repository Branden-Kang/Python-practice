{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How to Upload Huge CSV Data Faster.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWiRzWQI3aOPjKKQll4LG5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTfTYqUfIfgI"
      },
      "source": [
        "[Reference](https://truptishetty.medium.com/how-to-upload-huge-csv-data-faster-28f27114cb22)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqoFNt1MIYre"
      },
      "source": [
        "import multiprocessing\n",
        "import pymongo\n",
        "import pandas as pd\n",
        "import os\n",
        "'''\n",
        "Article - https://truptishetty.medium.com/how-to-upload-huge-csv-data-faster-28f27114cb22\n",
        "'''\n",
        "class CSVData:\n",
        "    def __init__(self, server_name, port, db_name, file_name):\n",
        "        self.server_name = server_name\n",
        "        self.port = port\n",
        "        self.db_name = db_name\n",
        "        self.file_name = file_name\n",
        "        self.chunk_size = 100000\n",
        "        # Change it according to the number of cores available\n",
        "        self.number_of_pool = 4\n",
        "\n",
        "    def read_csv(self):\n",
        "        if not os.path.exists(self.file_name):\n",
        "            return\n",
        "        header = [\"all\", \"headers\", \"or\", \"read\", \"from\", \"csv\"]\n",
        "        pool = multiprocessing.Pool(self.number_of_pool)\n",
        "        reader = pd.read_csv(self.file_name, names=header, chunksize=self.chunk_size, low_memory=False, skiprows=1)\n",
        "        for df in reader:\n",
        "            # Do any data cleaning or manipulation if required here\n",
        "\n",
        "            '''We will have to send the mongo details instead\n",
        "             of the connection because python doesn't share \n",
        "             variables between processes '''\n",
        "            pool.apply_async(upload_data, args=(df, header, self.server_name, self.port , self.db_name ))\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "\n",
        "def upload_data(data, header, server_name, port,  db_name):\n",
        "    try:\n",
        "        conn = pymongo.MongoClient(server_name, port)\n",
        "        db = conn[db_name]\n",
        "        db.collection.insert(data.to_dict('records'))\n",
        "    except Exception as ex:\n",
        "        print( \"Error while inserting CSV data - \", repr(ex))\n",
        "    finally:\n",
        "        conn.close()"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}