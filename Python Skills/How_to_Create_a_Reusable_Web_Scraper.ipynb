{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How to Create a Reusable Web Scraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIEqiUGTEunBOW85+yNxvZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7OXjs5xhKgE",
        "colab_type": "text"
      },
      "source": [
        "[Reference](https://medium.com/better-programming/how-to-create-a-reusable-web-scraper-5a5aa9af62d9)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuKbVkubhEdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import pickle\n",
        "from random import randint\n",
        "from urllib.parse import urlparse\n",
        "def _random_ua():\n",
        "    ua_list = [\"user agent1, user agent2\", \"user agent3\"]\n",
        "    random_num = randint(0, len(ua_list))\n",
        "    return ua_list[random_num]\n",
        "def _headers():\n",
        "    return { 'user-agent': _random_ua() }\n",
        "def _save_page(response):\n",
        "    uri = urlparse(response.url)\n",
        "    filename = uri.netloc + \".pickle\"\n",
        "    with open(filename, 'wb+') as pickle_file:            \n",
        "        pickle.dump(response, pickle_file)\n",
        "def download_page(url):\n",
        "    response = requests.get(url)\n",
        "    _save_page(request)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuWVlT-SkPAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def outer_element(page, identifyer):\n",
        "    root = page.find(*identifyer)\n",
        "    \n",
        "    if root == None:\n",
        "        raise Exception(\"Could not find root element\")\n",
        "\n",
        "    return root\n",
        "\n",
        "\n",
        "# Remove unwanted elements\n",
        "def trim_unwanted(page, identifier_list):\n",
        "    # Check if list has elements\n",
        "    if len(identifier_list) != 0:\n",
        "        for identifier in identifier_list:\n",
        "            for element in page.find_all(*identifier):\n",
        "                element.decompose()\n",
        "    return page\n",
        "\n",
        "\n",
        "# Extract text\n",
        "def get_text(page, identifier_list):\n",
        "    # Check if list has elements\n",
        "    if len(identifier_list) == 0:\n",
        "        raise Exception(\"Need text elements\")\n",
        "    page_text = []\n",
        "    \n",
        "    for identifier in identifier_list:\n",
        "        for element in page.find_all(*identifier):\n",
        "            page_text.append(element.text)\n",
        "        return page_text\n",
        "\n",
        "\n",
        "# Get page config\n",
        "def load_scrape_config():\n",
        "    '''Loads page scraping config data'''\n",
        "    return get_scrape_config()  \n",
        "\n",
        "\n",
        "# Get the scraping config for the site\n",
        "def get_site_config(url):\n",
        "    '''Get the scrape config for the site'''\n",
        "    domain = extract_domain(url)\n",
        "    config_data = load_scrape_config()\n",
        "    config = config_data.get(domain, None)\n",
        "    if config == None:\n",
        "        raise Exception(f\"Config does not exist for: {domain}\")\n",
        "    return config \n",
        "\n",
        "\n",
        "# Build Soup\n",
        "def page_processer(request):\n",
        "    '''Returns Article Text'''\n",
        "    # Get the page scrape config\n",
        "    site_config = get_site_config(request.url)     \n",
        "    \n",
        "    # Soupify page\n",
        "    soup = BeautifulSoup(request.text, 'lxml')\n",
        "    \n",
        "    # Retrieve root element    \n",
        "    root = outer_element(soup, site_config[\"root_element\"])     \n",
        "    # Remove unwanted elements\n",
        "    trimmed_tree = trim_unwanted(root, site_config[\"unwanted\"])\n",
        "    # Get the desired elements\n",
        "    text = get_text(trimmed_tree, site_config[\"text_elements\"])            \n",
        "    return \" \".join(text)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEQlEPBnkZmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if __name__==\"__main__\":\n",
        "#     # Load up test pages\n",
        "#     from util.load_test_data import get_test_pages\n",
        "    \n",
        "#     request_list = get_test_pages()\n",
        "\n",
        "#     for page in request_list:\n",
        "#         text = page_processer(page)\n",
        "#         print(text)\n",
        "#         print(\"\".join(['--'for i in range(20)]))"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}