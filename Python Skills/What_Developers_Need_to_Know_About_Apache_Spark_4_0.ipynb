{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+lBPIi5DaJlfYcyP4EjEL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@anchitgupt2012/what-developers-need-to-know-about-apache-spark-4-0-10200a9000bf)"
      ],
      "metadata": {
        "id": "bXa89Yb-PQQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VARIANT: The 8x JSON Performance Boost\n",
        "```\n",
        "CREATE TABLE events (\n",
        "  event_id STRING,\n",
        "  payload VARIANT\n",
        ") USING DELTA;\n",
        "\n",
        "-- Direct field access, no parsing overhead\n",
        "SELECT\n",
        "  payload.user.id AS user_id,\n",
        "  payload.metadata.source\n",
        "FROM events\n",
        "WHERE payload.type = 'click';\n",
        "```"
      ],
      "metadata": {
        "id": "zMNvkcxaPecw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python Data Source API"
      ],
      "metadata": {
        "id": "AdjnpZEKPkFx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fBaGUMcGPNPa"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.datasource import DataSource, DataSourceReader\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "class MyCustomSource(DataSource):\n",
        "    @classmethod\n",
        "    def name(cls):\n",
        "        return \"mycustom\"\n",
        "\n",
        "    def schema(self):\n",
        "        return StructType([\n",
        "            StructField(\"name\", StringType()),\n",
        "            StructField(\"value\", IntegerType())\n",
        "        ])\n",
        "\n",
        "    def reader(self, schema):\n",
        "        return MyCustomReader()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Native Plotting"
      ],
      "metadata": {
        "id": "YExEaw-cPpUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.range(10).withColumn(\"square\", col(\"id\") ** 2)\n",
        "df.plot.scatter(x=\"id\", y=\"square\")"
      ],
      "metadata": {
        "id": "fN9lXlZjPmjP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polymorphic UDTFs"
      ],
      "metadata": {
        "id": "8sAqWXrhPsrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FlexibleParser:\n",
        "    def eval(self, json_str: str, include_metadata: bool = False):\n",
        "        data = json.loads(json_str)\n",
        "        if include_metadata:\n",
        "            yield (data['id'], data['value'], data.get('timestamp'))\n",
        "        else:\n",
        "            yield (data['id'], data['value'])"
      ],
      "metadata": {
        "id": "pNfZG_K4Pq-u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PIPE Syntax\n",
        "```\n",
        "-- Old style: nested and hard to follow\n",
        "SELECT user_id FROM (\n",
        "  SELECT user_id, COUNT(*) AS purchases\n",
        "  FROM events\n",
        "  WHERE event_type = 'purchase'\n",
        "  GROUP BY user_id\n",
        ") t WHERE purchases > 10;\n",
        "\n",
        "-- New PIPE syntax: linear and clear\n",
        "FROM events\n",
        "WHERE event_type = 'purchase'\n",
        "|> SELECT user_id, COUNT(*) AS purchases GROUP BY user_id\n",
        "|> WHERE purchases > 10;\n",
        "````"
      ],
      "metadata": {
        "id": "UruF1HSEP3Ef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SQL UDFs\n",
        "\n",
        "```\n",
        "CREATE FUNCTION extract_domain(email STRING)\n",
        "  RETURNS STRING\n",
        "  RETURN SPLIT(email, '@')[1];\n",
        "\n",
        "-- Now use it everywhere\n",
        "SELECT extract_domain(user_email) AS domain\n",
        "FROM users;\n",
        "```"
      ],
      "metadata": {
        "id": "_dZpJuO5P7mZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SQL Scripting\n",
        "```\n",
        "DECLARE threshold INT DEFAULT 1000;\n",
        "\n",
        "CREATE TEMP VIEW filtered AS\n",
        "SELECT * FROM events WHERE count > threshold;\n",
        "INSERT INTO summary_table\n",
        "SELECT date, COUNT(*) FROM filtered GROUP BY date;\n",
        "````"
      ],
      "metadata": {
        "id": "mVSaiBQsP-MC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# String Collation\n",
        "```\n",
        "CREATE TABLE names (\n",
        "  name STRING COLLATE 'en_US.UTF8'\n",
        ");\n",
        "```"
      ],
      "metadata": {
        "id": "qp5E5cXRQA8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# State Data Source\n",
        "```\n",
        "df = spark.read.format(\"statestore\").load(\"<checkpointLocation>\")\n",
        "df.show()\n",
        "```"
      ],
      "metadata": {
        "id": "pJzXBg_WQEB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structured Logging\n",
        "```\n",
        "{\n",
        "  \"timestamp\": \"2025-05-28T12:34:56Z\",\n",
        "  \"level\": \"INFO\",\n",
        "  \"component\": \"TaskSetManager\",\n",
        "  \"message\": \"Finished task\",\n",
        "  \"taskId\": 200,\n",
        "  \"stageId\": 12,\n",
        "  \"durationMs\": 314,\n",
        "  \"host\": \"10.1.2.3\"\n",
        "}\n",
        "````"
      ],
      "metadata": {
        "id": "91GOGvJ7QIhy"
      }
    }
  ]
}
