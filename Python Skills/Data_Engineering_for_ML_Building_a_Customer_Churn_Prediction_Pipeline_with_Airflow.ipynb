{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObQDZZa6Yx2tYDpNZAk83r"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/towards-data-engineering/data-engineering-for-ml-building-a-customer-churn-prediction-pipeline-with-airflow-f6b50daf5443)"
      ],
      "metadata": {
        "id": "TG2uw3aXyKlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Data Extraction"
      ],
      "metadata": {
        "id": "pC8SS4LJyRvT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e50IB4q3yE4d"
      },
      "outputs": [],
      "source": [
        "# project_folder/scripts/extract_data.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def extract_data():\n",
        "    data = {\n",
        "        'customer_id': range(1, 31),\n",
        "        'age': [25, 45, 34, 23, 30, 28, 37, np.nan, 45, 24, 31, 29, 41, 35, 44, 27, 32, 26, 49, 36, 22, 39, 43, 21, 48, 33, 40, 38, 27, 55],\n",
        "        'location': ['NY', 'CA', 'TX', 'NY', 'CA', 'TX', 'NY', 'CA', 'TX', 'NY', 'CA', 'TX', 'NY', 'CA', 'TX', 'NY', 'CA', 'TX', 'NY', 'CA',\n",
        "                     'NY', 'CA', 'TX', 'NY', np.nan, 'TX', 'NY', 'CA', 'TX', 'NY'],\n",
        "        'last_login': ['2024-10-05', '2024-10-07', '2024-09-30', '2024-10-01', '2024-10-06',\n",
        "                       '2024-09-15', '2024-09-10', '2024-10-03', '2024-10-02', '2024-10-04',\n",
        "                       '2024-09-25', '2024-09-20', '2024-10-05', '2024-10-07', '2024-09-30',\n",
        "                       '2024-10-01', '2024-09-28', '2024-09-22', '2024-09-25', '2024-10-02',\n",
        "                       '2024-10-05', '2024-10-07', '2024-09-30', '2024-10-01', '2024-10-06',\n",
        "                       '2024-09-28', '2024-10-04', '2024-10-08', '2024-09-10', '2024-09-18'],\n",
        "        'num_logins': [10, 20, 15, 8, 5, 3, 25, 40, 18, 22, 9, 7, np.nan, 35, 12, 6, 8, 15, 21, 10,\n",
        "                       12, 17, 6, 8, 14, 9, 19, 23, 11, 18]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv('<parent_path>/project_folder/tmp/customer_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Data Cleaning & Transformation\n"
      ],
      "metadata": {
        "id": "CkGwej-qyUtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# project_folder/scripts/transform_data.py\n",
        "import pandas as pd\n",
        "\n",
        "def transform_data():\n",
        "    df = pd.read_csv('<parent_path>/project_folder/tmp/customer_data.csv')\n",
        "\n",
        "    # Data Cleaning\n",
        "    age_mean = df['age'].mean()\n",
        "    df['age'].fillna(age_mean, inplace=True)\n",
        "    df['location'].fillna('Unknown', inplace=True)\n",
        "    df['num_logins'].fillna(0, inplace=True)\n",
        "\n",
        "    # Feature Engineering\n",
        "    df['last_login'] = pd.to_datetime(df['last_login'])\n",
        "    df['days_since_login'] = (pd.to_datetime('today') - df['last_login']).dt.days\n",
        "\n",
        "    # Define churn based on more complex patterns ie. days_since_login>40  or num_logins <10\n",
        "    # This label is used for training only\n",
        "    df['churn'] = ((df['days_since_login'] > 40) | (df['num_logins'] < 10)).astype(int)\n",
        "    df.to_csv('<parent_path>/project_folder/tmp/transformed_customer_data.csv', index=False)"
      ],
      "metadata": {
        "id": "GzUZb7pmyT1y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Model Training"
      ],
      "metadata": {
        "id": "2zpyWKMiyXjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# project_folder/scripts/train_model.py\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "import joblib\n",
        "\n",
        "def train_model():\n",
        "    df = pd.read_csv('<parent_path>/project_folder/tmp/transformed_customer_data.csv')\n",
        "\n",
        "    # Features and target\n",
        "    X = df[['age', 'days_since_login', 'num_logins']]\n",
        "    y = df['churn']\n",
        "\n",
        "    # Split data in training and testing set\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "    # Train model\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Save model to pkl file\n",
        "    joblib.dump(model, '<parent_path>/project_folder/tmp/churn_model.pkl')\n",
        "\n",
        "    # Print model accuracy and f1 score\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions)\n",
        "\n",
        "    print(f\"Model Accuracy: {accuracy}\")\n",
        "    print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "UO3yFhwsyWjv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Prediction"
      ],
      "metadata": {
        "id": "poPXZAU7yazt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# project_folder/scripts/predict.py\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "def predict():\n",
        "    # Load the trained model\n",
        "    model = joblib.load('<parent_path>/project_folder/tmp/churn_model.pkl')\n",
        "\n",
        "    # New data for prediction\n",
        "    new_data = pd.DataFrame({\n",
        "        'age': [29, 52, 41, 23, 45, 30, 39, 40, 22, 50],\n",
        "        'days_since_login': [12, 45, 22, 10, 60, 35, 5, 90, 13, 20],\n",
        "        'num_logins': [18, 3, 7, 25, 1, 15, 20, 2, 17, 8]\n",
        "    })\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(new_data)\n",
        "\n",
        "\n",
        "    # Display results and save to file\n",
        "    output = pd.DataFrame({\n",
        "        'customer_id': range(31, 41),\n",
        "        'age': new_data['age'],\n",
        "        'days_since_login': new_data['days_since_login'],\n",
        "        'num_logins': new_data['num_logins'],\n",
        "        'predicted_churn': predictions  # Adding predictions as a new column\n",
        "    })\n",
        "\n",
        "    # Save predictions to a CSV file\n",
        "    output.to_csv('<parent_path>/project_folder/tmp/predicted_churn.csv', index=False)\n",
        "\n",
        "    # Optional: Print predictions for verification\n",
        "    for i, pred in enumerate(predictions):\n",
        "        print(f\"Customer {i+1} Churn Prediction: {'Churn' if pred == 1 else 'Not Churn'}\")"
      ],
      "metadata": {
        "id": "PzR9oWvuyZyK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Scheduling the Pipeline with Airflow"
      ],
      "metadata": {
        "id": "MjU26YX7yhAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# project_folder/dags/churn_prediction_dag.py\n",
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import datetime\n",
        "\n",
        "# Import functions from the scripts\n",
        "from scripts.extract_data import extract_data\n",
        "from scripts.transform_data import transform_data\n",
        "from scripts.train_model import train_model\n",
        "from scripts.predict import predict\n",
        "\n",
        "# Default DAG arguments\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'email_on_failure': True,\n",
        "    'email_on_retry': True,\n",
        "    'retries': 1,\n",
        "}\n",
        "\n",
        "# Initialize the DAG\n",
        "with DAG(\n",
        "    'customer_churn_prediction',\n",
        "    default_args=default_args,\n",
        "    schedule_interval='@daily',\n",
        "    start_date=datetime(2024, 11, 9),\n",
        "    catchup=False\n",
        ") as dag:\n",
        "    # Start node\n",
        "    start = DummyOperator(task_id='start')\n",
        "\n",
        "    # Define tasks\n",
        "    task_extract_data = PythonOperator(\n",
        "        task_id='extract_data',\n",
        "        python_callable=extract_data\n",
        "    )\n",
        "\n",
        "    task_transform_data = PythonOperator(\n",
        "        task_id='transform_data',\n",
        "        python_callable=transform_data\n",
        "    )\n",
        "\n",
        "    task_train_model = PythonOperator(\n",
        "        task_id='train_model',\n",
        "        python_callable=train_model\n",
        "    )\n",
        "\n",
        "    task_predict = PythonOperator(\n",
        "        task_id='predict',\n",
        "        python_callable=predict\n",
        "    )\n",
        "\n",
        "    # End node\n",
        "    end = DummyOperator(task_id='end')\n",
        "\n",
        "    # Set task dependencies\n",
        "    start >> task_extract_data >> task_transform_data >> task_train_model >> task_predict >> end"
      ],
      "metadata": {
        "id": "gRtC4J3Vyca6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Structure"
      ],
      "metadata": {
        "id": "_M4QM9dXyrfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "project_folder\n",
        "├── __init__.py\n",
        "├── dags\n",
        "│   └── churn_prediction_dag.py       # The main Airflow DAG\n",
        "├── scripts\n",
        "|   ├── __init__.py                   # Add an empty __init__.py file in both the project_folder and scripts directories. This will make them Python packages.\n",
        "│   ├── extract_data.py               # Script for data extraction\n",
        "│   ├── transform_data.py             # Script for data transformation\n",
        "│   ├── train_model.py                # Script for model training\n",
        "│   └── predict.py                    # Script for making predictions on new data\n",
        "└── tmp\n",
        "    ├── customer_data.csv             # Intermediate data file after extraction\n",
        "    ├── transformed_customer_data.csv # Transformed data file\n",
        "    └── churn_model.pkl               # Trained model file\n",
        "```"
      ],
      "metadata": {
        "id": "6mrlhPcwytGL"
      }
    }
  ]
}
