{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvLCOyXQT9/EmFmj1zGBW+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://mskadu.medium.com/async-programming-in-python-part-2-advanced-patterns-and-techniques-7f6b65061b74)"
      ],
      "metadata": {
        "id": "jDNVbfD4wM-r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AB--DNi5wFtp"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "async def limited_operation(semaphore, task_id):\n",
        "    \"\"\"An operation that should be limited in concurrency\"\"\"\n",
        "    async with semaphore:\n",
        "        print(f\"Task {task_id} started (semaphore acquired)\")\n",
        "        await asyncio.sleep(2)  # Simulate work\n",
        "        print(f\"Task {task_id} completed (semaphore released)\")\n",
        "        return f\"Result {task_id}\"\n",
        "\n",
        "async def demo_semaphore():\n",
        "    # Only allow 2 operations to run concurrently\n",
        "    semaphore = asyncio.Semaphore(2)\n",
        "\n",
        "    # Start 5 tasks, but only 2 will run at once\n",
        "    tasks = [\n",
        "        limited_operation(semaphore, i)\n",
        "        for i in range(5)\n",
        "    ]\n",
        "\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    return results\n",
        "\n",
        "asyncio.run(demo_semaphore())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import random\n",
        "\n",
        "async def producer(queue, producer_id):\n",
        "    \"\"\"Produces work items and puts them in the queue\"\"\"\n",
        "    for i in range(3):\n",
        "        item = f\"Item-{producer_id}-{i}\"\n",
        "        await queue.put(item)\n",
        "        print(f\"Producer {producer_id} created {item}\")\n",
        "        await asyncio.sleep(random.uniform(0.1, 0.5))\n",
        "\n",
        "async def consumer(queue, consumer_id):\n",
        "    \"\"\"Consumes work items from the queue\"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            # Wait for work, but timeout after 2 seconds\n",
        "            item = await asyncio.wait_for(queue.get(), timeout=2.0)\n",
        "            print(f\"Consumer {consumer_id} processing {item}\")\n",
        "            await asyncio.sleep(random.uniform(0.2, 0.8))  # Simulate processing\n",
        "            queue.task_done()  # Mark task as completed\n",
        "        except asyncio.TimeoutError:\n",
        "            print(f\"Consumer {consumer_id} timed out, shutting down\")\n",
        "            break\n",
        "\n",
        "async def demo_queue():\n",
        "    queue = asyncio.Queue(maxsize=5)  # Limit queue size\n",
        "\n",
        "    # Start producers and consumers\n",
        "    producers = [producer(queue, i) for i in range(2)]\n",
        "    consumers = [consumer(queue, i) for i in range(3)]\n",
        "\n",
        "    # Run everything concurrently\n",
        "    await asyncio.gather(*producers, *consumers)\n",
        "\n",
        "asyncio.run(demo_queue())"
      ],
      "metadata": {
        "id": "BBm1XyvewSwB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "# Shared resource\n",
        "counter = 0\n",
        "counter_lock = asyncio.Lock()\n",
        "\n",
        "async def increment_counter(worker_id):\n",
        "    \"\"\"Safely increment a shared counter\"\"\"\n",
        "    global counter\n",
        "\n",
        "    for i in range(3):\n",
        "        async with counter_lock:\n",
        "            # Critical section - only one coroutine can execute this\n",
        "            current = counter\n",
        "            await asyncio.sleep(0.1)  # Simulate some processing\n",
        "            counter = current + 1\n",
        "            print(f\"Worker {worker_id}: counter = {counter}\")\n",
        "\n",
        "async def demo_lock():\n",
        "    # Start multiple workers that all increment the same counter\n",
        "    workers = [increment_counter(i) for i in range(3)]\n",
        "    await asyncio.gather(*workers)\n",
        "    print(f\"Final counter value: {counter}\")\n",
        "\n",
        "asyncio.run(demo_lock())"
      ],
      "metadata": {
        "id": "K-JcT3mgwWk-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "async def unreliable_operation():\n",
        "    \"\"\"Simulate an operation that might hang\"\"\"\n",
        "    await asyncio.sleep(5)  # Takes too long\n",
        "    return \"Success\"\n",
        "\n",
        "async def with_timeout():\n",
        "    try:\n",
        "        # Don't wait more than 2 seconds\n",
        "        result = await asyncio.wait_for(unreliable_operation(), timeout=2.0)\n",
        "        return result\n",
        "    except asyncio.TimeoutError:\n",
        "        print(\"Operation timed out\")\n",
        "        return \"Timeout fallback\"\n",
        "\n",
        "async def multiple_timeouts():\n",
        "    \"\"\"Handle timeouts in concurrent operations\"\"\"\n",
        "    async def safe_operation(op_id):\n",
        "        try:\n",
        "            await asyncio.sleep(op_id)  # Different durations\n",
        "            return f\"Operation {op_id} succeeded\"\n",
        "        except asyncio.CancelledError:\n",
        "            return f\"Operation {op_id} was cancelled\"\n",
        "\n",
        "    # Wrap coroutines in tasks so we can cancel/check them\n",
        "    tasks = [asyncio.create_task(safe_operation(i)) for i in [0.5, 3, 1]]\n",
        "\n",
        "    try:\n",
        "        # All operations must complete within 2 seconds\n",
        "        results = await asyncio.wait_for(\n",
        "            asyncio.gather(*tasks),\n",
        "            timeout=2.0\n",
        "        )\n",
        "        return results\n",
        "    except asyncio.TimeoutError:\n",
        "        print(\"Some operations timed out\")\n",
        "        # Cancel remaining tasks\n",
        "        for task in tasks:\n",
        "            if not task.done():\n",
        "                task.cancel()\n",
        "        # Optionally, gather results from completed/cancelled tasks\n",
        "        results = []\n",
        "        for task in tasks:\n",
        "            try:\n",
        "                results.append(await task)\n",
        "            except asyncio.CancelledError:\n",
        "                results.append(\"Cancelled\")\n",
        "        return results\n",
        "\n",
        "print(asyncio.run(with_timeout()))\n",
        "print(asyncio.run(multiple_timeouts()))"
      ],
      "metadata": {
        "id": "XNUtw9urwZAe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "async def might_fail(task_id):\n",
        "    await asyncio.sleep(0.5)\n",
        "    if task_id == 2:\n",
        "        raise ValueError(f\"Task {task_id} failed\")\n",
        "    return f\"Task {task_id} succeeded\"\n",
        "\n",
        "async def handle_partial_failures():\n",
        "    \"\"\"Continue processing even if some operations fail\"\"\"\n",
        "    tasks = [might_fail(i) for i in range(5)]\n",
        "\n",
        "    # gather with return_exceptions=True\n",
        "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "    successes = []\n",
        "    failures = []\n",
        "\n",
        "    for i, result in enumerate(results):\n",
        "        if isinstance(result, Exception):\n",
        "            failures.append(f\"Task {i}: {result}\")\n",
        "        else:\n",
        "            successes.append(result)\n",
        "\n",
        "    print(f\"Successes: {successes}\")\n",
        "    print(f\"Failures: {failures}\")\n",
        "\n",
        "    return successes, failures\n",
        "\n",
        "asyncio.run(handle_partial_failures())"
      ],
      "metadata": {
        "id": "HvUC7tu1wccZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import random\n",
        "\n",
        "async def unreliable_api_call():\n",
        "    \"\"\"Simulate an API that fails randomly\"\"\"\n",
        "    if random.random() < 0.7:  # 70% failure rate\n",
        "        raise ConnectionError(\"API unavailable\")\n",
        "    return \"API success\"\n",
        "\n",
        "async def retry_with_backoff(max_retries=3):\n",
        "    \"\"\"Retry with exponential backoff\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            result = await unreliable_api_call()\n",
        "            print(f\"Success on attempt {attempt + 1}\")\n",
        "            return result\n",
        "        except ConnectionError as e:\n",
        "            if attempt == max_retries - 1:\n",
        "                print(f\"Failed after {max_retries} attempts\")\n",
        "                raise\n",
        "\n",
        "            wait_time = 2 ** attempt  # Exponential backoff\n",
        "            print(f\"Attempt {attempt + 1} failed, retrying in {wait_time}s...\")\n",
        "            await asyncio.sleep(wait_time)\n",
        "\n",
        "# This might take several attempts\n",
        "asyncio.run(retry_with_backoff())"
      ],
      "metadata": {
        "id": "SYxP4jcMwfT4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import concurrent.futures\n",
        "import requests  # Synchronous HTTP library\n",
        "import time\n",
        "\n",
        "def sync_http_call(url):\n",
        "    \"\"\"A blocking HTTP call using requests library\"\"\"\n",
        "    response = requests.get(url)\n",
        "    return {\n",
        "        'url': url,\n",
        "        'status': response.status_code,\n",
        "        'length': len(response.content)\n",
        "    }\n",
        "\n",
        "async def async_http_calls(urls):\n",
        "    \"\"\"Run synchronous HTTP calls in a thread pool\"\"\"\n",
        "    loop = asyncio.get_event_loop()\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        # Submit all sync operations to thread pool\n",
        "        futures = [\n",
        "            loop.run_in_executor(executor, sync_http_call, url)\n",
        "            for url in urls\n",
        "        ]\n",
        "\n",
        "        # Wait for all to complete\n",
        "        results = await asyncio.gather(*futures)\n",
        "        return results\n",
        "\n",
        "async def mixed_sync_async():\n",
        "    \"\"\"Demonstrate mixing sync and async operations\"\"\"\n",
        "    urls = [\n",
        "        'https://httpbin.org/delay/1',\n",
        "        'https://httpbin.org/delay/2',\n",
        "        'https://httpbin.org/status/200'\n",
        "    ]\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # These sync operations run concurrently in threads\n",
        "    http_results = await async_http_calls(urls)\n",
        "\n",
        "    # This is native async\n",
        "    await asyncio.sleep(0.5)\n",
        "\n",
        "    print(f\"Completed in {time.time() - start_time:.2f}s\")\n",
        "    return http_results\n",
        "\n",
        "# Run the mixed example\n",
        "results = asyncio.run(mixed_sync_async())\n",
        "for result in results:\n",
        "    print(f\"URL: {result['url']}, Status: {result['status']}\")"
      ],
      "metadata": {
        "id": "fY9iTp3CwkBy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional\n",
        "import aiohttp\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class ProcessingResult:\n",
        "    item_id: str\n",
        "    success: bool\n",
        "    data: Optional[dict] = None\n",
        "    error: Optional[str] = None\n",
        "\n",
        "class AsyncDataProcessor:\n",
        "    \"\"\"Main application class demonstrating async architecture\"\"\"\n",
        "\n",
        "    def __init__(self, max_concurrent_requests: int = 5):\n",
        "        self.semaphore = asyncio.Semaphore(max_concurrent_requests)\n",
        "        self.session: Optional[aiohttp.ClientSession] = None\n",
        "        self.results_queue = asyncio.Queue()\n",
        "\n",
        "    async def __aenter__(self):\n",
        "        \"\"\"Async context manager entry\"\"\"\n",
        "        self.session = aiohttp.ClientSession()\n",
        "        return self\n",
        "\n",
        "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"Cleanup resources\"\"\"\n",
        "        if self.session:\n",
        "            await self.session.close()\n",
        "\n",
        "    async def fetch_data(self, item_id: str) -> ProcessingResult:\n",
        "        \"\"\"Fetch data for a single item with rate limiting\"\"\"\n",
        "        async with self.semaphore:\n",
        "            try:\n",
        "                url = f\"https://jsonplaceholder.typicode.com/posts/{item_id}\"\n",
        "\n",
        "                async with self.session.get(url) as response:\n",
        "                    if response.status == 200:\n",
        "                        data = await response.json()\n",
        "                        logger.info(f\"Successfully fetched item {item_id}\")\n",
        "                        return ProcessingResult(item_id, True, data)\n",
        "                    else:\n",
        "                        error_msg = f\"HTTP {response.status}\"\n",
        "                        logger.error(f\"Failed to fetch item {item_id}: {error_msg}\")\n",
        "                        return ProcessingResult(item_id, False, error=error_msg)\n",
        "\n",
        "            except Exception as e:\n",
        "                error_msg = str(e)\n",
        "                logger.error(f\"Exception fetching item {item_id}: {error_msg}\")\n",
        "                return ProcessingResult(item_id, False, error=error_msg)\n",
        "\n",
        "    async def process_item(self, result: ProcessingResult) -> ProcessingResult:\n",
        "        \"\"\"Process fetched data\"\"\"\n",
        "        if not result.success:\n",
        "            return result\n",
        "\n",
        "        try:\n",
        "            # Simulate processing time\n",
        "            await asyncio.sleep(0.1)\n",
        "\n",
        "            # Example processing: extract title length\n",
        "            processed_data = {\n",
        "                'original_title': result.data.get('title', ''),\n",
        "                'title_length': len(result.data.get('title', '')),\n",
        "                'word_count': len(result.data.get('body', '').split()),\n",
        "                'processed_at': asyncio.get_event_loop().time()\n",
        "            }\n",
        "\n",
        "            result.data = processed_data\n",
        "            logger.info(f\"Processed item {result.item_id}\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Processing error: {str(e)}\"\n",
        "            logger.error(f\"Failed to process item {result.item_id}: {error_msg}\")\n",
        "            return ProcessingResult(result.item_id, False, error=error_msg)\n",
        "\n",
        "    async def worker(self, item_ids: List[str]):\n",
        "        \"\"\"Worker that fetches and processes items\"\"\"\n",
        "        for item_id in item_ids:\n",
        "            # Fetch data\n",
        "            fetch_result = await self.fetch_data(item_id)\n",
        "\n",
        "            # Process data\n",
        "            processed_result = await self.process_item(fetch_result)\n",
        "\n",
        "            # Store result\n",
        "            await self.results_queue.put(processed_result)\n",
        "\n",
        "    async def result_collector(self, expected_count: int) -> List[ProcessingResult]:\n",
        "        \"\"\"Collect results as they become available\"\"\"\n",
        "        results = []\n",
        "\n",
        "        while len(results) < expected_count:\n",
        "            try:\n",
        "                result = await asyncio.wait_for(\n",
        "                    self.results_queue.get(),\n",
        "                    timeout=30.0\n",
        "                )\n",
        "                results.append(result)\n",
        "                logger.info(f\"Collected result {len(results)}/{expected_count}\")\n",
        "\n",
        "            except asyncio.TimeoutError:\n",
        "                logger.error(\"Timeout waiting for results\")\n",
        "                break\n",
        "\n",
        "        return results\n",
        "\n",
        "    async def process_batch(self, item_ids: List[str],\n",
        "                          batch_size: int = 10) -> List[ProcessingResult]:\n",
        "        \"\"\"Process items in batches with concurrent workers and result collection\"\"\"\n",
        "\n",
        "        # Split items into batches\n",
        "        batches = [\n",
        "            item_ids[i:i + batch_size]\n",
        "            for i in range(0, len(item_ids), batch_size)\n",
        "        ]\n",
        "\n",
        "        # Start result collector\n",
        "        collector_task = asyncio.create_task(\n",
        "            self.result_collector(len(item_ids))\n",
        "        )\n",
        "\n",
        "        # Start worker tasks for each batch\n",
        "        worker_tasks = [\n",
        "            asyncio.create_task(self.worker(batch))\n",
        "            for batch in batches\n",
        "        ]\n",
        "\n",
        "        # Wait for all workers to complete\n",
        "        await asyncio.gather(*worker_tasks)\n",
        "\n",
        "        # Get collected results\n",
        "        results = await collector_task\n",
        "\n",
        "        return results\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Main application entry point\"\"\"\n",
        "    # Items to process\n",
        "    item_ids = [str(i) for i in range(1, 21)]  # Process items 1-20\n",
        "\n",
        "    async with AsyncDataProcessor(max_concurrent_requests=3) as processor:\n",
        "        logger.info(f\"Starting processing of {len(item_ids)} items\")\n",
        "\n",
        "        start_time = asyncio.get_event_loop().time()\n",
        "        results = await processor.process_batch(item_ids, batch_size=5)\n",
        "        end_time = asyncio.get_event_loop().time()\n",
        "\n",
        "        # Analyse results\n",
        "        successful = [r for r in results if r.success]\n",
        "        failed = [r for r in results if not r.success]\n",
        "\n",
        "        print(\"\\n=== Processing Complete ===\")\n",
        "        print(f\"Total time: {end_time - start_time:.2f}s\")\n",
        "        print(f\"Successful: {len(successful)}/{len(results)}\")\n",
        "        print(f\"Failed: {len(failed)}/{len(results)}\")\n",
        "\n",
        "        if failed:\n",
        "            print(\"\\nFailures:\")\n",
        "            for failure in failed:\n",
        "                print(f\"  {failure.item_id}: {failure.error}\")\n",
        "\n",
        "        if successful:\n",
        "            print(\"\\nSample successful result:\")\n",
        "            sample = successful[0]\n",
        "            print(f\"  Item {sample.item_id}: {sample.data}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "UcQYqkErwn3G"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}
