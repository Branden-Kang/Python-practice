{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple And Effective Data Preprocessing Toolbox.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPZsKlS9C83QigOdpdJI5ln"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H9yPaPyM37h"
      },
      "source": [
        "[Reference](https://towardsdatascience.com/simple-yet-effective-data-preprocessing-toolbox-416f551a12c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgpwbQMTM2GJ"
      },
      "source": [
        "# Binning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCYS4PTBMxxP"
      },
      "source": [
        "def fixed_width_cut(df,feature,labels=['Low','Medium','High']):\n",
        "  feature_slice, retbins = pd.cut(df[feature], len(labels) ,retbins=True, labels=labels)\n",
        "  retbins = [ '%.2f' % elem for elem in retbins ]\n",
        "  return feature_slice,retbins\n",
        "\n",
        "def quartile_cut(df,feature,labels=['Low','Medium','High']):\n",
        "  feature_slice, retbins = pd.qcut(df[feature], q=len(labels),retbins=True,labels=labels)\n",
        "  retbins = [ '%.2f' % elem for elem in retbins ]\n",
        "  return feature_slice,retbins"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnuB3DCLM8Mw"
      },
      "source": [
        "# Impute Null Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoF6KVyHM7b1"
      },
      "source": [
        "def impute_plot(df,features, strategy='median'):\n",
        "  impute_by_median = SimpleImputer(strategy=strategy)\n",
        "  cleaned_features = impute_by_median.fit_transform(df[features])\n",
        "  return cleaned_features"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XouID1B4M-JX"
      },
      "source": [
        "# Remove Outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdYdSyhDM9T1"
      },
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "def remove_outlier_using_z_score (df, column):\n",
        "  z_scores = stats.zscore(df[column])\n",
        "  abs_z_scores = np.abs(z_scores)\n",
        "  filtered_entries = (abs_z_scores < 3)\n",
        "  return df[filtered_entries]\n",
        "\n",
        "def compare_plot(df_list,x,y,subtitle,figsize=(25,10)):\n",
        "  fig, axes = plt.subplots(nrows=len(df_list),figsize=figsize)\n",
        "  fig.suptitle(subtitle, fontsize=16)\n",
        "  for i,df in enumerate(df_list):\n",
        "    sns.boxplot(x=x, y=y, data=df, ax=axes[i])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV45k3pvNB7Q"
      },
      "source": [
        "# Normalization (Feature Scaling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt5dG0YTM_xl"
      },
      "source": [
        "import sklearn.preprocessing as preproc\n",
        "\n",
        "def scale_feature(df,features,strategy='minmax'):\n",
        "  if strategy=='minmax':\n",
        "    scale = preproc.minmax_scale(df[features])\n",
        "  elif strategy=='standard':\n",
        "    scale = preproc.StandardScaler().fit_transform(df[features])\n",
        "  elif strategy == 'l2':\n",
        "    scale = preproc.normalize(df[features],axis=0)\n",
        "  return scale"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6on8QILQNFNg"
      },
      "source": [
        "# Log Transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7tT-rC7NEG9"
      },
      "source": [
        "def log_transform_feature(df,feature):\n",
        "  one_log_feature = np.log10(df[feature] + 1)\n",
        "  two_log_feature = np.log10(one_log_feature + 1)\n",
        "  return one_log_feature, two_log_feature"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n92t8C7mNI7y"
      },
      "source": [
        "# One Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuvkPuiyNGYo"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def create_onehot_encoder_df (df,feature):\n",
        "  encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "  feature_df = df[~df[feature].isnull()]\n",
        "  encoder.fit(feature_df[[feature]])\n",
        "\n",
        "  feature_lists = encoder.get_feature_names()\n",
        "  feature_encode =pd.DataFrame(encoder.transform(feature_df[[feature]]).toarray(), columns = feature_lists)\n",
        "  return pd.merge(feature_df, feature_encode ,left_index=True, right_index=True)"
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}