{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How I Redesigned over 100 ETL into ELT Data Pipelines.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOehi6N1d3A73WfObpJZlRz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jeZtbLlw9fV"
      },
      "source": [
        "[Reference](https://towardsdatascience.com/how-i-redesigned-over-100-etl-into-elt-data-pipelines-c58d3a3cb3c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kkH63LGxAv2"
      },
      "source": [
        "ETL/ELT Pipelines\n",
        "\n",
        "    ETL — Extract, Transform, Load\n",
        "    ELT — Extract, Load, Transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkXiNrXgw8e_"
      },
      "source": [
        "from airflow import models\n",
        "from airflow import DAG\n",
        "from datetime import datetime, timedelta\n",
        "from operators import DataSourceToCsv\n",
        "from operators import CsvToBigquery\n",
        "\n",
        "extract_query_source = \"\"\"Select \n",
        "                        a.user_id,\n",
        "                        b.country,\n",
        "                        a.revenue\n",
        "                        from transactions a \n",
        "                        left join users b on\n",
        "                        a.user_id = b.user_id\"\"\"\n",
        "\n",
        "default_dag_args = {\n",
        "    'start_date': datetime(2019, 5, 1, 7),\n",
        "    'email_on_failure': True,\n",
        "    'email_on_retry': True,\n",
        "    'project_id' : 'your_project_name',\n",
        "    'retries': 3,\n",
        "    'on_failure_callback': notify_email,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "with models.DAG(\n",
        "    dag_id='your_dag_name',\n",
        "    schedule_interval = timedelta(days=1),\n",
        "    catchup = True,\n",
        "    default_args=default_dag_args) as dag:\n",
        "\n",
        "# Define Tasks\n",
        "Extract_And_Transform = DataSourceToCsv.DataSourceToCsv(\n",
        "    task_id='Extract from Source',\n",
        "    table_name = 'source tablename',\n",
        "    extract_query = extract_query_source,\n",
        "    connection = 'your defined postgres db connection')\n",
        "\n",
        "Load = CsvToBigquery.CsvToBigquery(\n",
        "    task_id='Load into Destination',\n",
        "    bigquery_table_name = 'destination tablename',\n",
        "    dataset_name = 'destination dataset name',\n",
        "    write_mode = 'WRITE_TRUNCATE, WRITE_APPEND OR WRITE_EMPTY')\n",
        "\n",
        "# set dependencies and sequence\n",
        "Extract_And_Transform >> Load"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY9J5uLuxE5W"
      },
      "source": [
        "from airflow import models\n",
        "from airflow import DAG\n",
        "from datetime import datetime, timedelta\n",
        "from operators import DataSourceToCsv\n",
        "from operators import CsvToBigquery\n",
        "\n",
        "extract_query_source = \"\"\"select * from transactions\"\"\"\n",
        "\n",
        "default_dag_args = {\n",
        "    'start_date': datetime(2019, 5, 1, 7),\n",
        "    'email_on_failure': True,\n",
        "    'email_on_retry': True,\n",
        "    'project_id' : 'your_project_name',\n",
        "    'retries': 3,\n",
        "    'on_failure_callback': notify_email,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "with models.DAG(\n",
        "    dag_id='your_dag_name',\n",
        "    schedule_interval = timedelta(days=1),\n",
        "    catchup = True,\n",
        "    default_args=default_dag_args) as dag:\n",
        "\n",
        "# Define Tasks\n",
        "Extract = DataSourceToCsv.DataSourceToCsv(\n",
        "    task_id='Extract from Source',\n",
        "    table_name = 'source tablename',\n",
        "    extract_query = extract_query_source,\n",
        "    connection = 'your defined postgres db connection')\n",
        "\n",
        "Load = CsvToBigquery.CsvToBigquery(\n",
        "    task_id='Load into Destination Table',\n",
        "    bigquery_table_name = 'destination tablename',\n",
        "    dataset_name = 'destination dataset name',\n",
        "    write_mode = 'WRITE_TRUNCATE, WRITE_APPEND OR WRITE_EMPTY')\n",
        "\n",
        "# set dependencies and sequence\n",
        "Extract >> Load"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iaV3glnxHXl"
      },
      "source": [
        "from airflow import models\n",
        "from airflow import DAG\n",
        "from datetime import datetime, timedelta\n",
        "from operators import DataSourceToCsv\n",
        "from operators import CsvToBigquery\n",
        "from operators import ExternalSensor\n",
        "\n",
        "transformation_query_sample = \"\"\"Select \n",
        "                        a.user_id,\n",
        "                        b.country,\n",
        "                        a.revenue\n",
        "                        from transactions a \n",
        "                        left join users b on\n",
        "                        a.user_id = b.user_id\"\"\"\n",
        "\n",
        "default_dag_args = {\n",
        "    'start_date': datetime(2019, 5, 1, 7),\n",
        "    'email_on_failure': True,\n",
        "    'email_on_retry': True,\n",
        "    'project_id' : 'your_project_name',\n",
        "    'retries': 3,\n",
        "    'on_failure_callback': notify_email,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "with models.DAG(\n",
        "    dag_id='your_dag_name',\n",
        "    schedule_interval = timedelta(days=1),\n",
        "    catchup = True,\n",
        "    default_args=default_dag_args) as dag:\n",
        "\n",
        "# Define Tasks\n",
        "check_transactions=ExternalSensor.ExternalTaskSensor(\n",
        "        task_id='check transactions',\n",
        "        external_dag_id='transactions dag',\n",
        "        external_task_id= 'final task',\n",
        "        execution_delta = 'difference in execution times',\n",
        "        timeout = 5000)\n",
        "\n",
        "check_users=ExternalSensor.ExternalTaskSensor(\n",
        "        task_id='check users',\n",
        "        external_dag_id='users dag',\n",
        "        external_task_id= 'final task',\n",
        "        execution_delta = 'difference in execution times',\n",
        "        timeout = 5000)\n",
        "\n",
        "transform = TransformInBigquery.TransformInBigquery(\n",
        "    task_id='Transform in Bigquery',\n",
        "    transformation_query = transformation_query_sample)\n",
        "\n",
        "# set dependencies and sequence\n",
        "transform.set_upstream([check_transactions,check_users])"
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}