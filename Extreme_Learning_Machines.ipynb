{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extreme Learning Machines.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7G20Y/DX+OVUubleOyIZu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6-zt95PRTHz"
      },
      "source": [
        "[Reference](https://medium.com/datadriveninvestor/extreme-learning-machines-9c8be01f6f77)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCr4wbbWRPQR"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class ELM(object):  \n",
        "    \n",
        "    def __init__(self, inputSize, outputSize, hiddenSize):\n",
        "        \"\"\"\n",
        "        Initialize weight and bias between input layer and hidden layer\n",
        "        Parameters:\n",
        "        inputSize: int\n",
        "            The number of input layer dimensions or features in the training data\n",
        "        outputSize: int\n",
        "            The number of output layer dimensions\n",
        "        hiddenSize: int\n",
        "            The number of hidden layer dimensions        \n",
        "        \"\"\"    \n",
        "\n",
        "        self.inputSize = inputSize\n",
        "        self.outputSize = outputSize\n",
        "        self.hiddenSize = hiddenSize       \n",
        "        \n",
        "        # Initialize random weight with range [-0.5, 0.5]\n",
        "        self.weight = np.matrix(np.random.uniform(-0.5, 0.5, (self.hiddenSize, self.inputSize)))\n",
        "\n",
        "        # Initialize random bias with range [0, 1]\n",
        "        self.bias = np.matrix(np.random.uniform(0, 1, (1, self.hiddenSize)))\n",
        "        \n",
        "        self.H = 0\n",
        "        self.beta = 0\n",
        "\n",
        "    def relu(self, x):\n",
        "        \"\"\"\n",
        "        ReLu activation function\n",
        "        \n",
        "        Parameters:\n",
        "        x: array-like or matrix\n",
        "            The value that the activation output will look for\n",
        "        Returns:      \n",
        "            The results of activation using ReLu function\n",
        "        \"\"\"\n",
        "        return np.maximum(x, 0, x)\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the results of the training process using test data\n",
        "        Parameters:\n",
        "        X: array-like or matrix\n",
        "            Test data that will be used to determine output using ELM\n",
        "        Returns:\n",
        "            Predicted results or outputs from test data\n",
        "        \"\"\"\n",
        "        X = np.matrix(X)\n",
        "        y = self.relu((X * self.weight.T) + self.bias) * self.beta\n",
        "\n",
        "        return y\n",
        "\n",
        "    def train(self, X, y):\n",
        "        \"\"\"\n",
        "        Extreme Learning Machine training process\n",
        "        Parameters:\n",
        "        X: array-like or matrix\n",
        "            Training data that contains the value of each feature\n",
        "        y: array-like or matrix\n",
        "            Training data that contains the value of the target (class)\n",
        "        Returns:\n",
        "            The results of the training process   \n",
        "        \"\"\"\n",
        "\n",
        "        X = np.matrix(X)\n",
        "        y = np.matrix(y)        \n",
        "        \n",
        "        # Calculate hidden layer output matrix (Hinit)\n",
        "        self.H = (X * self.weight.T) + self.bias\n",
        "\n",
        "        # ReLu activation function\n",
        "        self.H = self.relu(self.H)\n",
        "\n",
        "        # Calculate the Moore-Penrose pseudoinverse matriks        \n",
        "        H_moore_penrose = np.linalg.inv(self.H.T * self.H) * self.H.T\n",
        "\n",
        "        # Calculate the output weight matrix beta\n",
        "        self.beta = H_moore_penrose * y\n",
        "\n",
        "        return self.H * self.beta"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}