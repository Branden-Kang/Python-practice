{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPd2X517+vCnkCwbzJQMi6J"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/mlearning-ai/object-oriented-programming-and-ml-model-development-in-python-ada4bf76529b)"
      ],
      "metadata": {
        "id": "EF10IQ_iX4J7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T3ZCezP3XL2q"
      },
      "outputs": [],
      "source": [
        "class HumanBeings():\n",
        "  \"\"\" \n",
        "  The base class for all the human beings.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, name, height, gender, contact = None):\n",
        "     self.name = name\n",
        "     self.height = height\n",
        "     self.gender = gender\n",
        "     self._contact = contact # A private attribute, please dont alter it from outside\n",
        " \n",
        "    \n",
        "\n",
        "  def say_the_word(self, word):\n",
        "    \"\"\"\n",
        "    A method for performing the action of speaking the passed word.\n",
        "    Parameters\n",
        "    ----------\n",
        "    word : str\n",
        "    Insert the word to be spoken.\n",
        "    Returns\n",
        "    --------\n",
        "    A string  \n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    return f\"{self.name} says {word}\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some OOPs concepts\n",
        "- Inheritance\n",
        "- Method Overloading\n",
        "- Abstract Classes and methods\n",
        "\n"
      ],
      "metadata": {
        "id": "2qHNqTOkYJuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC,abstractmethod\n",
        "import inspect\n",
        "\n",
        "\n",
        "\"\"\"Base class for all the predictors.\n",
        "   Since we cant create an instance of abstract class, we assume parameters (attributes) are initialzied by the child model class. \"\"\"\n",
        "\n",
        "class BasePredictor(ABC):\n",
        "   \n",
        "    @abstractmethod\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit predictor.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
        "            The input samples.\n",
        "        y : ndarray of shape (n_samples,)\n",
        "            The input target value. \n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Fitted estimator.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : ndarray of shape (n_samples,)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    \n",
        "    # copied from sklearn \n",
        "    @classmethod\n",
        "    def _get_param_names(cls):\n",
        "        \"\"\"Get parameter names for the estimator\"\"\"\n",
        "        # fetch the constructor or the original constructor before\n",
        "        # deprecation wrapping if any\n",
        "        init = getattr(cls.__init__, \"deprecated_original\", cls.__init__)\n",
        "        if init is object.__init__:\n",
        "            # No explicit constructor to introspect\n",
        "            return []\n",
        "\n",
        "        # introspect the constructor arguments to find the model parameters\n",
        "        # to represent\n",
        "        init_signature = inspect.signature(init)\n",
        "        # Consider the constructor parameters excluding 'self'\n",
        "        parameters = [\n",
        "            p\n",
        "            for p in init_signature.parameters.values()\n",
        "            if p.name != \"self\" and p.kind != p.VAR_KEYWORD\n",
        "        ]\n",
        "        for p in parameters:\n",
        "            if p.kind == p.VAR_POSITIONAL:\n",
        "                raise RuntimeError(\n",
        "                    \"scikit-learn estimators should always \"\n",
        "                    \"specify their parameters in the signature\"\n",
        "                    \" of their __init__ (no varargs).\"\n",
        "                    \" %s with constructor %s doesn't \"\n",
        "                    \" follow this convention.\" % (cls, init_signature)\n",
        "                )\n",
        "        # Extract and sort argument names excluding 'self'\n",
        "        return sorted([p.name for p in parameters])\n",
        "\n",
        "    #copied from sklearn\n",
        "    def get_params(self, deep=True):\n",
        "        \"\"\"\n",
        "        Get parameters for this estimator.\n",
        "        Parameters\n",
        "        ----------\n",
        "        deep : bool, default=True\n",
        "            If True, will return the parameters for this estimator and\n",
        "            contained subobjects that are estimators.\n",
        "        Returns\n",
        "        -------\n",
        "        params : dict\n",
        "            Parameter names mapped to their values.\n",
        "        \"\"\"\n",
        "        out = dict()\n",
        "        for key in self._get_param_names():\n",
        "            value = getattr(self, key)\n",
        "            if deep and hasattr(value, \"get_params\"):\n",
        "                deep_items = value.get_params().items()\n",
        "                out.update((key + \"__\" + k, val) for k, val in deep_items)\n",
        "            out[key] = value\n",
        "        return out\n",
        "\n",
        "    \n",
        "    def reset(self):\n",
        "       \"\"\"A method for reseting the predictor\"\"\"   \n",
        "       new = self.__class__(**self.get_params())\n",
        "       return new\n",
        "\n",
        "    \n",
        "    def load_params(self, params):\n",
        "      \"\"\"A method to load model configurations.\n",
        "      \n",
        "      Parameters\n",
        "      -----------\n",
        "      params : dict of parameters\n",
        "  \n",
        "      Returns\n",
        "      ---------\n",
        "      A new model instance with the new parameters.  \n",
        "      \"\"\"\n",
        "\n",
        "      self = self.__class__(**params)\n",
        "      print(\"params loaded\")\n",
        "      \n",
        "      return self"
      ],
      "metadata": {
        "id": "TbIcOaiuYEjE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.utils.validation import check_X_y, check_array\n",
        "\n",
        "class LinearRegression(BasePredictor):\n",
        "      def __init__(self, set_intercept = True):\n",
        "        self.set_intercept = set_intercept\n",
        "        self.intercept = 0\n",
        "          \n",
        "      \n",
        "      def fit(self,X,y):\n",
        "         X, y = check_X_y(X, y) #Checking if both X & y has correct shape and converting X, y into 2d and 1d array (even pandas dataframe gets converted into arrays)\n",
        "         if self.set_intercept == True:\n",
        "            X_ = np.c_[np.ones((X.shape[0],1)), X] #adding x0 = 1\n",
        "         else:\n",
        "           X_ = X   \n",
        "         self.beta = np.linalg.inv(X_.T.dot(X_)).dot(X_.T).dot(y)\n",
        "         if self.set_intercept == True:\n",
        "          self.coefficients = self.beta[1:]\n",
        "          self.intercept = self.beta[0]\n",
        "         else:\n",
        "            self.coefficients = self.beta\n",
        "         return self \n",
        "\n",
        "      def predict(self,X):\n",
        "        X_ = check_array(X) # Validate the input, convert into 2d numpy array\n",
        "        return X_@self.coefficients + self.intercept"
      ],
      "metadata": {
        "id": "QvNW2UCpY0H_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import BatchNormalization, Dense, Dropout\n",
        "from sklearn.utils import check_array\n",
        "\n",
        "class FFNNPredictor(BasePredictor):\n",
        "\n",
        "   \"\"\"\n",
        "   Parameters\n",
        "   -----------\n",
        "   hidden_layers : an array of integers  (Default = [])\n",
        "   Initialize the hidden layers of the neural network by passing a list of neurons per hidden layer.\n",
        "   example: \n",
        "   If hidden_layers = [10, 10 ,2], then the neural network will have 1st hidden layer with 10 neurons, second with 10 neurons and the third hidden layer with 2 neurons.\n",
        "   \n",
        "   activation: an array of integers  (Default = [])\n",
        "   Set the type of activation function for all the neurons present in each of the hidden layers.\n",
        "   example: activation = [\"relu\", \"relu\", \"relu\"] will set all the three layers to have relu activation function.\n",
        "   Note: The  size of the activation array should be same as the hidden_layers.\n",
        "   \n",
        "   dropout: float (between 0 and 1, Default = 0)\n",
        "   randomly sets input units to 0 with a frequency of dropout at each step during training time, which helps prevent overfitting.\n",
        "   The dropout layers are present in between all the subsequent layers if the model and has the same dropout rate given by dropout.\n",
        " \n",
        "   training parameters:\n",
        "   \n",
        "  \n",
        "   epochs: Integer (Default = 1)\n",
        "   Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. \n",
        "   Note that in conjunction with initial_epoch, epochs is to be understood as \"final epoch\". \n",
        "   The model is not trained for a number of iterations given by epochs, but merely until the epoch of index epochs is reached.\n",
        "   batch_size: Integer or None. \n",
        "   Number of samples per gradient update. If unspecified, batch_size will default to 32.\n",
        "   The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters. \n",
        "   Note: The final layer has only one neuron with identity activation. (For regression)\n",
        " \n",
        "   \"\"\"\n",
        "   def __init__(self, hidden_layers = [], activation = [], dropout = 0, epochs = 1, batch_size = None):\n",
        "        \n",
        "        self.activation = activation\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.model = Sequential()\n",
        "        self.dropout = dropout\n",
        "        for layer in range(len(hidden_layers)):\n",
        "            #sequentially add layers to the model\n",
        "            self.model.add(Dense(self.hidden_layers[layer], self.activation[layer], kernel_initializer = keras.initializers.glorot_uniform()))\n",
        "            self.model.add(BatchNormalization())\n",
        "            self.model.add(Dropout(self.dropout))\n",
        "        \n",
        "        #final regression layer\n",
        "        self.model.add(Dense(1))\n",
        "        # setting up the type of gradient descent\n",
        "        self.model.compile(loss = \"mse\" , optimizer=\"adam\")\n",
        "       \n",
        "        \n",
        "   def fit(self, X, y, **kwargs):\n",
        "           \n",
        "            \"\"\"\n",
        "            Fit FFNN model. \n",
        "        \n",
        "            Parameters\n",
        "            ----------\n",
        "            X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "             Training data. The column \"timestamp\" will be removed if it is found. (When X is a pandas dataframe) \n",
        "            \n",
        "            y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
        "            Target values.\"\"\"\n",
        "\n",
        "            try: #if X is a pandas object with timestamp column\n",
        "                if \"timestamp\" in X.columns:\n",
        "                    X = X.drop(\"timestamp\", axis = 1)\n",
        "            except:\n",
        "                pass\n",
        "            X = check_array(X)\n",
        "            y = check_array(y, ensure_2d = False)         \n",
        "            return self.model.fit(X, y, epochs = self.epochs, batch_size = self.batch_size, **kwargs) \n",
        "\n",
        "   def predict(self, X):\n",
        "        \n",
        "            \"\"\"\n",
        "            Parameters\n",
        "            ----------\n",
        "            X : array-like or sparse matrix, shape (n_samples, n_features)\n",
        "            \n",
        "            Returns\n",
        "            --------\n",
        "            An array of model estimates for input X.\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "            try: #if X is a pandas object with timestamp column\n",
        "                if \"timestamp\" in X.columns:\n",
        "                    X = X.drop(\"timestamp\",axis = 1)\n",
        "            except:\n",
        "                pass\n",
        "            #converting into numpy array\n",
        "            X = check_array(X) \n",
        "\n",
        "            return  self.model.predict(X)    \n",
        "   \n",
        "   def summary(self):\n",
        "       \"\"\"Once a model is \"built\", you can call its summary() method to display its contents\"\"\"\n",
        "\n",
        "       return self.model.summary()"
      ],
      "metadata": {
        "id": "rAO7dXntY6XP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "X, y = load_boston(return_X_y=True)\n",
        "print(X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfCVvYTjY_pG",
        "outputId": "a156ddce-207e-40a3-aa8f-a227aa87b2e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(506, 13)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LinearRegression()\n",
        "lr.get_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifXy-m4AZLBr",
        "outputId": "fc4fa320-440b-4d58-8741-5f8bd3c8b798"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'set_intercept': True}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5YST5e3ZOLX",
        "outputId": "64389285-0049-432b-9857-28fd8bc13fb0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.LinearRegression at 0x7fe5db2f8d30>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = lr.predict(X)\n",
        "mean_squared_error(pred, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqjRB8ekZPzV",
        "outputId": "fd8f2d34-984d-4842-c06d-739bcec523ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21.894831181729206"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression as LR\n",
        "lr_2 = LR()\n",
        "lr_2.fit(X,y)\n",
        "pred2 = lr_2.predict(X)\n",
        "mean_squared_error(pred2,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayB_3LyzZVIj",
        "outputId": "dd63e1b8-417d-472e-b438-ea3136d80bfb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21.894831181729202"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}