{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How to build a URL crawler to map a website using Python.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMeggYFEk8k3kQzGB0HGWLM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7nL-zWb5t57"
      },
      "source": [
        "[Reference](https://medium.com/swlh/how-to-build-a-url-crawler-to-map-a-website-using-python-3e7db83feb7a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A90TcAZc5sFJ"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import requests.exceptions\n",
        "from urllib.parse import urlsplit\n",
        "from urllib.parse import urlparse\n",
        "from collections import deque"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdDg9TuH57nQ"
      },
      "source": [
        "url = 'https://scrapethissite.com'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T32Qe6hZ59N0"
      },
      "source": [
        "# a queue of urls to be crawled next\n",
        "new_urls = deque([url])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXPbcI0_6J7-"
      },
      "source": [
        "# a set of urls that we have already processed \n",
        "processed_urls = set()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnusv5aG6K5K"
      },
      "source": [
        "# a set of domains inside the target website\n",
        "local_urls = set()\n",
        "\n",
        "# a set of domains outside the target website\n",
        "foreign_urls = set()\n",
        "\n",
        "# a set of broken urls\n",
        "broken_urls = set()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUfgyV4l6QkT",
        "outputId": "578ec79a-89bc-44e4-f50c-3b425cae0d70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# process urls one by one until we exhaust the queue\n",
        "while len(new_urls):\n",
        "    # move next url from the queue to the set of processed urls\n",
        "    url = new_urls.popleft()\n",
        "    processed_urls.add(url)\n",
        "    # get url's content\n",
        "    print(\"Processing %s\" % url)\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "    except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError, requests.exceptions.InvalidURL, requests.exceptions.InvalidSchema):\n",
        "        # add broken urls to it's own set, then continue\n",
        "        broken_urls.add(url)\n",
        "        continue\n",
        "    \n",
        "    # extract base url to resolve relative links\n",
        "    parts = urlsplit(url)\n",
        "    base = \"{0.netloc}\".format(parts)\n",
        "    strip_base = base.replace(\"www.\", \"\")\n",
        "    base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
        "    path = url[:url.rfind('/')+1] if '/' in parts.path else url\n",
        "\n",
        "    # create a beutiful soup for the html document\n",
        "    soup = BeautifulSoup(response.text, \"lxml\")\n",
        "\n",
        "    for link in soup.find_all('a'):\n",
        "        # extract link url from the anchor\n",
        "        anchor = link.attrs[\"href\"] if \"href\" in link.attrs else ''\n",
        "\n",
        "        if anchor.startswith('/'):\n",
        "            local_link = base_url + anchor\n",
        "            local_urls.add(local_link)\n",
        "        elif strip_base in anchor:\n",
        "            local_urls.add(anchor)\n",
        "        elif not anchor.startswith('http'):\n",
        "            local_link = path + anchor\n",
        "            local_urls.add(local_link)\n",
        "        else:\n",
        "            foreign_urls.add(anchor)\n",
        "\n",
        "        for i in local_urls:\n",
        "            if not i in new_urls and not i in processed_urls:\n",
        "                new_urls.append(i)\n",
        "\n",
        "print(processed_urls)  "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'https://scrapethissite.com'}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}