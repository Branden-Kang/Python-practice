{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reproducibility Deep Dive: Building Custom ETL Pipelines in Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP3Nqrv4CiP39df/B5O7Fne"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@josephgeorgelewis2000/building-custom-reproducible-etl-pipelines-in-python-7dbdbd32acd1)"
      ],
      "metadata": {
        "id": "txPidh3pbEDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. extract.py: Getting (new) data from the source and into the pipeline\n"
      ],
      "metadata": {
        "id": "YlOZFITIbIf-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rF480jdrat2e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "\n",
        "\n",
        "class ExtractData(object):\n",
        "    '''\n",
        "    Simple class to hold and find new extracted data.\n",
        "    '''\n",
        "    def __init__(self, path:str):\n",
        "        '''\n",
        "        Initialise with a path to a data set\n",
        "        :param path: Path to the dataset looking to load in\n",
        "        '''\n",
        "        self.path = path \n",
        "\n",
        "        self.data = None\n",
        "\n",
        "    \n",
        "    def load_data(self):\n",
        "        '''\n",
        "        Load in the new data \n",
        "        '''\n",
        "        try:\n",
        "            data = pd.read_csv(self.path)\n",
        "        except UnicodeDecodeError:\n",
        "            print('There has been an encoding error, please check the file you are loading is in \\\n",
        "                UTF-8 encoding.')\n",
        "        \n",
        "        self.data = data\n",
        "\n",
        "\n",
        "    def rationalise_data(self):\n",
        "        '''\n",
        "        Rationalise an existing and new dataset to check for changes and only append new records\n",
        "        '''\n",
        "        \n",
        "        # read in existing data set:\n",
        "        master = pd.read_csv('data/master.csv')\n",
        "\n",
        "        # select only records that do not have their case no. in master \n",
        "        self.data = self.data[~self.data.CaseNumber.isin(master['CaseNumber'])]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. transform.py: Perform checks and process data ready for the end of the pipeline"
      ],
      "metadata": {
        "id": "irhOygozbNtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dateutil\n",
        "import re \n",
        "import pandas as pd\n",
        "\n",
        "class TransformData(object):\n",
        "    '''\n",
        "    Transform the shark attacks dataset\n",
        "    '''\n",
        "    def __init__(self, data:pd.DataFrame, logging_path:str):\n",
        "        '''\n",
        "        Initialise with a dataset to transform and a path to build an error log\n",
        "        :param data: Data to transform \n",
        "        :param logging_path: Path to store log messages\n",
        "        '''\n",
        "\n",
        "        self.data = data\n",
        "        self.logging_path = logging_path\n",
        "        self.messages = {}\n",
        "        \n",
        "    \n",
        "    def check_duplicates(self, remove:bool=False):\n",
        "        '''\n",
        "        Check for duplicates \n",
        "        :param remove: Boolean set True if the dupes should be removed \n",
        "        '''\n",
        "\n",
        "        # find num duplicates\n",
        "        dupes = len(self.data) - len(self.data.drop_duplicates())\n",
        "        if dupes > 0:\n",
        "            self.messages.update({'Duplicates':dupes})\n",
        "\n",
        "        if remove:\n",
        "            self.data.drop_duplicates(inplace=True)\n",
        "\n",
        "        \n",
        "    def check_row_na(self, remove:bool=False):\n",
        "        '''\n",
        "        Check for NA values \n",
        "        \n",
        "        :param remove: Boolean set True if the NA rows should be removed \n",
        "        '''\n",
        "\n",
        "        # first drop index\n",
        "        self.data.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "\n",
        "        # find number of blank rows \n",
        "        na = len(self.data) - len(self.data.dropna(how='all'))\n",
        "        if na > 0:\n",
        "            self.messages.update({'NA': na})\n",
        "\n",
        "        if remove:\n",
        "            self.data.dropna(how='all', inplace=True)\n",
        "\n",
        "    \n",
        "    def fill_na_vals(self, to_fill:str, method:str=None, val:object=None):\n",
        "        '''\n",
        "        Fill NA values with either a specified fill value or a method to fill with\n",
        "        :param to_fill: Column to fill na values in \n",
        "        :param method: Method to use to fill values \n",
        "        :param val: Alternatively a specific value can be used to fill NA \n",
        "        '''\n",
        "\n",
        "        # find number of null records \n",
        "        na = self.data[to_fill].isna().sum()\n",
        "\n",
        "        self.messages.update({'FilledNA': na})\n",
        "\n",
        "        # error handle bad methods\n",
        "        if method and method not in ('backfill', 'bfill', 'pad', 'ffill', 'mean'):\n",
        "            self.messages.append({'FillNA': f'Failed on filling {to_fill}. Please use valid method.'})\n",
        "            return\n",
        "\n",
        "        # choose a method or val to fill \n",
        "        if method:\n",
        "            self.data[to_fill] = self.data[to_fill].fillna(method=method, inplace=True)\n",
        "\n",
        "        if val:\n",
        "            values = {to_fill: val}\n",
        "            self.data.fillna(value=values, inplace=True)\n",
        "\n",
        "\n",
        "    def check_type(self, data_types:dict, change:bool=False):\n",
        "        '''\n",
        "        Take a dict of col names and their data type and check that they match expected \n",
        "        :param data_types: dictionary of col names and expected dtypes \n",
        "        :param change: Boolean true if you want to change them to the datatypes listed (could cause errors)\n",
        "        '''\n",
        "\n",
        "        # loop through checking the data types \n",
        "        for col, data_type in data_types.items():\n",
        "            if self.data.dtypes[col] != data_type:\n",
        "                self.messages.update({f'DTypes {col}': 'Incorrect datatype in col'})\n",
        "\n",
        "            if change:\n",
        "                try:\n",
        "                    self.data[col] = self.data.astype({col: data_type})\n",
        "                except:\n",
        "                    self.messages.update({f'DTypes {col}': 'Error in converting col please check this is a valid conversion'})\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_date_col(date_to_infer:str):\n",
        "        '''\n",
        "        Clean a date column (strip out text and infer date type then set date object)\n",
        "        :param date_to_infer: The date to infer \n",
        "        '''\n",
        "        \n",
        "        # handle none \n",
        "        if pd.isna(date_to_infer):\n",
        "            return 'Unknown'\n",
        "        \n",
        "        # strip out text before date and after date\n",
        "        date_to_infer = re.sub('^([^0-9])*', '', date_to_infer)\n",
        "        date_to_infer = re.sub('[A-z]*$', '', date_to_infer)\n",
        "        date_to_infer = re.sub('--', '-', date_to_infer)\n",
        "        \n",
        "        # parse the date and return \n",
        "        try:\n",
        "            date = dateutil.parser.parse(date_to_infer)\n",
        "        except dateutil.parser.ParserError:\n",
        "            return 'Unknown'\n",
        "\n",
        "        return date\n",
        "    \n",
        "\n",
        "    def bespoke_format_columns(self):\n",
        "        '''\n",
        "        A bespoke function added to extend functionality for specific formatting of Shark Attacks data\n",
        "        '''\n",
        "        \n",
        "        # clean up date column \n",
        "        self.data['Date'] = self.data.apply(lambda row: self.clean_date_col(row['Date']), axis=1)\n",
        "\n",
        "    \n",
        "    def complete_transform(self):\n",
        "        '''\n",
        "        Complete the transformation (return df and save error log)\n",
        "        '''\n",
        "\n",
        "        # build messages df \n",
        "        messages = pd.DataFrame(self.messages, index=[0])\n",
        "\n",
        "        # output to JSON \n",
        "        messages.to_json(self.logging_path)\n",
        "\n",
        "        return self.data"
      ],
      "metadata": {
        "id": "o6p5WI4ZbTQp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. load.py: Loading data into a format ready for the analysis\n"
      ],
      "metadata": {
        "id": "aYXdDhF-bWdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "class LoadData(object):\n",
        "    '''\n",
        "    Complete the ETL process by loading data into a specified format based on function call\n",
        "    '''\n",
        "\n",
        "    def __init__(self, data:pd.DataFrame):\n",
        "        '''\n",
        "        Initialise with a dataframe to load\n",
        "        :param data: the dataframe to load \n",
        "        '''\n",
        "\n",
        "        self.data = data\n",
        "\n",
        "    \n",
        "    def load_csv(self, path:str):\n",
        "        '''\n",
        "        Load to a csv file \n",
        "        :param path: Path to the csv file to load data into\n",
        "        '''\n",
        "\n",
        "        self.data.to_csv(path)\n",
        "\n",
        "    \n",
        "    def load_excel(self, path):\n",
        "        '''\n",
        "        Load to an xlsx file \n",
        "        :param path: Path to the xlsx file to load data into \n",
        "        '''\n",
        "\n",
        "        self.data.to_excel(path)\n",
        "\n",
        "\n",
        "    def load_json(self, path):\n",
        "        '''\n",
        "        Load to an json file \n",
        "        :param path: Path to the json file to load data into \n",
        "        '''\n",
        "\n",
        "        self.data.to_json(path)\n",
        "\n",
        "\n",
        "    def load_pickle(self, path):\n",
        "        '''\n",
        "        Load to an pkl file \n",
        "        :param path: Path to the pkl file to load data into \n",
        "        '''\n",
        "\n",
        "        self.data.to_pickle(path)"
      ],
      "metadata": {
        "id": "wjXlsogbbVIA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. main.py: Don’t you just love when a good pipeline comes together?\n"
      ],
      "metadata": {
        "id": "3HBBlzsTbdKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--filename', '-f', help='The filename or path to process')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # parse args \n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # run processing pipeline\n",
        "    extractor = ExtractData(args.filename)\n",
        "    extractor.load_data()\n",
        "\n",
        "    # handle no rationalisation\n",
        "    try:\n",
        "        extractor.rationalise_data()\n",
        "    except FileNotFoundError:\n",
        "        print('No master file found so data has not been rationalised and is instead in a new file named master.csv')\n",
        "        pass\n",
        "\n",
        "    df = extractor.data\n",
        "    \n",
        "    transformer = TransformData(df, 'logging/log.json')\n",
        "    transformer.check_duplicates(True)\n",
        "    transformer.check_row_na(True)\n",
        "    transformer.check_type({'Year':'float64'}, change=True)\n",
        "    transformer.fill_na_vals('Area', val='Unknown')\n",
        "    transformer.fill_na_vals('Injury', '')\n",
        "    transformer.bespoke_format_columns()\n",
        "\n",
        "    df = transformer.complete_transform()\n",
        "\n",
        "    loader = LoadData(df)\n",
        "    loader.load_csv('data/master.csv')"
      ],
      "metadata": {
        "id": "AQOqluMHbbYi"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}