{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtGaKB6qLOt95fFzLhtxAD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://pub.towardsai.net/5-paradoxes-in-statistics-every-data-scientist-should-be-familiar-with-478b74310099)"
      ],
      "metadata": {
        "id": "YxKdCx16h9Vt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy Paradox"
      ],
      "metadata": {
        "id": "ixtnjDeVh_sE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO6Hn7iYhjFH",
        "outputId": "7624ec64-8caa-425e-e85f-d2f48ac4d4b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# create imbalanced dataset\n",
        "y_true = np.array([0] * 900 + [1] * 100)\n",
        "y_pred = np.zeros(1000)\n",
        "\n",
        "# calculate accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# False Positive Paradox"
      ],
      "metadata": {
        "id": "m5KrwRfRiDqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define variables\n",
        "normal_count = 9999\n",
        "fraud_count = 1\n",
        "false_positives = 499.95\n",
        "false_negatives = 0\n",
        "\n",
        "# Calculate precision\n",
        "precision = fraud_count / (fraud_count + false_positives)\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "\n",
        "# Calculate recall\n",
        "recall = fraud_count / (fraud_count + false_negatives)\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "\n",
        "# Calculate accuracy\n",
        "true_negatives = normal_count - false_positives\n",
        "accuracy = (true_negatives + fraud_count) / (normal_count + fraud_count)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddiVmQ-ziBXi",
        "outputId": "6491a48c-2bf1-44fa-dd08-77b7e532d626"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.00\n",
            "Recall: 1.00\n",
            "Accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# generate a binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=42)\n",
        "\n",
        "# split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# train a logistic regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predict on test set and get the confusion matrix\n",
        "y_pred = model.predict(X_test)\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "# calculate the accuracy, precision, and recall\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IDfecndiGAN",
        "outputId": "0cabf752-41a3-4019-ff23-9602a748e699"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.79\n",
            "Precision: 0.82\n",
            "Recall: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gambler’s Fallacy"
      ],
      "metadata": {
        "id": "-ipElvliiKV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simulate flipping a coin 10 times\n",
        "results = np.random.randint(0, 2, size=10)\n",
        "print(f\"Coin flips: {results}\")\n",
        "\n",
        "# Count the number of consecutive heads or tails\n",
        "consecutive = 0\n",
        "for i in range(1, len(results)):\n",
        "    if results[i] == results[i-1]:\n",
        "        consecutive += 1\n",
        "    else:\n",
        "        consecutive = 0\n",
        "\n",
        "# Print the result\n",
        "if consecutive > 0:\n",
        "    print(f\"Number of consecutive flips: {consecutive + 1}\")\n",
        "else:\n",
        "    print(\"No consecutive flips\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9sjfAO-iJW9",
        "outputId": "151f5169-e555-4357-cbd3-563ce8d588b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coin flips: [1 0 0 0 0 0 0 0 0 1]\n",
            "No consecutive flips\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simpson’s Paradox"
      ],
      "metadata": {
        "id": "fB8fgamKiNGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a dataframe\n",
        "df = pd.DataFrame({'Department': ['A', 'A', 'B', 'B'],\n",
        "                   'Gender': ['Male', 'Female', 'Male', 'Female'],\n",
        "                   'Applicants': [100, 80, 500, 400],\n",
        "                   'Admitted': [60, 40, 40, 70]})\n",
        "\n",
        "# Calculate admission rates\n",
        "df['Admission Rate'] = df['Admitted'] / df['Applicants'] * 100\n",
        "\n",
        "# Display the dataframe\n",
        "print(df)\n",
        "\n",
        "# Calculate overall admission rate\n",
        "overall_rate = df['Admitted'].sum() / df['Applicants'].sum() * 100\n",
        "print(f\"Overall Admission Rate: {overall_rate:.2f}%\")\n",
        "\n",
        "# Calculate admission rates by department and gender\n",
        "department_rates = df.groupby(['Department', 'Gender'])['Admission Rate'].mean()\n",
        "print(department_rates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MvGUloJiMAb",
        "outputId": "eec203d7-7e9a-49b7-c4e9-48d2283f0748"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Department  Gender  Applicants  Admitted  Admission Rate\n",
            "0          A    Male         100        60            60.0\n",
            "1          A  Female          80        40            50.0\n",
            "2          B    Male         500        40             8.0\n",
            "3          B  Female         400        70            17.5\n",
            "Overall Admission Rate: 19.44%\n",
            "Department  Gender\n",
            "A           Female    50.0\n",
            "            Male      60.0\n",
            "B           Female    17.5\n",
            "            Male       8.0\n",
            "Name: Admission Rate, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Berkson’s Paradox"
      ],
      "metadata": {
        "id": "2CzBstwsigAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "iris = sns.load_dataset('iris')\n",
        "\n",
        "correlation = iris['sepal_length'].corr(iris['sepal_width'])\n",
        "print('Correlation between sepal length and width:', correlation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x3LPK0tikL0",
        "outputId": "559a1843-6534-4895-a03c-6e15c8cf61fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation between sepal length and width: -0.11756978413300208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "setosa = iris[iris['species'] == 'setosa']\n",
        "correlation_setosa = setosa['sepal_length'].corr(setosa['sepal_width'])\n",
        "print('Correlation between sepal length and width for setosa:', correlation_setosa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFPeWV9Uie4p",
        "outputId": "28e3f6fe-68db-42b1-83d2-15bd7328bf43"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation between sepal length and width for setosa: 0.7425466856651597\n"
          ]
        }
      ]
    }
  ]
}