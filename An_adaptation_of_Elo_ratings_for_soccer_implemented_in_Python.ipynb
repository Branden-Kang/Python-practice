{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsBkrTrjU1Q3dz4BoHFjrU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://ofelipebandeira.medium.com/can-we-use-chess-to-predict-soccer-b7b22b75a92a)"
      ],
      "metadata": {
        "id": "wAhmPPFxbxiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Initial ratings calibration"
      ],
      "metadata": {
        "id": "Z1Af0SsYb5As"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9jcZSzxSbu-M"
      },
      "outputs": [],
      "source": [
        "def elo_predict(c, d, omega, teams, ratings_dict):\n",
        "  '''\n",
        "  Calculates predicted Elo outcome (E_H and E_A)\n",
        "\n",
        "  Inputs:\n",
        "    c, d, omega: int\n",
        "      Free variables for the formula\n",
        "    teams: list\n",
        "      Name of both teams in the match\n",
        "    ratings_dict: dict\n",
        "      Dictionary with the teams as keys and their Elo score as value\n",
        "\n",
        "  Outputs:\n",
        "    expected_home, expected_away: float\n",
        "      The expected Elo outcome (E_H and E_A) for each team\n",
        "    rating_difference: float\n",
        "      The difference in ratings between both teams (used to inform the logistic regression)\n",
        "\n",
        "  '''\n",
        "  rating_home = ratings_dict[teams[0]]\n",
        "  rating_away = ratings_dict[teams[1]]\n",
        "  rating_difference = rating_home - rating_away\n",
        "\n",
        "  exponent = (rating_away - rating_home - omega)/d\n",
        "\n",
        "  expected_home = 1/(1 + c**exponent) # This is E_H in the formula\n",
        "  expected_away = 1 - expected_home\n",
        "\n",
        "  return expected_home, expected_away, rating_difference\n",
        "\n",
        "def elo_update(k0, expected_home, expected_away, teams, goals, outcomes, ratings_dict):\n",
        "  '''\n",
        "  Updates Elo ratings for two teams based on the match outcome.\n",
        "\n",
        "  Inputs:\n",
        "    k0: int or float\n",
        "      Base scaling factor used for the rating update\n",
        "    expected_home, expected_away: float\n",
        "      The expected outcomes for the home and away teams (E_H and E_A)\n",
        "    teams: list\n",
        "      Name of both teams in the match (home team first, away team second)\n",
        "    goals: list\n",
        "      Number of goals scored by each team ([home_goals, away_goals])\n",
        "    outcomes: list\n",
        "      Actual match outcomes for both teams ([home_outcome, away_outcome])\n",
        "      Typically 1 for a win, 0.5 for a draw, and 0 for a loss\n",
        "    ratings_dict: dict\n",
        "      Dictionary with the teams as keys and their current Elo ratings as values\n",
        "\n",
        "  Outputs:\n",
        "    ratings_dict: dict\n",
        "      Updated dictionary with new Elo ratings for the two teams involved in the match\n",
        "  '''\n",
        "  # Unpacks variables\n",
        "  home = teams[0]\n",
        "  away = teams[1]\n",
        "  rating_home = ratings_dict[home]\n",
        "  rating_away = ratings_dict[away]\n",
        "  outcome_home = outcomes[0]\n",
        "  outcome_away = outcomes[1]\n",
        "  goal_diff = abs(goals[0] - goals[1])\n",
        "\n",
        "  ratings_dict[home] = rating_home + k0*(1+goal_diff) * (outcome_home - expected_home)\n",
        "  ratings_dict[away] = rating_away + k0*(1+goal_diff) * (outcome_away - expected_away)\n",
        "\n",
        "  return ratings_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_elo_outcome(row):\n",
        "  '''\n",
        "  Determines outcome of a match (S_H or S_A in the formula) according to Elo's standards:\n",
        "  0 for loss, 0.5 for draw, 1 for victory\n",
        "  '''\n",
        "  if row['Res'] == 'H':\n",
        "    return [1, 0]\n",
        "  elif row['Res'] == 'D':\n",
        "    return [0.5, 0.5]\n",
        "  else:\n",
        "    return [0, 1]"
      ],
      "metadata": {
        "id": "AAnPI-2Fb7HZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_elo_calibration(df, calibration_seasons, c=10, d=400, omega=100, k0=10):\n",
        "  '''\n",
        "  This function iteratively adjusts team ratings based on match results over multiple seasons.\n",
        "\n",
        "  Inputs:\n",
        "    df: pandas.DataFrame\n",
        "      Dataset containing match data, including columns for season, teams, goals etc.\n",
        "    calibration_seasons: list\n",
        "      List of seasons (or years) to be used for the calibration process\n",
        "    c, d: int or float, optional (default: 10 and 400)\n",
        "      Free variables for the Elo prediction formula\n",
        "    omega: int or float (default=100)\n",
        "      Free variable representing the advantage of the home team\n",
        "    k0: int or float, optional (default=10)\n",
        "      Scaling factor used to determine the influence of recent matches on team ratings\n",
        "\n",
        "  Outputs:\n",
        "    ratings_dict: dict\n",
        "      Dictionary with the final Elo ratings for all teams after calibration\n",
        "  '''\n",
        "  # Initialize Elo ratings for all teams\n",
        "  ratings_dict = create_elo_dict(df)\n",
        "\n",
        "  # Loop through the specified calibration seasons\n",
        "  for season in calibration_seasons:\n",
        "    # Filter data for the current season\n",
        "    season_df = df[df['Season'] == season]\n",
        "\n",
        "    # Adjust team ratings for inter-season changes\n",
        "    ratings_dict = adjust_teams_interseason(ratings_dict, season_df)\n",
        "\n",
        "    # Iterate over each match in the current season\n",
        "    for index, row in season_df.iterrows():\n",
        "      # Extract team names and match information\n",
        "      teams = [row['Home'], row['Away']]\n",
        "      goals = [row['HG'], row['AG']]\n",
        "\n",
        "      # Determine the actual match outcomes in Elo terms\n",
        "      elo_outcomes = determine_elo_outcome(row)\n",
        "\n",
        "      # Calculate expected outcomes using the Elo prediction formula\n",
        "      expected_home, expected_away, _ = elo_predict(c, d, omega, teams, ratings_dict)\n",
        "\n",
        "      # Update the Elo ratings based on the match results\n",
        "      ratings_dict = elo_update(k0, expected_home, expected_away, teams, goals, elo_outcomes, ratings_dict)\n",
        "\n",
        "  # Return the calibrated Elo ratings\n",
        "  return ratings_dict"
      ],
      "metadata": {
        "id": "xTdCRWOEcCr1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_teams_interseason(ratings_dict, elo_calibration_df):\n",
        "  '''\n",
        "  Implements the process in which promoted teams take the Elo ratings\n",
        "  of demoted teams in between seasons\n",
        "  '''\n",
        "  # Lists all teams in previous and upcoming seasons\n",
        "  old_season_teams = set(ratings_dict.keys())\n",
        "  new_season_teams = set(elo_calibration_df['Home'].unique())\n",
        "\n",
        "  # If any teams were demoted/promoted\n",
        "  if len(old_season_teams - new_season_teams) != 0:\n",
        "    demoted_teams = list(old_season_teams - new_season_teams)\n",
        "    promoted_teams = list(new_season_teams - old_season_teams)\n",
        "\n",
        "    # Inserts new team in the dictionary and removes the old one\n",
        "    for i in range(4):\n",
        "      ratings_dict[promoted_teams[i]] = ratings_dict.pop(demoted_teams[i])\n",
        "\n",
        "  return ratings_dict\n",
        "\n",
        "def create_elo_dict(df):\n",
        "  # Creates very first dictionary with initial rating of 1000 for all teams\n",
        "  teams = df[df['Season'] == 2012]['Home'].unique()\n",
        "  ratings_dict = {}\n",
        "\n",
        "  for team in teams:\n",
        "      ratings_dict[team] = 1000\n",
        "\n",
        "  return ratings_dict\n",
        "\n",
        "# Calling the function\n",
        "calibration_seasons = [2012, 2013, 2014]\n",
        "ratings_dict = run_elo_calibration(df, calibration_seasons)"
      ],
      "metadata": {
        "id": "UrHWMpcRb8Ii"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Calibrating the logistic regression"
      ],
      "metadata": {
        "id": "VrRNN9HfcFNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_logit_calibration(df, logit_seasons, ratings_dict, c=10, d=400, omega=100, k0=10):\n",
        "  '''\n",
        "  Runs the logistic regression calibration process for Elo ratings.\n",
        "\n",
        "  This function calibrates Elo ratings over multiple seasons while collecting data\n",
        "  (rating differences and outcomes) to prepare for training a logistic regression.\n",
        "  The logistic regression is later used to make outcome predictions based on rating differences.\n",
        "\n",
        "  Inputs:\n",
        "    df: pandas.DataFrame\n",
        "      Dataset containing match data, including columns for 'Season', 'Home', 'Away', 'HG', 'AG', 'Res', etc.\n",
        "    logit_seasons: list\n",
        "      List of seasons (or years) to be used for the logistic regression calibration process\n",
        "    ratings_dict: dict\n",
        "      Initial Elo ratings dictionary with teams as keys and their ratings as values\n",
        "    c, d: int or float, optional (default: 10 and 400)\n",
        "      Free variables for the Elo prediction formula\n",
        "    omega: int or float (default=100)\n",
        "      Free variable representing the advantage of the home team\n",
        "    k0: int or float, optional (default=10)\n",
        "      Scaling factor used to determine the influence of recent matches on team ratings\n",
        "\n",
        "\n",
        "  Outputs:\n",
        "    ratings_dict: dict\n",
        "      Updated Elo ratings dictionary after calibration\n",
        "    logit_df: pandas.DataFrame\n",
        "      DataFrame containing columns 'rating_diff' (Elo rating difference between teams)\n",
        "      and 'outcome' (match results) for logistic regression analysis\n",
        "  '''\n",
        "  # Initializes the Elo ratings dictionary\n",
        "  ratings_dict = ratings_dict\n",
        "\n",
        "  # Initializes an empty DataFrame to store rating differences and outcomes\n",
        "  logit_df = pd.DataFrame(columns=['season', 'rating_diff', 'outcome'])\n",
        "\n",
        "  # Loops through the specified seasons for logistic calibration\n",
        "  for season in logit_seasons:\n",
        "    # Filters data for the current season\n",
        "    season_df = df[df['Season'] == season]\n",
        "\n",
        "    # Adjusts team ratings for inter-season changes\n",
        "    ratings_dict = adjust_teams_interseason(ratings_dict, season_df)\n",
        "\n",
        "    # Iterates over each match in the current season\n",
        "    for index, row in season_df.iterrows():\n",
        "      # Extracts team names and match information\n",
        "      teams = [row['Home'], row['Away']]\n",
        "      goals = [row['HG'], row['AG']]\n",
        "\n",
        "      # Determines the match outcomes in Elo terms\n",
        "      elo_outcomes = determine_elo_outcome(row)\n",
        "\n",
        "      # Calculates expected outcomes and rating difference using the Elo prediction formula\n",
        "      expected_home, expected_away, rating_difference = elo_predict(c, d, omega, teams, ratings_dict)\n",
        "\n",
        "      # Updates Elo ratings based on the match results\n",
        "      ratings_dict = elo_update(k0, expected_home, expected_away, teams, goals, elo_outcomes, ratings_dict)\n",
        "\n",
        "      # Adds the rating difference and match outcome to the logit DataFrame\n",
        "      logit_df.loc[len(logit_df)] = {'season': season, 'rating_diff': rating_difference, 'outcome': row['Res']}\n",
        "\n",
        "  # Returns the updated ratings and the logistic regression dataset\n",
        "  return ratings_dict, logit_df\n",
        "\n",
        "# Calling the function\n",
        "logit_seasons = [2015, 2016, 2017, 2018]\n",
        "ratings_dict, logit_df = run_logit_calibration(df, logit_seasons, ratings_dict, c=10, d=400, omega=100, k0=10)"
      ],
      "metadata": {
        "id": "pzjjpgksb9Y3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_logistic_regression(logit_df, max_past_seasons = 15, report = True):\n",
        "\n",
        "  # Prunes the dataframe, if needed\n",
        "  most_recent_seasons = sorted(logit_df['season'].unique(), reverse=True)[:max_past_seasons]\n",
        "  filtered_df = logit_df[logit_df['season'].isin(most_recent_seasons)].copy()\n",
        "\n",
        "  # Adjust outcome columns from str to int\n",
        "  label_encoder = LabelEncoder()\n",
        "  filtered_df['outcome_encoded'] = label_encoder.fit_transform(filtered_df['outcome'])\n",
        "\n",
        "  # Isolates independent and dependent variables\n",
        "  X = filtered_df[['rating_diff']].values\n",
        "  y = filtered_df['outcome_encoded'].values\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "  # define the multinomial logistic regression model\n",
        "  model = LogisticRegression(solver='lbfgs')\n",
        "\n",
        "  # fit the model on the whole dataset\n",
        "  model.fit(X, y)\n",
        "\n",
        "  # report the model performance\n",
        "  if report:\n",
        "    # Generate predictions on the test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)\n",
        "\n",
        "    # Compute key metrics\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    loss = log_loss(y_test, y_prob)\n",
        "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'Recall (weighted): {recall}')\n",
        "    print(f'Balanced accuracy: {balanced_acc}')\n",
        "    print(f'Log loss: {loss}')\n",
        "    print()\n",
        "\n",
        "    # Display the confusion matrix\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
        "    disp.plot(cmap=\"Blues\")\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "XZ1wyAdOcHiU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Running the system"
      ],
      "metadata": {
        "id": "Qd7mIwxGcLto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_elo_predictions(df, logit_df, seasons, ratings_dict, plot_title,\n",
        "                        c=10, d=400, omega=100, k0=10, max_past_seasons=15,\n",
        "                        report_ml=False):\n",
        "    '''\n",
        "    Runs an Elo + logistic regression pipeline to predict match outcomes.\n",
        "\n",
        "    This function processes matches across multiple seasons, using Elo ratings\n",
        "    to estimate team strength and logistic regression to predict match outcomes.\n",
        "    It logs predictions and actual outcomes for performance evaluation.\n",
        "\n",
        "    Inputs:\n",
        "      df: pandas.DataFrame\n",
        "        Dataset with match data: 'Season', 'Home', 'Away', 'HG', 'AG', 'Res', etc.\n",
        "      logit_df: pandas.DataFrame\n",
        "        Historical data with Elo differences and match outcomes to train the model.\n",
        "      seasons: list\n",
        "        Seasons (or years) to include in the evaluation loop.\n",
        "      ratings_dict: dict\n",
        "        Current Elo ratings for all teams.\n",
        "      c, d: Elo parameters\n",
        "      omega: Home advantage parameter\n",
        "      k0: Elo update factor\n",
        "      max_past_seasons: int\n",
        "        How many seasons back to include when training logistic regression\n",
        "      report_ml: bool\n",
        "        Whether to print model performance each season\n",
        "\n",
        "    Outputs:\n",
        "      posterior_samples (array): Samples from the posterior of prediction accuracy\n",
        "      prediction_log (DataFrame): Logs model predictions vs actual outcomes\n",
        "    '''\n",
        "    ratings_dict = ratings_dict\n",
        "    logit_df = logit_df\n",
        "\n",
        "    prediction_log = pd.DataFrame(columns=['Season', 'Prediction', 'Actual', 'Correct'])\n",
        "\n",
        "    for season in seasons:\n",
        "        if season == seasons[-1]:\n",
        "            print('\\nLogistic regression performance at FINAL SEASON')\n",
        "            logistic_regression = fit_logistic_regression(logit_df, max_past_seasons, report=True)\n",
        "        else:\n",
        "            if report_ml:\n",
        "                print(f'Logistic regression performance PRE SEASON {season}')\n",
        "            logistic_regression = fit_logistic_regression(logit_df, max_past_seasons, report=report_ml)\n",
        "\n",
        "        season_df = df[df['Season'] == season]\n",
        "        ratings_dict = adjust_teams_interseason(ratings_dict, season_df)\n",
        "\n",
        "        for index, row in season_df.iterrows():\n",
        "            teams = [row['Home'], row['Away']]\n",
        "            goals = [row['HG'], row['AG']]\n",
        "            elo_outcomes = determine_elo_outcome(row)\n",
        "\n",
        "            expected_home, expected_away, rating_difference = elo_predict(c, d, omega, teams, ratings_dict)\n",
        "            yhat = logistic_regression.predict([[rating_difference]])[0]\n",
        "\n",
        "            prediction = 'A' if yhat == 0 else 'D' if yhat == 1 else 'H'\n",
        "            actual = row['Res']\n",
        "            correct = int(prediction == actual)\n",
        "\n",
        "            prediction_log.loc[len(prediction_log)] = {\n",
        "                'Season': season,\n",
        "                'Prediction': prediction,\n",
        "                'Actual': actual,\n",
        "                'Correct': correct\n",
        "            }\n",
        "\n",
        "            # Update Elo ratings and training data\n",
        "            ratings_dict = elo_update(k0, expected_home, expected_away, teams, goals, elo_outcomes, ratings_dict)\n",
        "            logit_df.loc[len(logit_df)] = {'season': season, 'rating_diff': rating_difference, 'outcome': actual}\n",
        "\n",
        "    # Analyze predictive performance using Bayesian modeling\n",
        "    num_predictions = len(prediction_log)\n",
        "    num_correct = prediction_log['Correct'].sum()\n",
        "\n",
        "    return num_predictions, num_correct"
      ],
      "metadata": {
        "id": "l1HLK8XQcKIt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating results"
      ],
      "metadata": {
        "id": "T3CdfqvncPRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_pymc(samples, success):\n",
        "  '''\n",
        "  Creates a PyMC model to estimate the accuracy of guesses\n",
        "  made with Elo ratings over a given period of time.\n",
        "  '''\n",
        "  with pm.Model() as model:\n",
        "    p = pm.Uniform('p', lower=0, upper=1) # Prior\n",
        "    x = pm.Binomial('x', n=samples, p=p, observed=success) # Likelihood\n",
        "\n",
        "  with model:\n",
        "    inference = pm.sample(progressbar=False, chains = 4, draws = 2000)\n",
        "\n",
        "  # Stores key variables\n",
        "  mean = az.summary(inference, hdi_prob = 0.95)['mean'].values[0]\n",
        "  lower = az.summary(inference, hdi_prob = 0.95)['hdi_2.5%'].values[0]\n",
        "  upper = az.summary(inference, hdi_prob = 0.95)['hdi_97.5%'].values[0]\n",
        "\n",
        "  return mean, [lower, upper]"
      ],
      "metadata": {
        "id": "68MSTGNBcN7j"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}