{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Selection Methods.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPdqGAOxXNuMbvuCtPWBlKT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie5pyB24AB4y",
        "colab_type": "text"
      },
      "source": [
        "[Reference](https://towardsdatascience.com/intro-to-feature-selection-methods-for-data-science-4cae2178a00a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP1lJem7AGay",
        "colab_type": "text"
      },
      "source": [
        "There are three types of feature selection: Wrapper methods (forward, backward, and stepwise selection), Filter methods (ANOVA, Pearson correlation, variance thresholding), and Embedded methods (Lasso, Ridge, Decision Tree). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6ivBUdwAHfH",
        "colab_type": "text"
      },
      "source": [
        "# Wrapper methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dmgw47JAQp6",
        "colab_type": "text"
      },
      "source": [
        "## Forward selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fniioKx2AMGY",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "def forward_selection(X, y, initial_list=[], threshold_in=0.01, verbose=True):\n",
        "    included = list(initial_list)\n",
        "    while True:\n",
        "        changed=False\n",
        "        # forward step\n",
        "        excluded = list(set(X.columns)-set(included))\n",
        "        new_pval = pd.Series(index=excluded)\n",
        "        for new_column in excluded:\n",
        "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
        "            new_pval[new_column] = model.pvalues[new_column]\n",
        "        ...  # Check the entire function in my github page. \n",
        "    return included\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y-EpmjNARbn",
        "colab_type": "text"
      },
      "source": [
        "## Backward selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df6vspGhAUlF",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "def back_selection(X, y, threshold_out = 0.05,verbose=True):\n",
        "    included = X.columns.tolist()\n",
        "    while True:\n",
        "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
        "        pvalues = model.pvalues.iloc[1:]\n",
        "        worst_pval = pvalues.max() # null if pvalues is empty\n",
        "        if worst_pval > threshold_out:\n",
        "            worst_feature = pvalues.argmax()\n",
        "            included.remove(worst_feature)\n",
        "         #check the entire function on my github page \n",
        "    return included\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmD1mDleAXdx",
        "colab_type": "text"
      },
      "source": [
        "## Stepwise selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIhQXfP4Ab-W",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "def stepwise_selection(X, y,initial_list=[],threshold_in=0.01,threshold_out = 0.05,verbose=True):\n",
        "    included = list(initial_list)\n",
        "    while True:\n",
        "        excluded = list(set(X.columns)-set(included))\n",
        "        new_pval = pd.Series(index=excluded)\n",
        "        for new_column in excluded:\n",
        "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
        "            new_pval[new_column] = model.pvalues[new_column]\n",
        "        best_pval = new_pval.min()\n",
        "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
        "        #check the entire function on my github page \n",
        "        if not changed:\n",
        "            break\n",
        "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
        "    return included \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzNYIbicAhvb",
        "colab_type": "text"
      },
      "source": [
        "# Filter methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps6cgRHqAskg",
        "colab_type": "text"
      },
      "source": [
        "## ANOVA (Analysis of variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP27UpmVAvq_",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "def ANOVA(X,y):\n",
        "    '''Univariate linear regression tests\n",
        "    Quick linear model for sequentially testing the effect of many regressors\n",
        "    Using scikit learn's Feature selection toolbox\n",
        "    Returns:\n",
        "        F (array) = F-values for regressors\n",
        "        pvalues (array) = p-values for F-scores'''\n",
        "    (F,pvalues) = f_regression(X,y)\n",
        "    return (F,pvalues)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbQtnEzSA0ZS",
        "colab_type": "text"
      },
      "source": [
        "## Pearson correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUv6bOxnA2jI",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "imt_features=[]\n",
        "imt_features.extend(df.corr()[\"SalePrice\"].sort_values(ascending=False).index.tolist()[:30])\n",
        "imt_features.extend(df.corr()[\"SalePrice\"].sort_values(ascending=True).index.tolist()[:30])\n",
        "#check the entire analysis on my github page \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "732P-pppA5vq",
        "colab_type": "text"
      },
      "source": [
        "## Variance thresholding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjDiOPlIA97b",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "# Create VarianceThreshold object with a variance with a default threshold of 0.5\n",
        "def variance_threshold_selector(data, threshold=0.5):\n",
        "    selector = VarianceThreshold(threshold)\n",
        "    selector.fit(data)\n",
        "    return data[data.columns[selector.get_support(indices=True)]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N62PTYDHBX7e",
        "colab_type": "text"
      },
      "source": [
        "## Interactions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMFmK-8wBVtk",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "#Add one interacting term and check its effect on model improvement. \n",
        "X_int=X_orig.copy()\n",
        "X_int[\"Quality*Condition\"]=X_int[\"OverallQual\"]*X_int[\"OverallCond\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZjni3NhBZIp",
        "colab_type": "text"
      },
      "source": [
        "# Embedded Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA1EF7oJEvKF",
        "colab_type": "text"
      },
      "source": [
        "## Ridge regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8TpzfvjEz3g",
        "colab_type": "text"
      },
      "source": [
        "![Ridge](https://miro.medium.com/max/1312/0*KYSUZMCtRuwrlmYW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCbzfLBeE3CC",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "from sklearn.linear_model import Ridge\n",
        "rr = Ridge(alpha=0.01) # higher the alpha value, more restriction on the coefficients; low alpha > more generalization, coefficients are barely\n",
        "# restricted and in this case linear and ridge regression resembles\n",
        "rr.fit(X_train, y_train)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euR8nlP9FGfl",
        "colab_type": "text"
      },
      "source": [
        "## Lasso Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz5mQ5XaFKoD",
        "colab_type": "text"
      },
      "source": [
        "![Lasso](https://miro.medium.com/max/1274/0*7wWuuNtsjiGbBXre)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w41rzwVdFjQo",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso()\n",
        "lasso.fit(X_train,y_train)\n",
        "train_score=lasso.score(X_train,y_train)\n",
        "test_score=lasso.score(X_test,y_test)\n",
        "coeff_used = np.sum(lasso.coef_!=0)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1OEIhL-Foag",
        "colab_type": "text"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o8mHxdEGpdQ",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
        "rf.fit(X_train, y_train)\n",
        "# Get numerical feature importances\n",
        "importances = list(rf.feature_importances_)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35lk3A3UGxSu",
        "colab_type": "text"
      },
      "source": [
        "- Feature: an x variable, most often a column in a dataset\n",
        "- Feature selection: optimizing a model by selecting a subset of the features to use\n",
        "- Wrapper method: trying models with different subsets of features and picking the best combination\n",
        "- Forward selection: adding features one by one to reach the optimal model\n",
        "Backward selection: removing features one by one to reach the optimal model\n",
        "- Stepwise selection: hybrid of forward and backward selection.adding and removing features one by one to reach the optimal model\n",
        "Filter method: selecting a subset of features by a measure other than error (a measure that is inherent to the feature and not dependent on a model)\n",
        "- Pearson Correlation: a measure of the linear correlation between two variables\n",
        "- Variance thresholding: selecting the features above a variance cutoff to preserve most of the information from the data\n",
        "- ANOVA: (analysis of variance) a group of statistical estimation procedures and models that is used to observe differences in treatment (sample) means; can be used to tell when a feature is statistically significant to a model\n",
        "- Interacting term: quantifies the relationship between two of the features when they depend on the value of the other; alleviates multicollinearity and can provide further insight into the data\n",
        "- Multicollinearity: occurs when two or more independent variables are highly correlated with each other\n",
        "Embedded method: selecting and tuning the subset of features during the model creation process\n",
        "- Ridge Regression: a modified least squares regression that penalizes features for having inflated beta coefficients by applying a lambda term to the cost function\n",
        "- Lasso Regression: similar to ridge regression, but different in that the lambda term added to the cost function can force a beta coefficient to zero\n",
        "- Decision Tree: a non-parametric model that using features as nodes to split samples to correctly classify an observation. In a random forest model, feature importance can be calculated using mean decrease gini score. \n",
        "- Cross Validation: a method to iteratively generate training and test datasets to estimate model performance on future unknown datasets"
      ]
    }
  ]
}