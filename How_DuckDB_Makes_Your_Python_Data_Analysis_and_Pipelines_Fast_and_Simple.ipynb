{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM/sD3vdvQRnImGFQmA2yp"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://python.plainenglish.io/how-duckdb-makes-your-python-data-analysis-and-pipelines-fast-and-simple-a8bb792ec5d5)"
      ],
      "metadata": {
        "id": "xEVR5js6d85-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# True Data Integrity with ACID Transactions"
      ],
      "metadata": {
        "id": "4TXkoX_seA6V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT3KlTvPdxT9",
        "outputId": "dce04957-ba58-4739-e185-7e9349687441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transaction failed: IO Error: No files found that match the pattern \"input.csv\"\n"
          ]
        }
      ],
      "source": [
        "import duckdb\n",
        "\n",
        "# Connects to or creates the 'pipeline.duckdb' file\n",
        "con = duckdb.connect('pipeline.duckdb')\n",
        "\n",
        "try:\n",
        "    # ðŸ’¥ Atomicity in Action: If step 2 fails, step 1 is automatically undone.\n",
        "    con.execute(\"BEGIN TRANSACTION;\")\n",
        "\n",
        "    # 1. Complex operation 1: Create a staging table\n",
        "    con.execute(\"CREATE TABLE staging_data AS SELECT * FROM read_csv_auto('input.csv');\")\n",
        "\n",
        "    # 2. Complex operation 2 (This step might fail because the file not found)\n",
        "    con.execute(\"INSERT INTO final_table SELECT * FROM non_existent_file.csv;\")\n",
        "\n",
        "    # Commit only if all steps succeed\n",
        "    con.execute(\"COMMIT;\")\n",
        "    print(\"Transaction succeeded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Transaction failed: {e}\")\n",
        "    # DuckDB handles rollback automatically on failure, ensuring data integrity.\n",
        "    con.execute(\"ROLLBACK;\")\n",
        "\n",
        "con.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Single, Portable .duckdb File"
      ],
      "metadata": {
        "id": "ddvn4KS1eDwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb\n",
        "\n",
        "# The entire database lives in this single file\n",
        "db_file_path = 'project_warehouse.duckdb'\n",
        "you_conn = duckdb.connect(database=db_file_path)\n",
        "\n",
        "# Execute schema and data operations\n",
        "you_conn.execute(\"CREATE TABLE sales (id INTEGER, amount DOUBLE, region VARCHAR);\")\n",
        "you_conn.execute(\"INSERT INTO sales VALUES (1, 100.50, 'East');\")\n",
        "you_conn.close()\n",
        "\n",
        "# Share 'project_warehouse.duckdb' with your colleague.\n",
        "# They can reopen it instantly with all data intact.\n",
        "conn_by_colleague = duckdb.connect(database=db_file_path)\n",
        "results_reopen = conn_by_colleague.execute(\"SELECT COUNT(*) FROM sales;\").fetchone()\n",
        "print(f\"File Size Check: {results_reopen[0]} records found.\")\n",
        "conn_by_colleague.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4FYO9TBeCWo",
        "outputId": "08d98cff-25fa-46bf-e4f0-a7eb199d1194"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Size Check: 1 records found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eliminating Dependency Hell with Built-in Ecosystem"
      ],
      "metadata": {
        "id": "KHplxgIoeI_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use SQL to install and load the S3 extension\n",
        "# DuckDB handles all underlying networking dependencies\n",
        "con.execute(\"INSTALL httpfs;\")\n",
        "con.execute(\"LOAD httpfs;\")\n",
        "\n",
        "# Assuming S3 credentials are set up...\n",
        "\n",
        "# Query a remote Parquet file directly from the S3 URL\n",
        "remote_query = \"\"\"\n",
        "SELECT\n",
        "    sum(trip_distance),\n",
        "    count(*)\n",
        "FROM\n",
        "    read_parquet('s3://nyc-tlc/trip_data/yellow_tripdata_2023-01.parquet')\n",
        "WHERE\n",
        "    passenger_count > 2\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "remote_data = con.execute(remote_query).fetchall()\n",
        "print(f\"Results from S3: {remote_data}\")"
      ],
      "metadata": {
        "id": "0c59igxoeGTs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integration with Pandas and Polars"
      ],
      "metadata": {
        "id": "k9G4tmjOeN-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb\n",
        "import pandas as pd\n",
        "\n",
        "con = duckdb.connect(':memory:') # Use in-memory connection for quick demo\n",
        "\n",
        "# 1. Create a Pandas DataFrame\n",
        "df_pandas = pd.DataFrame({'id': range(1000), 'value': [i * 1.5 for i in range(1000)]})\n",
        "\n",
        "# 2. Query the Pandas DataFrame directly using SQL!\n",
        "# No need to load it into a separate table first.\n",
        "sql_query = \"\"\"\n",
        "SELECT\n",
        "    sum(value) as total,\n",
        "    count(id)  as total_number\n",
        "FROM\n",
        "    df_pandas\n",
        "WHERE\n",
        "    value > 500;\n",
        "\"\"\"\n",
        "\n",
        "# Get the results back as a Pandas DataFrame\n",
        "result_df = con.execute(sql_query).df()\n",
        "print(\"\\nQuerying a Pandas DF with DuckDB SQL:\")\n",
        "print(result_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTj9MeYPeKpC",
        "outputId": "38695834-712e-47af-a874-d7ee799b8afc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Querying a Pandas DF with DuckDB SQL:\n",
            "      total  total_number\n",
            "0  665833.5           666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seamless File Integration for Data Lake Queries"
      ],
      "metadata": {
        "id": "n3-f8eoMeRTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "-- Querying a local data lake using pure SQL\n",
        "SELECT\n",
        "    t1.user_id,\n",
        "    t2.purchase_date,\n",
        "    t2.item_count\n",
        "FROM\n",
        "    read_parquet('./local_users_master.parquet') AS t1\n",
        "JOIN\n",
        "    read_json_auto('./api_purchases_*.json') AS t2 -- Supports glob patterns\n",
        "ON\n",
        "    t1.user_id = t2.user_id\n",
        "WHERE\n",
        "    t2.item_count > 5;\n",
        "```"
      ],
      "metadata": {
        "id": "Sfu25DQReT5H"
      }
    }
  ]
}