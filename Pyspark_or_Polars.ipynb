{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnV3fUvewYN/fG/XCCMivC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://viv1kv.medium.com/pyspark-or-polars-what-should-you-use-breakdown-of-similarities-and-differences-b261a825b9d6)"
      ],
      "metadata": {
        "id": "sVPzNRN1uHMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install polars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXLFf5F9t8Dl",
        "outputId": "558cf691-fc47-40dc-83ea-7bbbbce7a586"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.9/dist-packages (0.17.5)\n",
            "Requirement already satisfied: typing_extensions>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from polars) (4.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJRBsYqpt6Q4",
        "outputId": "47950dfe-bd0c-4cfe-f78f-15404da14c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (3, 3)\n",
            "┌───────────────┬────────────────┬──────────────────────────┐\n",
            "│ city          ┆ average_income ┆ average_increased_income │\n",
            "│ ---           ┆ ---            ┆ ---                      │\n",
            "│ str           ┆ f64            ┆ f64                      │\n",
            "╞═══════════════╪════════════════╪══════════════════════════╡\n",
            "│ Chicago       ┆ 65000.0        ┆ 70000.0                  │\n",
            "│ Los Angeles   ┆ 60000.0        ┆ 65000.0                  │\n",
            "│ San Francisco ┆ 55000.0        ┆ 60000.0                  │\n",
            "└───────────────┴────────────────┴──────────────────────────┘\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "\n",
        "# Create Polars DataFrames\n",
        "data1 = {\"id\": [1, 2, 3, 4],\n",
        "         \"age\": [25, 30, 35, 40],\n",
        "         \"salary\": [50000, 55000, 60000, 65000]}\n",
        "data2 = {\"id\": [1, 2, 3, 4],\n",
        "         \"city\": [\"New York\", \"San Francisco\", \"Los Angeles\", \"Chicago\"]}\n",
        "\n",
        "df1_polars = pl.DataFrame(data1)\n",
        "df2_polars = pl.DataFrame(data2)\n",
        "\n",
        "# Perform operations\n",
        "selected_df = df1_polars.select([\"id\", \"salary\"])\n",
        "filtered_df = selected_df.filter(pl.col(\"salary\") > 50000)\n",
        "renamed_df = filtered_df.rename({\"salary\": \"income\"})\n",
        "joined_df = renamed_df.join(df2_polars, on=\"id\", how=\"inner\")\n",
        "conditional_df = joined_df.with_columns(pl.when(joined_df[\"income\"] > 60000).then(1).otherwise(0).alias(\"high_income\"))\n",
        "\n",
        "# Apply UDF\n",
        "def salary_increase(salary: int) -> int:\n",
        "    return salary + 5000\n",
        "\n",
        "udf_applied_df = conditional_df.with_columns(pl.col(\"income\").apply(salary_increase).alias(\"increased_income\"))\n",
        "\n",
        "# Window function\n",
        "grouped_df = udf_applied_df.groupby(\"city\")\n",
        "ranked_df = grouped_df.agg(pl.col(\"income\").mean().alias(\"average_income\"),\n",
        "                            pl.col(\"increased_income\").mean().alias(\"average_increased_income\"))\n",
        "sorted_df = ranked_df.sort(by=[\"average_income\"], descending=True)\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "print(sorted_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql import SparkSession, Row\n",
        "# from pyspark.sql.functions import avg, col, when, row_number\n",
        "# from pyspark.sql.window import Window\n",
        "\n",
        "# # Create a Spark session\n",
        "# spark = SparkSession.builder \\\n",
        "#     .appName(\"PySpark Example\") \\\n",
        "#     .getOrCreate()\n",
        "\n",
        "# # Create PySpark DataFrames\n",
        "# data1 = [Row(id=1, age=25, salary=50000),\n",
        "#          Row(id=2, age=30, salary=55000),\n",
        "#          Row(id=3, age=35, salary=60000),\n",
        "#          Row(id=4, age=40, salary=65000)]\n",
        "# data2 = [Row(id=1, city=\"New York\"),\n",
        "#          Row(id=2, city=\"San Francisco\"),\n",
        "#          Row(id=3, city=\"Los Angeles\"),\n",
        "#          Row(id=4, city=\"Chicago\")]\n",
        "\n",
        "# df1_pyspark = spark.createDataFrame(data1)\n",
        "# df2_pyspark = spark.createDataFrame(data2)\n",
        "\n",
        "# # Perform operations\n",
        "# selected_df = df1_pyspark.select(\"id\", \"salary\")\n",
        "# filtered_df = selected_df.filter(col(\"salary\") > 50000)\n",
        "# renamed_df = filtered_df.withColumnRenamed(\"salary\", \"income\")\n",
        "# joined_df = renamed_df.join(df2_pyspark, on=\"id\", how=\"inner\")\n",
        "# conditional_df = joined_df.withColumn(\"high_income\", when(col(\"income\") > 60000, 1).otherwise(0))\n",
        "\n",
        "# def salary_increase(salary: int) -> int:\n",
        "#   return salary + 5000\n",
        "\n",
        "# from pyspark.sql.functions import udf\n",
        "# from pyspark.sql.types import IntegerType\n",
        "# salary_increase_udf = udf(salary_increase, IntegerType())\n",
        "# udf_applied_df = conditional_df.withColumn(\"increased_income\", salary_increase_udf(col(\"income\")))\n",
        "\n",
        "# window_spec = Window.orderBy(\"id\")\n",
        "# ranked_df = udf_applied_df.withColumn(\"rank\", row_number().over(window_spec))\n",
        "\n",
        "# #GroupBy and aggregation\n",
        "# result_df = (ranked_df.groupBy(\"city\")\n",
        "# .agg(avg(\"income\").alias(\"average_income\"),\n",
        "# avg(\"increased_income\").alias(\"average_increased_income\"))\n",
        "# .sort(\"average_income\", ascending=False))\n",
        "\n",
        "# #Show the resulting DataFrame\n",
        "# result_df.show()\n",
        "\n",
        "# #Stop the Spark session\n",
        "# spark.stop()"
      ],
      "metadata": {
        "id": "vDsB79e7t9hP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import timeit\n",
        "# import random\n",
        "# import string\n",
        "\n",
        "# # Random Data - I am using One Million Rows for this experiment.\n",
        "# num_rows = 1000000\n",
        "# ages = [random.randint(18, 65) for _ in range(num_rows)]\n",
        "# salaries = [random.randint(30000, 200000) for _ in range(num_rows)]\n",
        "# cities = [random.choice([\"New York\", \"San Francisco\", \"Los Angeles\", \"Chicago\"]) for _ in range(num_rows)]\n",
        "\n",
        "# data1 = [{\"id\": i, \"age\": age, \"salary\": salary} for i, (age, salary) in enumerate(zip(ages, salaries), start=1)]\n",
        "# data2 = [{\"id\": i, \"city\": city} for i, city in enumerate(cities, start=1)]\n",
        "    \n",
        "# def pyspark_benchmark():\n",
        "#     from pyspark.sql import Row, SparkSession\n",
        "#     from pyspark.sql.functions import avg, col, when, row_number\n",
        "#     from pyspark.sql.window import Window\n",
        "\n",
        "#     spark = SparkSession.builder.appName(\"PySpark Benchmark\").getOrCreate()\n",
        "#     df1_pyspark = spark.createDataFrame([Row(**row) for row in data1])\n",
        "#     df2_pyspark = spark.createDataFrame([Row(**row) for row in data2])\n",
        "\n",
        "#     joined_df = df1_pyspark.join(df2_pyspark, on=\"id\", how=\"inner\")\n",
        "#     conditional_df = joined_df.withColumn(\"high_income\", when(col(\"salary\") > 100000, 1).otherwise(0))\n",
        "#     window_spec = Window.orderBy(\"id\")\n",
        "#     ranked_df = conditional_df.withColumn(\"rank\", row_number().over(window_spec))\n",
        "#     result_df = (ranked_df.groupBy(\"city\")\n",
        "#     .agg(avg(\"salary\").alias(\"average_salary\"))\n",
        "#     .orderBy(\"average_salary\", ascending=False)\n",
        "#     .limit(10))\n",
        "    \n",
        "#     result_df.show()\n",
        "\n",
        "# def polars_benchmark():\n",
        "#     import polars as pl\n",
        "\n",
        "#     df1_polars = pl.DataFrame(data1)\n",
        "#     df2_polars = pl.DataFrame(data2)\n",
        "\n",
        "#     joined_df = df1_polars.join(df2_polars, on=\"id\", how=\"inner\")\n",
        "#     conditional_df = joined_df.with_columns(pl.when(joined_df[\"salary\"] > 100000).then(1).otherwise(0).alias(\"high_income\"))\n",
        "#     ranked_df = conditional_df.with_columns(pl.col(\"id\").rank().over(\"id\").alias(\"rank\"))\n",
        "#     result_df = (ranked_df.groupby(\"city\")\n",
        "#                  .agg(pl.col(\"salary\").mean().alias(\"average_salary\"))\n",
        "#                  .sort(\"average_salary\", descending=True)\n",
        "#                  .head(10))\n",
        "\n",
        "#     print(result_df)\n",
        "\n",
        "\n",
        "# pyspark_time = timeit.timeit(\"pyspark_benchmark()\", globals=globals(), number=1)\n",
        "# polars_time = timeit.timeit(\"polars_benchmark()\", globals=globals(), number=1)\n",
        "\n",
        "# print(f\"PySpark execution time: {pyspark_time:.2f} seconds\")\n",
        "# print(f\"Polars execution time: {polars_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "sfE_Yj19t_kk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0cw1WMC5uDYi"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}