{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzWSU6jS/Eo1fh+lAoQdba"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@Rohan_Dutt/why-feature-stores-are-essential-for-consistency-in-ml-and-how-to-build-a-simple-one-969cc38599a9)"
      ],
      "metadata": {
        "id": "GgddGoy3jXkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Core Architecture"
      ],
      "metadata": {
        "id": "jk-XKsWIj4rs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DUZ8yYD2jUhC"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "# The Single Source of Truth\n",
        "# I define the logic ONCE here. No copy-pasting into SQL files.\n",
        "@dataclass\n",
        "class FeatureDefinition:\n",
        "    name: str\n",
        "    entity_id: str\n",
        "    description: str\n",
        "    # The logic that runs in the Offline Store (Batch)\n",
        "    batch_sql: str = \"\"\"\n",
        "        SELECT user_id, count(*) as val\n",
        "        FROM logs\n",
        "        WHERE action = 'click'\n",
        "        AND timestamp > current_date - interval '7' day\n",
        "        GROUP BY user_id\n",
        "    \"\"\"\n",
        "    # The storage type for the Online Store (Inference)\n",
        "    online_type: str = \"int32\"\n",
        "click_feature = FeatureDefinition(\n",
        "    name=\"user_clicks_7d\",\n",
        "    entity_id=\"user_id\",\n",
        "    description=\"Total clicks in last 7 days\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import redis\n",
        "\n",
        "# The \"Materialization\" Job\n",
        "# This runs every hour to keep the Online Store fresh.\n",
        "def sync_offline_to_online(feature_def, redis_client):\n",
        "    # 1. Get the heavy data from the Offline Warehouse\n",
        "    df = pd.read_sql(feature_def.batch_sql, offline_db_connection)\n",
        "\n",
        "    # 2. Push it to the Online Cache (Fast access)\n",
        "    # Using a pipeline makes this blazingly fast.\n",
        "    pipe = redis_client.pipeline()\n",
        "    for _, row in df.iterrows():\n",
        "        # Key: \"user_id:123\", Field: \"user_clicks_7d\", Value: 50\n",
        "        key = f\"{feature_def.entity_id}:{row[feature_def.entity_id]}\"\n",
        "        pipe.hset(key, feature_def.name, row['val'])\n",
        "\n",
        "    pipe.execute()\n",
        "    print(f\"Synced {len(df)} records for {feature_def.name}.\")\n",
        "# usage\n",
        "# sync_offline_to_online(click_feature, r)"
      ],
      "metadata": {
        "id": "aygIchpIjxR8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a â€œDIYâ€ Feature Store"
      ],
      "metadata": {
        "id": "8839mZfUj7Ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import redis\n",
        "import json\n",
        "\n",
        "# My Simple SDK\n",
        "def get_online_features(entity_ids, feature_names, redis_client):\n",
        "    # Pipeline is the secret sauce.\n",
        "    # It sends ONE request instead of hundreds.\n",
        "    pipe = redis_client.pipeline()\n",
        "\n",
        "    for entity_id in entity_ids:\n",
        "        # Key format: \"user:12345\"\n",
        "        pipe.hmget(f\"user:{entity_id}\", feature_names)\n",
        "\n",
        "    results = pipe.execute()\n",
        "\n",
        "    # Format the mess into clean JSON\n",
        "    features = []\n",
        "    for i, entity_id in enumerate(entity_ids):\n",
        "        features.append({\n",
        "            \"entity_id\": entity_id,\n",
        "            \"values\": dict(zip(feature_names, results[i]))\n",
        "        })\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "4twC-_bij0RM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Time Machine (PySpark)\n",
        "# I save this to S3 for the offline store\n",
        "df = spark.read.parquet(\"s3://raw-logs/\")\n",
        "\n",
        "# Calculate features for a specific day in the past\n",
        "daily_features = df.filter(df.date == \"2023-10-27\") \\\n",
        "    .groupBy(\"user_id\") \\\n",
        "    .agg({\"amount\": \"sum\"}) \\\n",
        "    .withColumnRenamed(\"sum(amount)\", \"total_spend_1d\")\n",
        "# Write to partitioned folders\n",
        "daily_features.write.partitionBy(\"date\").parquet(\"s3://feature-store/offline/\")"
      ],
      "metadata": {
        "id": "QIaJJ0smj917"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Safety Net\n",
        "# When I sync data, I make sure it dies eventually.\n",
        "def sync_to_redis(user_id, features, redis_client):\n",
        "    key = f\"user:{user_id}\"\n",
        "\n",
        "    # Set the data\n",
        "    redis_client.hset(key, mapping=features)\n",
        "\n",
        "    # Expire after 7 days (604800 seconds)\n",
        "    # If the user stops visiting, we stop paying for storage.\n",
        "    redis_client.expire(key, 604800)"
      ],
      "metadata": {
        "id": "biG4G6v6kALG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Patterns for Scale"
      ],
      "metadata": {
        "id": "PEQ0RJ5akDJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On-Demand Logic\n",
        "# The Feature Store fetches 'home_loc' from Redis (Fast)\n",
        "# But it calculates 'dist' using the Request Context (Live)\n",
        "\n",
        "def get_distance_feature(user_id, current_lat, current_lon, redis_client):\n",
        "    # 1. Get static data\n",
        "    home_loc = json.loads(redis_client.get(f\"user:{user_id}:home\"))\n",
        "\n",
        "    # 2. Compute dynamic feature NOW\n",
        "    # If I pre-computed this, it would be wrong by the time you read it.\n",
        "    return haversine(current_lat, current_lon, home_loc['lat'], home_loc['lon'])"
      ],
      "metadata": {
        "id": "udmZfaI7kBwV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import entropy\n",
        "import numpy as np\n",
        "\n",
        "# The Panic Button ðŸš¨\n",
        "# I run this check every hour.\n",
        "def check_drift(training_dist, live_dist):\n",
        "    # KL Divergence: 0 means identical. High means trouble.\n",
        "    score = entropy(training_dist, live_dist)\n",
        "\n",
        "    if score > 0.1:\n",
        "        raise Alert(f\"Data is drifting! Score: {score}\")\n",
        "\n",
        "    return \"All good.\""
      ],
      "metadata": {
        "id": "iL6yPcl5kGLr"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}