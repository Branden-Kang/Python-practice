{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1+tnOo3kaZ+8EVYiaDBO3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfDS2A2r1bEV",
        "outputId": "f844c893-fb71-4af7-908e-0de6014316f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - train_loss: 0.460041\n",
            "          val_loss: 0.285489\n",
            "Epoch 2/10 - train_loss: 0.248642\n",
            "          val_loss: 0.210745\n",
            "Epoch 3/10 - train_loss: 0.226196\n",
            "          val_loss: 0.267167\n",
            "Epoch 4/10 - train_loss: 0.208846\n",
            "          val_loss: 0.191431\n",
            "Epoch 5/10 - train_loss: 0.191726\n",
            "          val_loss: 0.422729\n",
            "Epoch 6/10 - train_loss: 0.228785\n",
            "          val_loss: 0.198735\n",
            "Early stopping at epoch 6\n",
            "\n",
            "===== Test Metrics =====\n",
            "Accuracy : 0.9222\n",
            "ROC-AUC  : 0.9821\n",
            "Logloss  : 0.1785\n",
            "Uncertainty rate (stream disagreement): 0.020\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Monolithic Tabular-Text Transformer (TTT) — single-class model\n",
        "- 모든 모델 컴포넌트를 TabularTextTransformerMono 내부 메서드로 통합\n",
        "- DTQ, 사인/코사인 PE, Overall Attention, FFN을 별도 클래스 없이 구현\n",
        "- DataLoader엔 CPU 텐서만 넣고, 배치 루프에서만 .to(device) (pin_memory 에러 방지)\n",
        "\"\"\"\n",
        "\n",
        "# Third Party\n",
        "import math\n",
        "import re\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 0) Utilities\n",
        "# =========================\n",
        "def _to_array(X):\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        return X.values\n",
        "    return np.asarray(X, dtype=object)\n",
        "\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    \"\"\"아주 단순한 공백 기반 토크나이저 + vocab\"\"\"\n",
        "    def __init__(self, lower=True, min_freq=2, max_vocab=30000, pad_token=\"<pad>\", unk_token=\"<unk>\"):\n",
        "        self.lower = lower\n",
        "        self.min_freq = min_freq\n",
        "        self.max_vocab = max_vocab\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "        self.stoi = {pad_token: 0, unk_token: 1}\n",
        "        self.itos = [pad_token, unk_token]\n",
        "        self.fitted_ = False\n",
        "\n",
        "    def _basic_tokenize(self, s: str):\n",
        "        if s is None:\n",
        "            return []\n",
        "        if not isinstance(s, str):\n",
        "            s = str(s)\n",
        "        if self.lower:\n",
        "            s = s.lower()\n",
        "        return [t for t in re.split(r\"\\W+\", s) if t]\n",
        "\n",
        "    def fit(self, texts: List[str]):\n",
        "        from collections import Counter\n",
        "        c = Counter()\n",
        "        for t in texts:\n",
        "            c.update(self._basic_tokenize(t))\n",
        "        items = sorted([kv for kv in c.items() if kv[1] >= self.min_freq], key=lambda x: (-x[1], x[0]))\n",
        "        for tok, _ in items[: self.max_vocab - len(self.itos)]:\n",
        "            if tok not in self.stoi:\n",
        "                self.stoi[tok] = len(self.itos)\n",
        "                self.itos.append(tok)\n",
        "        self.fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def encode(self, s: str):\n",
        "        toks = self._basic_tokenize(s)\n",
        "        return [self.stoi.get(t, 1) for t in toks]  # 1 = <unk>\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 1) Preprocessor (cat/num/text)\n",
        "# =========================\n",
        "class TTTPreprocessor:\n",
        "    \"\"\"\n",
        "    - Categorical: 문자열→정수 ID (0 = OOV)\n",
        "    - Numeric    : per-column quantiles 저장 (DTQ)\n",
        "    - Text       : vocab 구성 후 고정 길이 인코딩\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        categorical_indices: Optional[List[int]] = None,\n",
        "        text_indices: Optional[List[int]] = None,\n",
        "        use_oov: bool = True,\n",
        "        add_na_token: bool = True,\n",
        "        max_text_len: int = 128,\n",
        "        min_freq: int = 2,\n",
        "        max_vocab: int = 30000,\n",
        "    ):\n",
        "        self.categorical_indices = None if categorical_indices is None else list(categorical_indices)\n",
        "        self.text_indices = [] if text_indices is None else list(text_indices)\n",
        "        self.use_oov = use_oov\n",
        "        self.add_na_token = add_na_token\n",
        "\n",
        "        self.cat_idx, self.cont_idx = [], []\n",
        "        self.cat_maps = {}\n",
        "        self.cardinalities = []\n",
        "        self.num_quantiles = None\n",
        "        self.s = 6\n",
        "\n",
        "        self.max_text_len = max_text_len\n",
        "        self.tok = SimpleTokenizer(min_freq=min_freq, max_vocab=max_vocab)\n",
        "        self.fitted_ = False\n",
        "\n",
        "    def fit(self, X, categorical_indices=None, text_indices=None, s: int = 6):\n",
        "        X = _to_array(X)\n",
        "        n_cols = X.shape[1]\n",
        "        if categorical_indices is not None:\n",
        "            self.categorical_indices = list(categorical_indices)\n",
        "        if text_indices is not None:\n",
        "            self.text_indices = list(text_indices)\n",
        "\n",
        "        # 범주형 자동 추론(텍스트 컬럼 제외)\n",
        "        if self.categorical_indices is None:\n",
        "            self.categorical_indices = []\n",
        "            for j in range(n_cols):\n",
        "                if j in (self.text_indices or []):\n",
        "                    continue\n",
        "                if X[:, j].dtype == object:\n",
        "                    self.categorical_indices.append(j)\n",
        "\n",
        "        cat_set = set(self.categorical_indices or [])\n",
        "        text_set = set(self.text_indices or [])\n",
        "        all_idx = set(range(n_cols))\n",
        "        self.cont_idx = sorted(list(all_idx - cat_set - text_set))\n",
        "        self.cat_idx = sorted(list(cat_set))\n",
        "\n",
        "        # 범주형 매핑\n",
        "        self.cat_maps, self.cardinalities = {}, []\n",
        "        for j in self.cat_idx:\n",
        "            col = X[:, j]\n",
        "            if self.add_na_token:\n",
        "                col = np.where(pd.isna(col), \"<NA>\", col)\n",
        "            uniq = pd.unique(col)\n",
        "            self.cat_maps[j] = {v: i + 1 for i, v in enumerate(uniq)}  # 1..K\n",
        "            self.cardinalities.append(len(uniq))\n",
        "\n",
        "        # 수치형 분위수\n",
        "        self.s = int(s)\n",
        "        if len(self.cont_idx) > 0:\n",
        "            cont = X[:, self.cont_idx].astype(\"float32\")\n",
        "            qs = np.linspace(0, 1, self.s, dtype=np.float32)\n",
        "            self.num_quantiles = np.quantile(cont, qs, axis=0).T  # (n_num, s)\n",
        "        else:\n",
        "            self.num_quantiles = None\n",
        "\n",
        "        # 텍스트 vocab\n",
        "        if len(self.text_indices) > 0:\n",
        "            texts = (X[:, self.text_indices].astype(str)\n",
        "                     if len(self.text_indices) > 1 else X[:, self.text_indices[0]].astype(str))\n",
        "            if texts.ndim == 2:\n",
        "                joined = [\" \".join(row.tolist()) for row in texts]\n",
        "            else:\n",
        "                joined = texts.tolist()\n",
        "            self.tok.fit(joined)\n",
        "\n",
        "        self.fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        assert self.fitted_, \"Call fit() first.\"\n",
        "        X = _to_array(X)\n",
        "        B = X.shape[0]\n",
        "\n",
        "        # categorical\n",
        "        if len(self.cat_idx) > 0:\n",
        "            x_cat = np.zeros((B, len(self.cat_idx)), dtype=\"int64\")\n",
        "            for ti, j in enumerate(self.cat_idx):\n",
        "                col = X[:, j]\n",
        "                if self.add_na_token:\n",
        "                    col = np.where(pd.isna(col), \"<NA>\", col)\n",
        "                m = self.cat_maps[j]\n",
        "                x_cat[:, ti] = np.array([m.get(v, 0) for v in col], dtype=\"int64\")\n",
        "        else:\n",
        "            x_cat = np.zeros((B, 0), dtype=\"int64\")\n",
        "\n",
        "        # numeric\n",
        "        x_num = X[:, self.cont_idx].astype(\"float32\") if len(self.cont_idx) > 0 else None\n",
        "\n",
        "        # text\n",
        "        if len(self.text_indices) > 0:\n",
        "            texts = (X[:, self.text_indices].astype(str)\n",
        "                     if len(self.text_indices) > 1 else X[:, self.text_indices[0]].astype(str))\n",
        "            if texts.ndim == 2:\n",
        "                joined = [\" \".join(row.tolist()) for row in texts]\n",
        "            else:\n",
        "                joined = texts.tolist()\n",
        "            enc = []\n",
        "            for s in joined:\n",
        "                ids = self.tok.encode(s)[: self.max_text_len]\n",
        "                if len(ids) < self.max_text_len:\n",
        "                    ids = ids + [0] * (self.max_text_len - len(ids))\n",
        "                enc.append(ids)\n",
        "            x_text = np.asarray(enc, dtype=\"int64\")\n",
        "        else:\n",
        "            x_text = None\n",
        "\n",
        "        return x_cat, x_num, x_text\n",
        "\n",
        "    def fit_transform(self, X, **kw):\n",
        "        self.fit(X, **kw)\n",
        "        return self.transform(X)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 2) Monolithic TTT Model (single class)\n",
        "# =========================\n",
        "class TabularTextTransformerMono(nn.Module):\n",
        "    \"\"\"\n",
        "    단일 클래스 내부에:\n",
        "      - 텍스트 임베딩(+사인/코사인 위치인코딩), [CLS]_text\n",
        "      - 범주형 임베딩(+OOV=0)\n",
        "      - 수치형 DTQ(분위수 역거리 가중합 임베딩)\n",
        "      - Dual-stream Overall Attention (text/tab)\n",
        "      - 듀얼 헤드(BCE), 최종 출력은 평균\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        cat_cardinalities: List[int],\n",
        "        n_num: int,\n",
        "        vocab_size: int,\n",
        "        d_model=128, n_heads=4, n_layers=2, dim_feedforward=256, dropout=0.1,\n",
        "        num_quantiles: Optional[np.ndarray] = None,  # (n_num, s)\n",
        "        max_text_len: int = 2048,\n",
        "        text_pad_id: int = 0,\n",
        "        use_text_positional_encoding: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.n_cat = len(cat_cardinalities)\n",
        "        self.n_num = n_num\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d = d_model\n",
        "        self.text_pad_id = text_pad_id\n",
        "        self.use_text_pos = use_text_positional_encoding\n",
        "        self.max_text_len = max_text_len\n",
        "\n",
        "        # ---- 텍스트 임베딩 & [CLS] ----\n",
        "        self.text_tok = nn.Embedding(vocab_size, d_model, padding_idx=text_pad_id)\n",
        "        nn.init.normal_(self.text_tok.weight, std=0.02)\n",
        "        self.cls_text = nn.Parameter(torch.zeros(1, 1, d_model)); nn.init.normal_(self.cls_text, std=0.02)\n",
        "\n",
        "        # ---- 사인/코사인 위치인코딩 버퍼 ----\n",
        "        if self.use_text_pos:\n",
        "            pe = self._build_sinusoidal_pe(d_model, max_len=max_text_len + 1)  # +1 for [CLS]\n",
        "            self.register_buffer(\"pe_text\", pe)  # (max_len+1, d)\n",
        "        else:\n",
        "            self.register_buffer(\"pe_text\", None)\n",
        "\n",
        "        # ---- 범주형 임베딩 ----\n",
        "        if self.n_cat > 0:\n",
        "            self.cat_embs = nn.ModuleList([nn.Embedding(c + 1, d_model, padding_idx=0) for c in cat_cardinalities])\n",
        "            for emb in self.cat_embs:\n",
        "                nn.init.normal_(emb.weight, std=0.02)\n",
        "        else:\n",
        "            self.cat_embs = nn.ModuleList()\n",
        "\n",
        "        # ---- DTQ 파라미터/버퍼 ----\n",
        "        if n_num > 0:\n",
        "            assert num_quantiles is not None and num_quantiles.shape[0] == n_num\n",
        "            self.register_buffer(\"quantiles\", torch.tensor(num_quantiles.astype(\"float32\")))  # (n_num, s)\n",
        "            self.S_dtq = nn.Parameter(torch.randn(n_num, self.quantiles.size(1), d_model) * 0.02)\n",
        "        else:\n",
        "            self.register_buffer(\"quantiles\", None)\n",
        "            self.S_dtq = None\n",
        "\n",
        "        # ---- [CLS]_tab ----\n",
        "        self.cls_tab = nn.Parameter(torch.zeros(1, 1, d_model)); nn.init.normal_(self.cls_tab, std=0.02)\n",
        "        self.emb_drop = nn.Dropout(dropout)\n",
        "\n",
        "        # ---- Dual-stream Overall Attention 레이어 스택 ----\n",
        "        self.layers_text = nn.ModuleList([\n",
        "            nn.ModuleDict(dict(\n",
        "                ln_q = nn.LayerNorm(d_model),\n",
        "                ln_kv= nn.LayerNorm(d_model),\n",
        "                attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True),\n",
        "                ln_ff= nn.LayerNorm(d_model),\n",
        "                ffn  = nn.Sequential(\n",
        "                    nn.Linear(d_model, dim_feedforward),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Dropout(dropout),\n",
        "                    nn.Linear(dim_feedforward, d_model),\n",
        "                    nn.Dropout(dropout),\n",
        "                ),\n",
        "            )) for _ in range(n_layers)\n",
        "        ])\n",
        "        self.layers_tab = nn.ModuleList([\n",
        "            nn.ModuleDict(dict(\n",
        "                ln_q = nn.LayerNorm(d_model),\n",
        "                ln_kv= nn.LayerNorm(d_model),\n",
        "                attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True),\n",
        "                ln_ff= nn.LayerNorm(d_model),\n",
        "                ffn  = nn.Sequential(\n",
        "                    nn.Linear(d_model, dim_feedforward),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Dropout(dropout),\n",
        "                    nn.Linear(dim_feedforward, d_model),\n",
        "                    nn.Dropout(dropout),\n",
        "                ),\n",
        "            )) for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # ---- Heads ----\n",
        "        self.head_text = nn.Sequential(nn.Linear(d_model, 1), nn.Sigmoid())\n",
        "        self.head_tab  = nn.Sequential(nn.Linear(d_model, 1), nn.Sigmoid())\n",
        "\n",
        "    # ===== 내부 유틸 =====\n",
        "    @staticmethod\n",
        "    def _build_sinusoidal_pe(d_model: int, max_len: int):\n",
        "        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
        "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
        "        return pe  # (max_len, d)\n",
        "\n",
        "    def _add_pos_encoding_text(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        # z: (B, 1+T, d)  (앞에 [CLS])\n",
        "        if self.pe_text is None:\n",
        "            return z\n",
        "        T = z.size(1)\n",
        "        return z + self.pe_text[:T].unsqueeze(0)\n",
        "\n",
        "    def _embed_text(self, x_text: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        x_text: (B, T) long (0=pad)\n",
        "        return: z_text0 (B, 1+T, d), pad_mask_text (B, 1+T)\n",
        "        \"\"\"\n",
        "        if x_text is None:\n",
        "            # 텍스트가 아예 없는 경우: [CLS] 하나만\n",
        "            B = 1\n",
        "            z = self.cls_text.expand(B, 1, -1)\n",
        "            pad_mask = torch.zeros(B, 1, dtype=torch.bool, device=z.device)\n",
        "            return z, pad_mask\n",
        "\n",
        "        B, T = x_text.size()\n",
        "        z = self.text_tok(x_text)             # (B, T, d)\n",
        "        cls = self.cls_text.expand(B, 1, -1)  # (B, 1, d)\n",
        "        z = torch.cat([cls, z], dim=1)        # (B, 1+T, d)\n",
        "        z = self._add_pos_encoding_text(z)\n",
        "        z = self.emb_drop(z)\n",
        "\n",
        "        pad_mask = (x_text == self.text_pad_id)  # (B, T)\n",
        "        pad_mask = torch.cat([torch.zeros(B, 1, dtype=torch.bool, device=x_text.device), pad_mask], dim=1)\n",
        "        return z, pad_mask\n",
        "\n",
        "    def _embed_tab(self, x_cat: Optional[torch.Tensor], x_num: Optional[torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        반환: z_tab0 (B, 1+T_tab, d)  with [CLS]_tab\n",
        "        \"\"\"\n",
        "        B = (x_cat.size(0) if x_cat is not None else (x_num.size(0) if x_num is not None else 1))\n",
        "        parts = []\n",
        "        if self.n_cat > 0 and x_cat is not None:\n",
        "            cat_tok = [emb(x_cat[:, j]).unsqueeze(1) for j, emb in enumerate(self.cat_embs)]  # (B,1,d) list\n",
        "            parts.append(torch.cat(cat_tok, dim=1))  # (B, n_cat, d)\n",
        "\n",
        "        if self.n_num > 0 and x_num is not None and self.S_dtq is not None:\n",
        "            # DTQ: 역거리 가중합으로 per-feature 임베딩\n",
        "            # x_num: (B, n_num), Q: (n_num, s), S: (n_num, s, d)\n",
        "            x = x_num.unsqueeze(-1)                         # (B, n_num, 1)\n",
        "            Q = self.quantiles.unsqueeze(0)                 # (1, n_num, s)\n",
        "            eq = (x == Q)\n",
        "            any_eq = eq.any(dim=-1, keepdim=True)\n",
        "            inv = torch.where(eq, torch.ones_like(Q), 1.0 / (torch.abs(x - Q) + 1e-12))\n",
        "            inv = torch.where(any_eq, torch.where(eq, torch.ones_like(inv), torch.zeros_like(inv)), inv)\n",
        "            w = inv / (inv.sum(dim=-1, keepdim=True) + 1e-12)   # (B, n_num, s)\n",
        "\n",
        "            # (B, n_num, s) @ (n_num, s, d) → (B, n_num, d)  (feature별로 계산)\n",
        "            z_num_list = []\n",
        "            for j in range(self.n_num):\n",
        "                wj = w[:, j, :].unsqueeze(1)                 # (B, 1, s)\n",
        "                Sj = self.S_dtq[j:j+1, :, :]                 # (1, s, d)\n",
        "                z_num_list.append(wj @ Sj)                   # (B, 1, d)\n",
        "            z_num = torch.cat(z_num_list, dim=1)             # (B, n_num, d)\n",
        "            parts.append(z_num)\n",
        "\n",
        "        z = torch.cat(parts, dim=1) if parts else torch.zeros(B, 0, self.d, device=self.cls_tab.device)\n",
        "        cls = self.cls_tab.expand(B, 1, -1)\n",
        "        z = torch.cat([cls, z], dim=1)\n",
        "        return self.emb_drop(z)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x_cat: Optional[torch.LongTensor],\n",
        "        x_num: Optional[torch.FloatTensor],\n",
        "        x_text: Optional[torch.LongTensor],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        반환:\n",
        "          p_avg (B,1) = 0.5*(p_tab + p_text), p_tab (B,1), p_text (B,1)\n",
        "        \"\"\"\n",
        "        # 초기 임베딩\n",
        "        z_tab0 = self._embed_tab(x_cat, x_num)                 # (B, Ttab0, d)\n",
        "        pad_mask_tab = torch.zeros(z_tab0.size()[:2], dtype=torch.bool, device=z_tab0.device)\n",
        "\n",
        "        if x_text is not None:\n",
        "            z_text0, pad_mask_text = self._embed_text(x_text)  # (B, Ttxt0, d)\n",
        "        else:\n",
        "            B = z_tab0.size(0)\n",
        "            z_text0 = self.cls_text.expand(B, 1, -1)\n",
        "            pad_mask_text = torch.zeros(B, 1, dtype=torch.bool, device=z_tab0.device)\n",
        "\n",
        "        # 진행 상태\n",
        "        z_tab = z_tab0\n",
        "        z_text = z_text0\n",
        "\n",
        "        # L 레이어 반복 (Overall Attention: self + initial-cross)\n",
        "        for i in range(len(self.layers_tab)):\n",
        "            # --- Tab stream ---\n",
        "            lt = self.layers_tab[i]\n",
        "            q = lt.ln_q(z_tab)\n",
        "            kv = lt.ln_kv(torch.cat([z_tab, z_text0], dim=1))\n",
        "            kpm = torch.cat([pad_mask_tab, pad_mask_text], dim=1)\n",
        "            attn_out, _ = lt.attn(query=q, key=kv, value=kv, key_padding_mask=kpm)\n",
        "            z_tab = z_tab + attn_out\n",
        "            z_tab = z_tab + lt.ffn(lt.ln_ff(z_tab))\n",
        "\n",
        "            # --- Text stream ---\n",
        "            lx = self.layers_text[i]\n",
        "            qx = lx.ln_q(z_text)\n",
        "            kvx = lx.ln_kv(torch.cat([z_text, z_tab0], dim=1))\n",
        "            kpmx = torch.cat([pad_mask_text, pad_mask_tab], dim=1)\n",
        "            attn_text, _ = lx.attn(query=qx, key=kvx, value=kvx, key_padding_mask=kpmx)\n",
        "            z_text = z_text + attn_text\n",
        "            z_text = z_text + lx.ffn(lx.ln_ff(z_text))\n",
        "\n",
        "        # [CLS] 추출 후 듀얼 헤드\n",
        "        cls_tab  = z_tab[:, 0, :]\n",
        "        cls_text = z_text[:, 0, :]\n",
        "        p_tab  = self.head_tab(cls_tab)     # (B,1)\n",
        "        p_text = self.head_text(cls_text)   # (B,1)\n",
        "        p = 0.5 * (p_tab + p_text)\n",
        "        return p, p_tab, p_text\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 3) Sklearn-compatible wrapper (CPU DataLoader + move-to-device in loop)\n",
        "# =========================\n",
        "class TTTBinaryClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        auto_preprocess=True,\n",
        "        categorical_indices: Optional[List[int]] = None,\n",
        "        text_indices: Optional[List[int]] = None,\n",
        "        use_oov=True,\n",
        "        max_text_len=128,\n",
        "        min_freq=2,\n",
        "        max_vocab=30000,\n",
        "        quantiles_s=6,\n",
        "        d_model=128, n_heads=4, n_layers=2, dim_feedforward=256, dropout=0.1,\n",
        "        lr=1e-3, weight_decay=1e-4,\n",
        "        loss_fn=\"logloss\",\n",
        "        device=None,\n",
        "    ):\n",
        "        self.auto_preprocess = auto_preprocess\n",
        "        self.categorical_indices = categorical_indices\n",
        "        self.text_indices = text_indices\n",
        "        self.use_oov = use_oov\n",
        "        self.max_text_len = max_text_len\n",
        "        self.min_freq = min_freq\n",
        "        self.max_vocab = max_vocab\n",
        "        self.quantiles_s = quantiles_s\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.dim_feedforward = dim_feedforward\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.loss_fn_name = loss_fn\n",
        "\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # filled later\n",
        "        self.preproc: Optional[TTTPreprocessor] = None\n",
        "        self.model: Optional[TabularTextTransformerMono] = None\n",
        "        self.best_model_state = None\n",
        "\n",
        "        self.cat_idx: List[int] = []\n",
        "        self.cont_idx: List[int] = []\n",
        "        self.text_idx: List[int] = []\n",
        "        self.cat_cardinalities: List[int] = []\n",
        "        self.num_quantiles: Optional[np.ndarray] = None\n",
        "        self.vocab_size: int = 2\n",
        "\n",
        "    # internals\n",
        "    def _define_loss(self):\n",
        "        if self.loss_fn_name == \"logloss\":\n",
        "            return nn.BCELoss(reduction=\"none\")\n",
        "        raise ValueError(self.loss_fn_name)\n",
        "\n",
        "    def _split_to_cpu_tensors(self, X):\n",
        "        \"\"\"DataLoader용 CPU 텐서 반환(루프에서만 .to(device))\"\"\"\n",
        "        if self.auto_preprocess:\n",
        "            x_cat_np, x_num_np, x_text_np = self.preproc.transform(X)\n",
        "        else:\n",
        "            arr = X if not isinstance(X, torch.Tensor) else X.detach().cpu().numpy()\n",
        "            x_cat_np = arr[:, self.cat_idx].astype(\"int64\") if len(self.cat_idx) > 0 else np.zeros((arr.shape[0], 0), dtype=\"int64\")\n",
        "            x_num_np = arr[:, self.cont_idx].astype(\"float32\") if len(self.cont_idx) > 0 else None\n",
        "            x_text_np = arr[:, self.text_idx].astype(\"int64\") if len(self.text_idx) > 0 else None\n",
        "\n",
        "        x_cat  = torch.tensor(x_cat_np, dtype=torch.long) if x_cat_np is not None else None\n",
        "        x_num  = torch.tensor(x_num_np, dtype=torch.float32) if x_num_np is not None else None\n",
        "        x_text = torch.tensor(x_text_np, dtype=torch.long) if x_text_np is not None else None\n",
        "        return x_cat, x_num, x_text\n",
        "\n",
        "    def _build_model(self):\n",
        "        n_num = len(self.cont_idx)\n",
        "        m = TabularTextTransformerMono(\n",
        "            cat_cardinalities=self.cat_cardinalities,\n",
        "            n_num=n_num,\n",
        "            vocab_size=self.vocab_size,\n",
        "            d_model=self.d_model,\n",
        "            n_heads=self.n_heads,\n",
        "            n_layers=self.n_layers,\n",
        "            dim_feedforward=self.dim_feedforward,\n",
        "            dropout=self.dropout,\n",
        "            num_quantiles=self.num_quantiles,\n",
        "            max_text_len=self.max_text_len + 1,  # [CLS] 포함 여유\n",
        "        )\n",
        "        return m.to(self.device)\n",
        "\n",
        "    # public API\n",
        "    def fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None,\n",
        "            max_epochs=10, patience=None, batch_size=256, num_workers=0, verbose=True):\n",
        "\n",
        "        # auto preprocess\n",
        "        if self.auto_preprocess:\n",
        "            self.preproc = TTTPreprocessor(\n",
        "                categorical_indices=self.categorical_indices,\n",
        "                text_indices=self.text_indices,\n",
        "                use_oov=self.use_oov,\n",
        "                max_text_len=self.max_text_len,\n",
        "                min_freq=self.min_freq,\n",
        "                max_vocab=self.max_vocab,\n",
        "            )\n",
        "            self.preproc.fit(X, s=self.quantiles_s)\n",
        "            self.cat_idx = self.preproc.cat_idx\n",
        "            self.cont_idx = self.preproc.cont_idx\n",
        "            self.text_idx = self.preproc.text_indices\n",
        "            self.cat_cardinalities = self.preproc.cardinalities\n",
        "            self.num_quantiles = self.preproc.num_quantiles\n",
        "            self.vocab_size = self.preproc.tok.vocab_size\n",
        "\n",
        "        # CPU tensors\n",
        "        x_cat, x_num, x_text = self._split_to_cpu_tensors(X)\n",
        "        y_t = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
        "        w_t = torch.tensor(sample_weight, dtype=torch.float32).view(-1, 1) if sample_weight is not None else torch.ones_like(y_t)\n",
        "\n",
        "        # validation\n",
        "        if eval_set is not None:\n",
        "            Xv, yv = eval_set[0]\n",
        "            xc_v, xn_v, xt_v = self._split_to_cpu_tensors(Xv)\n",
        "            yv_t = torch.tensor(yv, dtype=torch.float32).view(-1, 1)\n",
        "        else:\n",
        "            xc_v = xn_v = xt_v = yv_t = None\n",
        "\n",
        "        if self.model is None:\n",
        "            self.model = self._build_model()\n",
        "\n",
        "        loss_fn = self._define_loss()\n",
        "        opt = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        # datasets/dataloaders (CPU)\n",
        "        if x_num is None and x_text is None:\n",
        "            train_ds = TensorDataset(x_cat, y_t, w_t)\n",
        "            def itr(dl):\n",
        "                for a, yb, wb in dl: yield a, None, None, yb, wb\n",
        "        elif x_num is None:\n",
        "            train_ds = TensorDataset(x_cat, x_text, y_t, w_t)\n",
        "            def itr(dl):\n",
        "                for a, t, yb, wb in dl: yield a, None, t, yb, wb\n",
        "        elif x_text is None:\n",
        "            train_ds = TensorDataset(x_cat, x_num, y_t, w_t)\n",
        "            def itr(dl):\n",
        "                for a, n, yb, wb in dl: yield a, n, None, yb, wb\n",
        "        else:\n",
        "            train_ds = TensorDataset(x_cat, x_num, x_text, y_t, w_t)\n",
        "            def itr(dl):\n",
        "                for a, n, t, yb, wb in dl: yield a, n, t, yb, wb\n",
        "\n",
        "        pin = (self.device == \"cuda\")\n",
        "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
        "                              num_workers=num_workers, pin_memory=pin)\n",
        "\n",
        "        best, pc = float(\"inf\"), 0\n",
        "        for ep in range(max_epochs):\n",
        "            self.model.train()\n",
        "            acc_loss, steps = 0.0, 0\n",
        "            for a, n, t, yb, wb in itr(train_dl):\n",
        "                # move to device\n",
        "                if a is not None: a = a.to(self.device, non_blocking=True)\n",
        "                if n is not None: n = n.to(self.device, non_blocking=True)\n",
        "                if t is not None: t = t.to(self.device, non_blocking=True)\n",
        "                yb = yb.to(self.device, non_blocking=True)\n",
        "                wb = wb.to(self.device, non_blocking=True)\n",
        "\n",
        "                opt.zero_grad()\n",
        "                p, pt, px = self.model(a, n, t)\n",
        "                l = 0.5 * (loss_fn(pt, yb) + loss_fn(px, yb))  # dual loss\n",
        "                wloss = (l * wb).sum() / wb.sum()\n",
        "                wloss.backward()\n",
        "                opt.step()\n",
        "                acc_loss += wloss.item(); steps += 1\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Epoch {ep+1}/{max_epochs} - train_loss: {acc_loss/max(1,steps):.6f}\")\n",
        "\n",
        "            # validation\n",
        "            if eval_set is not None:\n",
        "                self.model.eval()\n",
        "                with torch.no_grad():\n",
        "                    if xn_v is None and xt_v is None:\n",
        "                        vds = TensorDataset(xc_v, yv_t)\n",
        "                        vdl = DataLoader(vds, batch_size=2048, shuffle=False, pin_memory=pin)\n",
        "                        ev, c = 0.0, 0\n",
        "                        for a, yb in vdl:\n",
        "                            a = a.to(self.device, non_blocking=True)\n",
        "                            yb = yb.to(self.device, non_blocking=True)\n",
        "                            _, pt, px = self.model(a, None, None)\n",
        "                            l = 0.5*(loss_fn(pt, yb) + loss_fn(px, yb))\n",
        "                            ev += (l.sum()/len(l)).item(); c += 1\n",
        "                    elif xn_v is None:\n",
        "                        vds = TensorDataset(xc_v, xt_v, yv_t)\n",
        "                        vdl = DataLoader(vds, batch_size=2048, shuffle=False, pin_memory=pin)\n",
        "                        ev, c = 0.0, 0\n",
        "                        for a, t, yb in vdl:\n",
        "                            a = a.to(self.device, non_blocking=True)\n",
        "                            t = t.to(self.device, non_blocking=True)\n",
        "                            yb = yb.to(self.device, non_blocking=True)\n",
        "                            _, pt, px = self.model(a, None, t)\n",
        "                            l = 0.5*(loss_fn(pt, yb) + loss_fn(px, yb))\n",
        "                            ev += (l.sum()/len(l)).item(); c += 1\n",
        "                    elif xt_v is None:\n",
        "                        vds = TensorDataset(xc_v, xn_v, yv_t)\n",
        "                        vdl = DataLoader(vds, batch_size=2048, shuffle=False, pin_memory=pin)\n",
        "                        ev, c = 0.0, 0\n",
        "                        for a, n, yb in vdl:\n",
        "                            a = a.to(self.device, non_blocking=True)\n",
        "                            n = n.to(self.device, non_blocking=True)\n",
        "                            yb = yb.to(self.device, non_blocking=True)\n",
        "                            _, pt, px = self.model(a, n, None)\n",
        "                            l = 0.5*(loss_fn(pt, yb) + loss_fn(px, yb))\n",
        "                            ev += (l.sum()/len(l)).item(); c += 1\n",
        "                    else:\n",
        "                        vds = TensorDataset(xc_v, xn_v, xt_v, yv_t)\n",
        "                        vdl = DataLoader(vds, batch_size=2048, shuffle=False, pin_memory=pin)\n",
        "                        ev, c = 0.0, 0\n",
        "                        for a, n, t, yb in vdl:\n",
        "                            a = a.to(self.device, non_blocking=True)\n",
        "                            n = n.to(self.device, non_blocking=True)\n",
        "                            t = t.to(self.device, non_blocking=True)\n",
        "                            yb = yb.to(self.device, non_blocking=True)\n",
        "                            _, pt, px = self.model(a, n, t)\n",
        "                            l = 0.5*(loss_fn(pt, yb) + loss_fn(px, yb))\n",
        "                            ev += (l.sum()/len(l)).item(); c += 1\n",
        "                    ev /= max(1, c)\n",
        "                    if verbose:\n",
        "                        print(f\"          val_loss: {ev:.6f}\")\n",
        "                    if patience is not None:\n",
        "                        if ev < best:\n",
        "                            best = ev; pc = 0\n",
        "                            self.best_model_state = {k: v.detach().cpu().clone() for k, v in self.model.state_dict().items()}\n",
        "                        else:\n",
        "                            pc += 1\n",
        "                            if pc >= patience:\n",
        "                                if verbose:\n",
        "                                    print(f\"Early stopping at epoch {ep+1}\")\n",
        "                                break\n",
        "\n",
        "        if self.best_model_state is not None:\n",
        "            self.model.load_state_dict(self.best_model_state)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X, batch_size=2048):\n",
        "        xc, xn, xt = self._split_to_cpu_tensors(X)\n",
        "        pin = (self.device == \"cuda\")\n",
        "\n",
        "        if xn is None and xt is None:\n",
        "            ds = TensorDataset(xc)\n",
        "            def itr(dl):\n",
        "                for (a,) in dl: yield a, None, None\n",
        "        elif xn is None:\n",
        "            ds = TensorDataset(xc, xt)\n",
        "            def itr(dl):\n",
        "                for a, t in dl: yield a, None, t\n",
        "        elif xt is None:\n",
        "            ds = TensorDataset(xc, xn)\n",
        "            def itr(dl):\n",
        "                for a, n in dl: yield a, n, None\n",
        "        else:\n",
        "            ds = TensorDataset(xc, xn, xt)\n",
        "            def itr(dl):\n",
        "                for a, n, t in dl: yield a, n, t\n",
        "\n",
        "        dl = DataLoader(ds, batch_size=batch_size, shuffle=False, pin_memory=pin)\n",
        "        self.model.eval()\n",
        "        outs = []\n",
        "        with torch.no_grad():\n",
        "            for a, n, t in itr(dl):\n",
        "                if a is not None: a = a.to(self.device, non_blocking=True)\n",
        "                if n is not None: n = n.to(self.device, non_blocking=True)\n",
        "                if t is not None: t = t.to(self.device, non_blocking=True)\n",
        "                p, _, _ = self.model(a, n, t)\n",
        "                outs.append(p.detach().cpu())\n",
        "        p = torch.cat(outs, dim=0).numpy().astype(\"float\")\n",
        "        return np.hstack([1.0 - p, p])\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self.predict_proba(X)[:, 1] >= 0.5).astype(int)\n",
        "\n",
        "    def predict_with_uncertainty_flag(self, X, batch_size=2048):\n",
        "        xc, xn, xt = self._split_to_cpu_tensors(X)\n",
        "        pin = (self.device == \"cuda\")\n",
        "\n",
        "        if xn is None and xt is None:\n",
        "            ds = TensorDataset(xc)\n",
        "            def itr(dl):\n",
        "                for (a,) in dl: yield a, None, None\n",
        "        elif xn is None:\n",
        "            ds = TensorDataset(xc, xt)\n",
        "            def itr(dl):\n",
        "                for a, t in dl: yield a, None, t\n",
        "        elif xt is None:\n",
        "            ds = TensorDataset(xc, xn)\n",
        "            def itr(dl):\n",
        "                for a, n in dl: yield a, n, None\n",
        "        else:\n",
        "            ds = TensorDataset(xc, xn, xt)\n",
        "            def itr(dl):\n",
        "                for a, n, t in dl: yield a, n, t\n",
        "\n",
        "        dl = DataLoader(ds, batch_size=batch_size, shuffle=False, pin_memory=pin)\n",
        "        self.model.eval()\n",
        "        ps, tabs, texts = [], [], []\n",
        "        loss_fn = self._define_loss()\n",
        "        with torch.no_grad():\n",
        "            for a, n, t in itr(dl):\n",
        "                if a is not None: a = a.to(self.device, non_blocking=True)\n",
        "                if n is not None: n = n.to(self.device, non_blocking=True)\n",
        "                if t is not None: t = t.to(self.device, non_blocking=True)\n",
        "                p, pt, px = self.model(a, n, t)\n",
        "                ps.append(p.detach().cpu()); tabs.append(pt.detach().cpu()); texts.append(px.detach().cpu())\n",
        "        p = torch.cat(ps).numpy().reshape(-1)\n",
        "        pt = torch.cat(tabs).numpy().reshape(-1)\n",
        "        px = torch.cat(texts).numpy().reshape(-1)\n",
        "        disagree = ((pt >= 0.5).astype(int) != (px >= 0.5).astype(int))\n",
        "        return p, disagree\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 4) 샘플 데이터 생성 (텍스트 포함)\n",
        "# =========================\n",
        "def make_mixed_text_sample(n_samples=40000, seed=7):\n",
        "    rng = np.random.RandomState(seed)\n",
        "\n",
        "    genders = np.array([\"남성\", \"여성\", \"기타\"], dtype=object)\n",
        "    cities  = np.array([\"서울\",\"부산\",\"대구\",\"인천\",\"수원\",\"고양\"], dtype=object)\n",
        "    devices = np.array([\"ios\",\"android\",\"web\"], dtype=object)\n",
        "\n",
        "    gender_col = rng.choice(genders, size=n_samples, p=[0.48,0.48,0.04])\n",
        "    city_col   = rng.choice(cities , size=n_samples)\n",
        "    device_col = rng.choice(devices, size=n_samples, p=[0.35,0.55,0.10])\n",
        "\n",
        "    n_cont = 10\n",
        "    X_cont = rng.randn(n_samples, n_cont).astype(\"float32\")\n",
        "\n",
        "    adjs = [\"빠른\",\"튼튼한\",\"가벼운\",\"비싼\",\"저렴한\",\"세련된\",\"불편한\",\"편안한\",\"예쁜\",\"심플한\",\"강력한\"]\n",
        "    nouns= [\"자켓\",\"코트\",\"셔츠\",\"신발\",\"가방\",\"지갑\",\"바지\",\"스웨터\",\"시계\",\"안경\"]\n",
        "    sentiments = [\"최고\",\"별로\",\"만족\",\"불만\",\"추천\",\"애매\"]\n",
        "    def make_text():\n",
        "        k = rng.randint(5, 16)\n",
        "        toks = []\n",
        "        for _ in range(k):\n",
        "            bucket = rng.choice([0,1,2], p=[0.5,0.35,0.15])\n",
        "            if bucket == 0: toks.append(rng.choice(adjs))\n",
        "            elif bucket == 1: toks.append(rng.choice(nouns))\n",
        "            else: toks.append(rng.choice(sentiments))\n",
        "        return \" \".join(toks)\n",
        "\n",
        "    reviews = np.array([make_text() for _ in range(n_samples)], dtype=object)\n",
        "    titles  = np.array([rng.choice(adjs) + \" \" + rng.choice(nouns) for _ in range(n_samples)], dtype=object)\n",
        "\n",
        "    w_gender = {g: w for g, w in zip(genders, rng.uniform(-0.8, 0.8, len(genders)))}\n",
        "    w_city   = {c: w for c, w in zip(cities , rng.uniform(-0.6, 1.0, len(cities )))}\n",
        "    w_device = {d: w for d, w in zip(devices, rng.uniform(-0.5, 0.9, len(devices)))}\n",
        "    w_cont   = rng.randn(n_cont).astype(\"float32\")\n",
        "\n",
        "    score_cat = (np.vectorize(w_gender.get)(gender_col)\n",
        "                 + np.vectorize(w_city.get)(city_col)\n",
        "                 + np.vectorize(w_device.get)(device_col)).astype(\"float32\")\n",
        "    score_cont = (X_cont * w_cont).sum(axis=1).astype(\"float32\")\n",
        "    score_text = np.array([1.0 if \"최고\" in r or \"추천\" in r else (-0.7 if \"불만\" in r else 0.0) for r in reviews], dtype=\"float32\")\n",
        "\n",
        "    bias, noise = 0.1, rng.normal(scale=0.5, size=n_samples).astype(\"float32\")\n",
        "    logit = 0.6*score_cat + 0.8*score_cont + 0.5*score_text + bias + noise\n",
        "    prob = 1/(1+np.exp(-logit))\n",
        "    y = (prob > 0.5).astype(\"int64\")\n",
        "\n",
        "    X = np.empty((n_samples, 5 + n_cont), dtype=object)\n",
        "    X[:, 0] = gender_col\n",
        "    X[:, 1] = city_col\n",
        "    X[:, 2] = device_col\n",
        "    X[:, 3] = titles\n",
        "    X[:, 4] = reviews\n",
        "    X[:, 5:] = X_cont\n",
        "\n",
        "    categorical_feature_indices = [0,1,2]\n",
        "    text_feature_indices = [3,4]\n",
        "    return X, y, categorical_feature_indices, text_feature_indices\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 5) 데모 (학습/평가)\n",
        "# =========================\n",
        "def train_and_evaluate_demo():\n",
        "    np.random.seed(0); torch.manual_seed(0)\n",
        "\n",
        "    X, y, cat_idx, text_idx = make_mixed_text_sample(n_samples=30000, seed=123)\n",
        "\n",
        "    N = X.shape[0]; idx = np.arange(N); np.random.shuffle(idx)\n",
        "    tr_end = int(N*0.7); va_end = int(N*0.85)\n",
        "    tr, va, te = idx[:tr_end], idx[tr_end:va_end], idx[va_end:]\n",
        "    X_tr, y_tr = X[tr], y[tr]\n",
        "    X_va, y_va = X[va], y[va]\n",
        "    X_te, y_te = X[te], y[te]\n",
        "\n",
        "    clf = TTTBinaryClassifier(\n",
        "        auto_preprocess=True,\n",
        "        categorical_indices=cat_idx,\n",
        "        text_indices=text_idx,\n",
        "        use_oov=True,\n",
        "        max_text_len=96,\n",
        "        min_freq=2,\n",
        "        max_vocab=40000,\n",
        "        quantiles_s=6,\n",
        "        d_model=128, n_heads=4, n_layers=2, dim_feedforward=256, dropout=0.1,\n",
        "        lr=1e-3, weight_decay=1e-4,\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric=[\"logloss\"],\n",
        "        max_epochs=10,\n",
        "        patience=2,\n",
        "        batch_size=512,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    proba = clf.predict_proba(X_te)[:, 1]\n",
        "    pred  = (proba >= 0.5).astype(int)\n",
        "    acc = accuracy_score(y_te, pred)\n",
        "    auc = roc_auc_score(y_te, proba)\n",
        "    ll  = log_loss(y_te, np.vstack([1-proba, proba]).T)\n",
        "\n",
        "    print(\"\\n===== Test Metrics =====\")\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"ROC-AUC  : {auc:.4f}\")\n",
        "    print(f\"Logloss  : {ll:.4f}\")\n",
        "\n",
        "    p_avg, disagree = clf.predict_with_uncertainty_flag(X_te)\n",
        "    print(f\"Uncertainty rate (stream disagreement): {disagree.mean():.3f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate_demo()"
      ]
    }
  ]
}