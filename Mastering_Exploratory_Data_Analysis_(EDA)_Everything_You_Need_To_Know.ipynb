{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYb5WZ0/+4NpempGniBTpR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/data-and-beyond/mastering-exploratory-data-analysis-eda-everything-you-need-to-know-7e3b48d63a95)"
      ],
      "metadata": {
        "id": "hLEScDjJcF-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pure Understanding of Original Data"
      ],
      "metadata": {
        "id": "UsL7w4zucJeL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-Uf_4uX1XSqN"
      },
      "outputs": [],
      "source": [
        "def column_summary(df):\n",
        "    summary_data = []\n",
        "\n",
        "    for col_name in df.columns:\n",
        "        col_dtype = df[col_name].dtype\n",
        "        num_of_nulls = df[col_name].isnull().sum()\n",
        "        num_of_non_nulls = df[col_name].notnull().sum()\n",
        "        num_of_distinct_values = df[col_name].nunique()\n",
        "\n",
        "        if num_of_distinct_values <= 10:\n",
        "            distinct_values_counts = df[col_name].value_counts().to_dict()\n",
        "        else:\n",
        "            top_10_values_counts = df[col_name].value_counts().head(10).to_dict()\n",
        "            distinct_values_counts = {k: v for k, v in sorted(top_10_values_counts.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "        summary_data.append({\n",
        "            'col_name': col_name,\n",
        "            'col_dtype': col_dtype,\n",
        "            'num_of_nulls': num_of_nulls,\n",
        "            'num_of_non_nulls': num_of_non_nulls,\n",
        "            'num_of_distinct_values': num_of_distinct_values,\n",
        "            'distinct_values_counts': distinct_values_counts\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    return summary_df\n",
        "\n",
        "# Example usage:\n",
        "# Assuming df is your DataFrame\n",
        "summary_df = column_summary(df)\n",
        "display(summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "print(df.head())\n",
        "print(df.describe())\n",
        "print(df.duplicated().sum())"
      ],
      "metadata": {
        "id": "khHtH_VdcLZq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify numerical columns\n",
        "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Perform univariate analysis on numerical columns\n",
        "for column in numerical_columns:\n",
        "    # For continuous variables\n",
        "    if len(df[column].unique()) > 10:  # Assuming if unique values > 10, consider it continuous\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.histplot(df[column], kde=True)\n",
        "        plt.title(f'Histogram of {column}')\n",
        "        plt.xlabel(column)\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.show()\n",
        "    else:  # For discrete or ordinal variables\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        ax = sns.countplot(x=column, data=df)\n",
        "        plt.title(f'Count of {column}')\n",
        "        plt.xlabel(column)\n",
        "        plt.ylabel('Count')\n",
        "\n",
        "        # Annotate each bar with its count\n",
        "        for p in ax.patches:\n",
        "            ax.annotate(format(p.get_height(), '.0f'),\n",
        "                        (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                        ha = 'center', va = 'center',\n",
        "                        xytext = (0, 5),\n",
        "                        textcoords = 'offset points')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "ZBJnZyclcPFl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation of Original Data"
      ],
      "metadata": {
        "id": "94dNkn4acRyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Rename the column names for familiarity\n",
        "# This is if there is no requirement to use back the same column names.\n",
        "# This is also only done if there is no pre-existing format, or if the col names don't follow conventional format.\n",
        "# Normally will follow feature mart / dept format to name columns for easy understanding across board.\n",
        "\n",
        "df_l1 = df.copy()\n",
        "df_l1.rename(columns=lambda x: x.lower().replace(' ', '_'), inplace=True)\n",
        "new_col_dict = {'pc': 'c_pc', 'incm_typ': 'c_incm_typ', 'gn_occ': 'c_occ',\n",
        "                 'num_prd': 'prod_nos', 'casatd_cnt': 'casa_td_nos', 'mthcasa': 'casa_bal_avg_mth',\n",
        "                 'maxcasa': 'casa_bal_max_yr', 'mincasa': 'casa_bal_min_yr', 'drvcr': 'dr_cr_ratio_yr',\n",
        "                 'mthtd': 'td_bal_avg', 'maxtd': 'td_bal_max', 'asset_value': 'asset_tot_val',\n",
        "                 'hl_tag': 'loan_home_tag', 'al_tag': 'loan_auto_tag', 'pur_price_avg': 'prop_pur_price',\n",
        "                 'ut_ave': 'ut_avg', 'maxut': 'ut_max', 'n_funds': 'funds_nos',\n",
        "                 'cc_ave': 'cc_out_bal_avg_mth', 'max_mth_trn_amt': 'cc_txn_amt_max_mth', 'min_mth_trn_amt': 'cc_txn_amt_min_mth',\n",
        "                 'avg_trn_amt': 'cc_txn_amt_avg_mth', 'ann_trn_amt': 'cc_txn_amt_yr', 'ann_n_trx': 'cc_txn_nos_yr'}\n",
        "df_l1.rename(columns=new_col_dict, inplace=True)"
      ],
      "metadata": {
        "id": "4fyb26lNcP4f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the boxplot\n",
        "plt.figure(figsize=(10, 6))  # Set the size of the plot\n",
        "sns.boxplot(x='c_incm_typ', y='casa_bal_max_yr', data=df_l1)\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Income Type')\n",
        "plt.ylabel('casa_bal_max_yr')\n",
        "plt.title('Boxplot of casa_bal_max_yr by Income Type')\n",
        "plt.yscale('log')\n",
        "\n",
        "# Show the plot\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kL2E9Z_ocTlw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = df_l1[['prop_pur_price','loan_home_tag']]\n",
        "null_loan_home = new_df[new_df['loan_home_tag'].isnull()]\n",
        "not_null_count = null_loan_home[~null_loan_home[['prop_pur_price']].isnull().any(axis=1)].shape[0]\n",
        "print(\"Number of rows where 'loan_home_tag' is null, but 'prop_pur_price' is not null:\", not_null_count)\n",
        "\n",
        "new_df = df_l1[['prop_pur_price','loan_home_tag']]\n",
        "null_loan_home = new_df[new_df['prop_pur_price'].isnull()]\n",
        "not_null_count = null_loan_home[~null_loan_home[['loan_home_tag']].isnull().any(axis=1)].shape[0]\n",
        "print(\"Number of rows where 'prop_pur_price' is null, but 'loan_home_tag' is not null:\", not_null_count)\n",
        "\n",
        "new_df = df_l1[['prop_pur_price','loan_home_tag']]\n",
        "condition = new_df['loan_home_tag'] == 1\n",
        "new_df[condition].describe()"
      ],
      "metadata": {
        "id": "Q1_qXXs6cV8Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols = ['c_age', 'prod_nos',\n",
        "                  'casa_td_nos', 'casa_bal_avg_mth', 'casa_bal_max_yr', 'casa_bal_min_yr',\n",
        "                  'dr_cr_ratio_yr', 'td_bal_avg', 'td_bal_max', 'asset_tot_val',\n",
        "                  'prop_pur_price', 'ut_avg', 'ut_max', 'funds_nos',\n",
        "                  'cc_out_bal_avg_mth', 'cc_txn_amt_max_mth', 'cc_txn_amt_min_mth', 'cc_txn_amt_avg_mth',\n",
        "                  'cc_txn_amt_yr', 'cc_txn_nos_yr', 'cc_lmt']\n",
        "categorical_cols = ['c_edu_encoded', 'c_hse_encoded', 'c_pc', 'c_incm_typ', 'c_occ_encoded',\n",
        "                    'loan_home_tag', 'loan_auto_tag']\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "correlation_matrix = df_l2[numerical_cols].corr()\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(20, 16))  # Set the size of the plot\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "\n",
        "# Set title\n",
        "plt.title('Correlation Heatmap')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find the max correlation\n",
        "upper_triangular = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "max_correlation = upper_triangular.max().max()\n",
        "print(f\"Maximum pairwise correlation: {max_correlation:.2f}\")"
      ],
      "metadata": {
        "id": "_W2JBz3hcXx-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def corr_v(df_input, col1, col2):\n",
        "    correlation_value = df_input[col1].corr(df_input[col2])\n",
        "    return f\"Correlation value between {col1} and {col2} is: {correlation_value}\"\n",
        "\n",
        "print(corr_v(df_l2, 'casa_bal_avg_mth', 'casa_bal_max_yr'))\n",
        "print(corr_v(df_l2, 'td_bal_avg', 'td_bal_max'))\n",
        "print(corr_v(df_l2, 'ut_avg', 'ut_max'))\n",
        "print(corr_v(df_l2, 'cc_txn_amt_max_mth', 'cc_txn_amt_yr'))\n",
        "print(corr_v(df_l2, 'cc_txn_amt_avg_mth', 'cc_txn_amt_yr'))"
      ],
      "metadata": {
        "id": "PQ-J-qV-cdSx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iv_woe(data, target, bins=10, show_woe=False):\n",
        "\n",
        "    #Empty Dataframe\n",
        "    newDF,woeDF = pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    #Extract Column Names\n",
        "    cols = data.columns\n",
        "\n",
        "    #Run WOE and IV on all the independent variables\n",
        "    for ivars in cols[~cols.isin([target])]:\n",
        "        print(\"Processing variable:\", ivars)\n",
        "        if (data[ivars].dtype.kind in 'bifc') and (len(np.unique(data[ivars]))>10):\n",
        "            binned_x = pd.qcut(data[ivars], bins,  duplicates='drop')\n",
        "            d0 = pd.DataFrame({'x': binned_x, 'y': data[target]})\n",
        "        else:\n",
        "            d0 = pd.DataFrame({'x': data[ivars], 'y': data[target]})\n",
        "\n",
        "\n",
        "        # Calculate the number of events in each group (bin)\n",
        "        d = d0.groupby(\"x\", as_index=False).agg({\"y\": [\"count\", \"sum\"]})\n",
        "        d.columns = ['Cutoff', 'N', 'Events']\n",
        "\n",
        "        # Calculate % of events in each group.\n",
        "        d['% of Events'] = np.maximum(d['Events'], 0.5) / d['Events'].sum()\n",
        "\n",
        "        # Calculate the non events in each group.\n",
        "        d['Non-Events'] = d['N'] - d['Events']\n",
        "        # Calculate % of non events in each group.\n",
        "        d['% of Non-Events'] = np.maximum(d['Non-Events'], 0.5) / d['Non-Events'].sum()\n",
        "\n",
        "        # Calculate WOE by taking natural log of division of % of non-events and % of events\n",
        "        d['WoE'] = np.log(d['% of Events']/d['% of Non-Events'])\n",
        "        d['IV'] = d['WoE'] * (d['% of Events'] - d['% of Non-Events'])\n",
        "        d.insert(loc=0, column='Variable', value=ivars)\n",
        "        print(\"Information value of \" + ivars + \" is \" + str(round(d['IV'].sum(),6)))\n",
        "        temp =pd.DataFrame({\"Variable\" : [ivars], \"IV\" : [d['IV'].sum()]}, columns = [\"Variable\", \"IV\"])\n",
        "        newDF=pd.concat([newDF,temp], axis=0)\n",
        "        woeDF=pd.concat([woeDF,d], axis=0)\n",
        "\n",
        "        #Show WOE Table\n",
        "        if show_woe == True:\n",
        "            print(d)\n",
        "    return newDF, woeDF\n",
        "\n",
        "numerical_cols = ['c_age', 'prod_nos',\n",
        "                  'casa_td_nos', 'casa_bal_avg_mth', 'casa_bal_max_yr', 'casa_bal_min_yr',\n",
        "                  'dr_cr_ratio_yr', 'td_bal_avg', 'td_bal_max', 'asset_tot_val',\n",
        "                  'prop_pur_price', 'ut_avg', 'ut_max', 'funds_nos',\n",
        "                  'cc_out_bal_avg_mth', 'cc_txn_amt_max_mth', 'cc_txn_amt_min_mth', 'cc_txn_amt_avg_mth',\n",
        "                  'cc_txn_amt_yr', 'cc_txn_nos_yr', 'cc_lmt']\n",
        "categorical_cols = ['c_edu_encoded', 'c_hse_encoded', 'c_pc', 'c_incm_typ', 'c_occ_encoded',\n",
        "                    'loan_home_tag', 'loan_auto_tag']\n",
        "dependent_col = ['c_seg_encoded']\n",
        "all_cols = numerical_cols + categorical_cols + dependent_col\n",
        "\n",
        "IVDF, woeDF = iv_woe(df_l2[all_cols], 'c_seg_encoded', bins=10, show_woe=True)\n",
        "\n",
        "sorted_IVDF = IVDF.sort_values(by='IV', ascending=False)\n",
        "display(sorted_IVDF)"
      ],
      "metadata": {
        "id": "N_nLDPuCcfLi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Base Settings\n",
        "df_l2 = df_l1.copy()\n",
        "numerical_cols = ['c_age', 'prod_nos',\n",
        "                  'casa_td_nos', 'casa_bal_avg_mth', 'casa_bal_max_yr', 'casa_bal_min_yr',\n",
        "                  'dr_cr_ratio_yr', 'td_bal_avg', 'td_bal_max', 'asset_tot_val',\n",
        "                  'prop_pur_price', 'ut_avg', 'ut_max', 'funds_nos',\n",
        "                  'cc_out_bal_avg_mth', 'cc_txn_amt_max_mth', 'cc_txn_amt_min_mth', 'cc_txn_amt_avg_mth',\n",
        "                  'cc_txn_amt_yr', 'cc_txn_nos_yr', 'cc_lmt']\n",
        "categorical_cols = ['c_edu_encoded', 'c_hse_encoded', 'c_pc', 'c_incm_typ', 'c_occ_encoded',\n",
        "                    'loan_home_tag', 'loan_auto_tag']\n",
        "dependent_col = ['c_seg_encoded']\n",
        "independent_col = numerical_cols + categorical_cols\n",
        "all_cols = numerical_cols + categorical_cols + dependent_col\n",
        "\n",
        "# Settings Train / Test Split.\n",
        "# We will not be doing Train / Validation / Test split as this is for feature importance only.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting into Training and Holdout Test Sets\n",
        "# Ensure stratification for now. We will adjust the ratio only later if required.\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_l2[independent_col], df_l2[dependent_col],\\\n",
        "                                                    stratify=df_l2[dependent_col], test_size=0.2, random_state=88)\n",
        "\n",
        "# From Standard Scaler for Numerical Columns (when necessary) Eg. Logistic Regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "preprocessor = ColumnTransformer(\\\n",
        "    transformers=[('num', StandardScaler(), numerical_cols)],\\\n",
        "    remainder='passthrough') # Pass through categorical features unchanged\n",
        "\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=independent_col)\n",
        "X_test_transformed = preprocessor.fit_transform(X_test)\n",
        "X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=independent_col)\n",
        "y_train_transformed = y_train.values.ravel()\n",
        "y_test_transformed = y_test.values.ravel()"
      ],
      "metadata": {
        "id": "rtgG1JjTcicn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for getting feature importance sorted.\n",
        "def feature_importance_sorted(classification_model_input, X_train, y_train, feature_importance_input=None):\n",
        "    if classification_model_input is not None:\n",
        "        some_model = classification_model_input\n",
        "        some_model.fit(X_train, y_train)\n",
        "        feature_importances = some_model.feature_importances_\n",
        "    else:\n",
        "        feature_importances = feature_importance_input\n",
        "    feature_importances_sorted = sorted(zip(X_train.columns, feature_importances), key=lambda x: x[1], reverse=True)\n",
        "    df_feature_importances = pd.DataFrame(feature_importances_sorted, columns=['Feature', 'Importance'])\n",
        "    for feature_name, importance in feature_importances_sorted:\n",
        "        print(f\"Feature {feature_name}: {importance}\")\n",
        "\n",
        "    df_feature_importances['rank'] = range(1, len(df_feature_importances)+1)\n",
        "    return df_feature_importances\n",
        "\n",
        "# Decision Tree Classifier Feature Importance\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dtc_fi = feature_importance_sorted(DecisionTreeClassifier(), X_train, y_train)\n",
        "\n",
        "# Random Forest Classifier Feature Importance\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc_fi = feature_importance_sorted(RandomForestClassifier(), X_train, y_train.values.ravel())\n",
        "\n",
        "# XGB Feature Importance\n",
        "import xgboost as xgb\n",
        "xgb_fi = feature_importance_sorted(xgb.XGBClassifier(), X_train, y_train)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(max_iter=10000)\n",
        "lr.fit(X_train, y_train.values.ravel())\n",
        "feature_importances = lr.coef_[0]  # Assuming binary classification\n",
        "lr_fi = feature_importance_sorted(None, X_train, y_train.values.ravel(), feature_importances)"
      ],
      "metadata": {
        "id": "nfqkE29Icl8T"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtc_fi = dtc_fi.rename(columns={'Importance': 'imp_dtc', 'rank': 'rank_dtc'})\n",
        "rfc_fi = rfc_fi.rename(columns={'Importance': 'imp_rfc', 'rank': 'rank_rfc'})\n",
        "xgb_fi = xgb_fi.rename(columns={'Importance': 'imp_xgb', 'rank': 'rank_xgb'})\n",
        "lr_fi = lr_fi.rename(columns={'Importance': 'imp_lr', 'rank': 'rank_lr'})\n",
        "\n",
        "merged_df = dtc_fi.merge(rfc_fi, on='Feature', how='left')\\\n",
        "                  .merge(xgb_fi, on='Feature', how='left')\\\n",
        "                  .merge(lr_fi, on='Feature', how='left')\n",
        "\n",
        "merged_df"
      ],
      "metadata": {
        "id": "6KbC0G5vcoWR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aff_df = df_l2[df_l2['c_seg_encoded']==1]\n",
        "norm_df = df_l2[df_l2['c_seg_encoded']==0]\n",
        "norm_df_2 = norm_df.sample(frac=0.2, random_state=88)\n",
        "# Using a smaller sample of the norm_df, since original norm_df is 5x bigger.\n",
        "# Don't anticipate much change but just trying.\n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "def individual_t_test(df_1, df_2, listoffeatures, alpha_val):\n",
        "    '''\n",
        "    For continuous variable individual t-tests\n",
        "    '''\n",
        "    newlist = []\n",
        "    for feature in listoffeatures:\n",
        "        fea_1 = df_1[feature]\n",
        "        fea_2 = df_2[feature]\n",
        "\n",
        "        t_stat, p_val = ttest_ind(fea_1, fea_2, equal_var=False)\n",
        "        t_stat1 = f'{t_stat:.3f}'\n",
        "        p_val1 = f'{p_val:.3f}'\n",
        "\n",
        "        if p_val < alpha_val:\n",
        "            sig = 'Significant'\n",
        "        else:\n",
        "            sig = 'Insignificant'\n",
        "\n",
        "        newdict = {'feature': feature, 't_stat': t_stat1,\n",
        "                   'p_value': p_val1, 'significance': sig}\n",
        "        newlist.append(newdict)\n",
        "\n",
        "    df_result = pd.DataFrame(newlist)\n",
        "    return df_result\n",
        "\n",
        "individual_t_test(aff_df, norm_df, numerical_cols, 0.05)\n",
        "\n",
        "individual_t_test(aff_df, norm_df_2, numerical_cols, 0.05)"
      ],
      "metadata": {
        "id": "yigKZcjycp1P"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_l2 = df_l1.copy()\n",
        "numerical_cols = ['c_age', 'prod_nos',\n",
        "                  'casa_td_nos', 'casa_bal_avg_mth', 'casa_bal_max_yr', 'casa_bal_min_yr',\n",
        "                  'dr_cr_ratio_yr', 'td_bal_avg', 'td_bal_max', 'asset_tot_val',\n",
        "                  'prop_pur_price', 'ut_avg', 'ut_max', 'funds_nos',\n",
        "                  'cc_out_bal_avg_mth', 'cc_txn_amt_max_mth', 'cc_txn_amt_min_mth', 'cc_txn_amt_avg_mth',\n",
        "                  'cc_txn_amt_yr', 'cc_txn_nos_yr', 'cc_lmt']\n",
        "categorical_cols = ['c_edu_encoded', 'c_hse_encoded', 'c_pc', 'c_incm_typ', 'c_occ_encoded',\n",
        "                    'loan_home_tag', 'loan_auto_tag']\n",
        "dependent_col = ['c_seg_encoded']\n",
        "independent_col = numerical_cols + categorical_cols\n",
        "all_cols = numerical_cols + categorical_cols + dependent_col\n",
        "\n",
        "for feature in numerical_cols:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    boxplot = sns.boxplot(x='c_seg_encoded', y=feature, data=df_l2)\n",
        "    plt.title(f'Box Plot of {feature} by AFFLUENT / NORMAL')\n",
        "\n",
        "    # Add condition to use log scale if values are greater than 1000\n",
        "    if df_l2[feature].max() > 1000:\n",
        "        boxplot.set_yscale('log')\n",
        "\n",
        "    plt.xlabel('Customer Type')\n",
        "    plt.ylabel(feature)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5ph10RZ9cr4J"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}