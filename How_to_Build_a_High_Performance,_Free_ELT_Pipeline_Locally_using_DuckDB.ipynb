{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjU9AhJqWJB4GrAk1EyO7M"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://levelup.gitconnected.com/how-to-build-a-high-performance-free-elt-pipeline-locally-using-duckdb-8fe9d7235079)"
      ],
      "metadata": {
        "id": "2s92EtlN_CuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "pip install duckdb pandas\n",
        "```\n"
      ],
      "metadata": {
        "id": "liZlHS5S_h_8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-uvt79wP_ArX"
      },
      "outputs": [],
      "source": [
        "import duckdb\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Define constants for file paths\n",
        "DB_PATH = 'urbancycle_warehouse.duckdb'\n",
        "ORDERS_CSV = 'orders_data.csv'\n",
        "PRODUCTS_JSON = 'product_inventory.json'\n",
        "\n",
        "# Assume the two data sources are ORDERS_CSV and PRODUCTS_JSON\n",
        "# Below are what they look like\n",
        "\n",
        "\n",
        "# 1. Orders Data (CSV)\n",
        "csv_content = (\n",
        "        \"order_id,product_sku,quantity,sale_date,customer_id\\n\"\n",
        "        \"1001,TIRE_R_001,2,2023-10-01,C123\\n\"\n",
        "        \"1002,HELMET_A_005,1,2023-10-01,C124\\n\"\n",
        "        \"1003,TIRE_R_001,5,2023-10-02,C125\\n\"\n",
        ")\n",
        "\n",
        "# 2. Products Data (JSON - Simulating API dump)\n",
        "products_data = [\n",
        "        {'sku': 'TIRE_R_001', 'name': 'Road Tire Pro', 'category': 'Wheels', 'unit_price': 50.00, 'is_active': True},\n",
        "        {'sku': 'HELMET_A_005', 'name': 'Aero Helmet 5', 'category': 'Accessories', 'unit_price': 120.00, 'is_active': True}\n",
        "]\n",
        "\n",
        "def run_el_phase():\n",
        "    \"\"\"Connects to DuckDB and loads raw data from CSV and JSON.\"\"\"\n",
        "\n",
        "    print(\"\\n--- Starting EL Phase (Extract & Load) ---\")\n",
        "\n",
        "    # Use 'with' for safe connection management\n",
        "    try:\n",
        "        with duckdb.connect(DB_PATH) as con:\n",
        "            # Create a dedicated schema for raw, untransformed data\n",
        "            con.execute(\"CREATE SCHEMA IF NOT EXISTS raw_layer\")\n",
        "\n",
        "            # 1. Load CSV Orders Data (Native Ingest)\n",
        "            print(f\"Loading {ORDERS_CSV} into raw_layer.orders...\")\n",
        "            con.execute(f\"\"\"\n",
        "                CREATE OR REPLACE TABLE raw_layer.orders AS\n",
        "                SELECT * FROM read_csv_auto('{ORDERS_CSV}')\n",
        "            \"\"\")\n",
        "\n",
        "            # 2. Load JSON Products Data (Native JSON Read)\n",
        "            # We treat the JSON file as a source table.\n",
        "            # The 'UNNEST' function is key to flattening complex arrays in JSON.\n",
        "            print(f\"Loading {PRODUCTS_JSON} into raw_layer.products...\")\n",
        "            con.execute(f\"\"\"\n",
        "                CREATE OR REPLACE TABLE raw_layer.products AS\n",
        "                SELECT\n",
        "                    unnest(json_array_of_objects('{PRODUCTS_JSON}')) as product_data\n",
        "            \"\"\")\n",
        "\n",
        "            # Now, flatten the complex JSON structure into a usable table\n",
        "            con.execute(\"\"\"\n",
        "                CREATE OR REPLACE TABLE raw_layer.products_flat AS\n",
        "                SELECT\n",
        "                    product_data ->> 'sku' AS product_sku,\n",
        "                    product_data ->> 'name' AS product_name,\n",
        "                    product_data ->> 'category' AS product_category,\n",
        "                    (product_data ->> 'unit_price')::FLOAT AS unit_price,\n",
        "                    (product_data ->> 'is_active')::BOOLEAN AS is_active\n",
        "                FROM raw_layer.products\n",
        "            \"\"\")\n",
        "\n",
        "            # Verify the load count\n",
        "            order_count = con.sql(\"SELECT count(order_id) FROM raw_layer.orders\").fetchone()[0]\n",
        "            product_count = con.sql(\"SELECT count(product_sku) FROM raw_layer.products_flat\").fetchone()[0]\n",
        "\n",
        "            print(f\"✅ Loaded {order_count} orders and {product_count} products.\")\n",
        "            return True\n",
        "\n",
        "    except duckdb.Error as e:\n",
        "        print(f\"❌ DuckDB Error during EL phase: {e}\")\n",
        "        return False\n",
        "    except IOError as e:\n",
        "        print(f\"❌ File I/O Error: Ensure {ORDERS_CSV} and {PRODUCTS_JSON} exist. {e}\")\n",
        "        return False\n",
        "\n",
        "def run_t_phase():\n",
        "    \"\"\"Performs transformation and aggregation for the unified report.\"\"\"\n",
        "\n",
        "    print(\"\\n--- Starting T Phase (Transform) ---\")\n",
        "\n",
        "    try:\n",
        "        with duckdb.connect(DB_PATH) as con:\n",
        "            con.execute(\"CREATE SCHEMA IF NOT EXISTS analytical_layer\")\n",
        "\n",
        "            # The transformation query for the report sales by product and category\n",
        "            transformation_query = \"\"\"\n",
        "                -- Create the final report table for the business users\n",
        "                CREATE OR REPLACE TABLE analytical_layer.revenue_by_category AS\n",
        "                SELECT\n",
        "                    p.product_category,\n",
        "                    p.product_name,\n",
        "                    CAST(STRFTIME(o.sale_date, '%Y-%m') AS VARCHAR) AS sales_month,\n",
        "                    SUM(o.quantity) AS total_units_sold,\n",
        "                    SUM(o.quantity * p.unit_price) AS total_revenue_usd,\n",
        "                    COUNT(DISTINCT o.customer_id) AS distinct_customers\n",
        "                FROM raw_layer.orders o\n",
        "                -- Key join operation\n",
        "                JOIN raw_layer.products_flat p ON o.product_sku = p.product_sku\n",
        "                WHERE p.is_active = TRUE -- Only analyze active products\n",
        "                GROUP BY 1, 2, 3\n",
        "                ORDER BY total_revenue_usd DESC;\n",
        "            \"\"\"\n",
        "\n",
        "            con.execute(transformation_query)\n",
        "            print(\"✅ Transformation query executed successfully.\")\n",
        "\n",
        "            # Display the final report\n",
        "            print(\"\\n--- Final UrbanCycle Revenue Report ---\")\n",
        "            result_df = con.sql(\"SELECT * FROM analytical_layer.revenue_by_category\").df()\n",
        "            print(result_df.to_markdown(index=False))\n",
        "\n",
        "            return True\n",
        "\n",
        "    except duckdb.Error as e:\n",
        "        print(f\"❌ DuckDB Error during T phase: {e}. Check your SQL syntax or schema names.\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from datetime import datetime\n",
        "from functools import partial\n",
        "\n",
        "# We use functools.partial to pass the connection functions to the operator\n",
        "# The functions run_el_phase and run_t_phase defined above are used here.\n",
        "\n",
        "def pipeline_start():\n",
        "    print(\"Pipeline started.\")\n",
        "\n",
        "def pipeline_end():\n",
        "    print(\"Pipeline finished. The report is ready in the DuckDB file.\")\n",
        "\n",
        "with DAG(\n",
        "    dag_id='urbancycle_duckdb_elt',\n",
        "    start_date=datetime(2024, 1, 1),\n",
        "    schedule_interval='@daily',\n",
        "    catchup=False,\n",
        "    tags=['duckdb', 'elt', 'revenue_report']\n",
        ") as dag:\n",
        "\n",
        "    start_task = PythonOperator(\n",
        "        task_id='start_pipeline',\n",
        "        python_callable=pipeline_start\n",
        "    )\n",
        "\n",
        "    # Task 1: Extract and Load\n",
        "    el_task = PythonOperator(\n",
        "        task_id='extract_and_load',\n",
        "        python_callable=run_el_phase,\n",
        "        # Airflow will mark the task failed if the function returns False\n",
        "        do_xcom_push=False\n",
        "    )\n",
        "\n",
        "    # Task 2: Transform\n",
        "    t_task = PythonOperator(\n",
        "        task_id='transform_and_report',\n",
        "        python_callable=run_t_phase,\n",
        "        do_xcom_push=False\n",
        "    )\n",
        "\n",
        "    end_task = PythonOperator(\n",
        "        task_id='end_pipeline',\n",
        "        python_callable=pipeline_end\n",
        "    )\n",
        "\n",
        "    # Define the sequence of tasks\n",
        "    start_task >> el_task >> t_task >> end_task"
      ],
      "metadata": {
        "id": "1tQdv289AEP2"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}