{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO48DcX2fIJ1ocZ28GAyxK0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Third Party\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# -----------------------------\n",
        "# TabTransformer Backbone\n",
        "# -----------------------------\n",
        "class TabTransformerBackbone(nn.Module):\n",
        "    \"\"\"\n",
        "    Backbone that contextualizes only categorical columns using a Transformer.\n",
        "    - Sum of per-column embedding and column embedding (tabular positional embedding)\n",
        "    - (Optional) prepend a [CLS] token for pooling\n",
        "    - Contextualize with TransformerEncoder\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        cat_cardinalities,              # List[int], number of unique values per categorical column\n",
        "        d_token=32,                     # token embedding dimension (Transformer d_model)\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,               # PyTorch uses one dropout (applies in both self-attn and FFN)\n",
        "        ff_dropout=0.1,                 # kept for API compatibility (treated same as attn_dropout)\n",
        "        embedding_dropout=0.1,\n",
        "        add_cls=False,                  # if True, prepend a [CLS] token\n",
        "        pooling=\"concat\",               # \"concat\" or \"cls\"\n",
        "        padding_idx=None,               # specify padding index if needed for embeddings\n",
        "        norm_first=True                 # Pre-LN architecture\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert pooling in (\"concat\", \"cls\")\n",
        "        self.n_cat = len(cat_cardinalities)\n",
        "        self.d_token = d_token\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "\n",
        "        if self.n_cat == 0:\n",
        "            # No categorical columns -> act as a dummy pass-through\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "            self.col_embedding = None\n",
        "        else:\n",
        "            # Per-column embeddings\n",
        "            self.cat_embeddings = nn.ModuleList([\n",
        "                nn.Embedding(\n",
        "                    num_embeddings=c + (1 if (padding_idx is None) else 0),\n",
        "                    embedding_dim=d_token,\n",
        "                    padding_idx=None if (padding_idx is None) else padding_idx\n",
        "                )\n",
        "                for c in cat_cardinalities\n",
        "            ])\n",
        "            # Column embeddings (tabular counterpart of positional encoding)\n",
        "            self.col_embedding = nn.Embedding(self.n_cat, d_token)\n",
        "\n",
        "        # (Optional) CLS token\n",
        "        if self.add_cls:\n",
        "            self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n",
        "            nn.init.normal_(self.cls_token, std=0.02)\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout(embedding_dropout)\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_token,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=attn_dropout,   # single dropout parameter in PyTorch\n",
        "            batch_first=True,\n",
        "            norm_first=norm_first\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        # Weight initialization\n",
        "        for emb in self.cat_embeddings:\n",
        "            nn.init.normal_(emb.weight, std=0.02)\n",
        "        if self.col_embedding is not None:\n",
        "            nn.init.normal_(self.col_embedding.weight, std=0.02)\n",
        "\n",
        "    def forward(self, x_cat: torch.LongTensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x_cat: LongTensor of shape (B, n_cat)\n",
        "        Returns:\n",
        "            If pooling='concat' -> FloatTensor of shape (B, n_cat * d_token)\n",
        "            If pooling='cls'    -> FloatTensor of shape (B, d_token)\n",
        "        \"\"\"\n",
        "        B = x_cat.size(0)\n",
        "        if self.n_cat == 0:\n",
        "            # No categorical columns\n",
        "            if self.pooling == \"cls\":\n",
        "                out = torch.zeros(B, self.d_token, device=x_cat.device, dtype=torch.float32)\n",
        "            else:\n",
        "                out = torch.zeros(B, 0, device=x_cat.device, dtype=torch.float32)\n",
        "            return out\n",
        "\n",
        "        tok_list = []\n",
        "        for j, emb in enumerate(self.cat_embeddings):\n",
        "            tok = emb(x_cat[:, j])                         # (B, d)\n",
        "            if self.col_embedding is not None:\n",
        "                tok = tok + self.col_embedding.weight[j]   # add column embedding (d,)\n",
        "            tok_list.append(tok.unsqueeze(1))              # (B, 1, d)\n",
        "\n",
        "        x_tok = torch.cat(tok_list, dim=1)                 # (B, n_cat, d)\n",
        "\n",
        "        if self.add_cls:\n",
        "            cls = self.cls_token.expand(B, -1, -1)         # (B, 1, d)\n",
        "            x_tok = torch.cat([cls, x_tok], dim=1)         # (B, 1 + n_cat, d)\n",
        "\n",
        "        x_tok = self.embedding_dropout(x_tok)              # (B, T, d)\n",
        "        z = self.transformer(x_tok)                        # (B, T, d)\n",
        "\n",
        "        if self.pooling == \"cls\" and self.add_cls:\n",
        "            out = z[:, 0, :]                               # (B, d)\n",
        "        elif self.pooling == \"cls\" and not self.add_cls:\n",
        "            out = z.mean(dim=1)                            # (B, d)\n",
        "        else:\n",
        "            if self.add_cls:\n",
        "                z = z[:, 1:, :]                            # (B, n_cat, d) — drop CLS\n",
        "            out = z.reshape(B, -1)                         # (B, n_cat * d)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Full Model (Backbone + Head)\n",
        "# -----------------------------\n",
        "class TabTransformerModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Full TabTransformer:\n",
        "    - Transformer backbone contextualizes categorical features\n",
        "    - Continuous features are normalized + optionally projected\n",
        "    - Concatenate -> MLP head -> single logit -> sigmoid\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        cat_cardinalities,          # List[int]\n",
        "        n_continuous=0,\n",
        "        d_token=32,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,\n",
        "        ff_dropout=0.1,             # kept for API compatibility\n",
        "        embedding_dropout=0.1,\n",
        "        add_cls=False,\n",
        "        pooling=\"concat\",\n",
        "        cont_proj=\"linear\",         # \"none\" or \"linear\"\n",
        "        mlp_hidden_dims=(128, 64),  # head MLP sizes\n",
        "        mlp_dropout=0.2\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.n_cont = n_continuous\n",
        "        self.cont_proj = cont_proj\n",
        "\n",
        "        # Backbone\n",
        "        self.backbone = TabTransformerBackbone(\n",
        "            cat_cardinalities=cat_cardinalities,\n",
        "            d_token=d_token,\n",
        "            n_heads=n_heads,\n",
        "            n_layers=n_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            attn_dropout=attn_dropout,\n",
        "            ff_dropout=ff_dropout,\n",
        "            embedding_dropout=embedding_dropout,\n",
        "            add_cls=add_cls,\n",
        "            pooling=pooling,\n",
        "            norm_first=True\n",
        "        )\n",
        "\n",
        "        # Continuous feature processing\n",
        "        if n_continuous > 0:\n",
        "            self.cont_bn = nn.BatchNorm1d(n_continuous)\n",
        "            if cont_proj == \"linear\":\n",
        "                self.cont_linear = nn.Linear(n_continuous, d_token)\n",
        "                nn.init.kaiming_uniform_(self.cont_linear.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "                cont_out_dim = d_token\n",
        "            else:\n",
        "                self.cont_linear = nn.Identity()\n",
        "                cont_out_dim = n_continuous\n",
        "        else:\n",
        "            self.cont_bn = None\n",
        "            self.cont_linear = None\n",
        "            cont_out_dim = 0\n",
        "\n",
        "        # Backbone output dimension\n",
        "        if pooling == \"cls\":\n",
        "            backbone_out = d_token\n",
        "        else:\n",
        "            backbone_out = len(cat_cardinalities) * d_token\n",
        "\n",
        "        in_dim = backbone_out + cont_out_dim\n",
        "\n",
        "        # Head MLP\n",
        "        mlp_layers = []\n",
        "        prev = in_dim\n",
        "        for h in mlp_hidden_dims:\n",
        "            lin = nn.Linear(prev, h)\n",
        "            nn.init.kaiming_uniform_(lin.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "            mlp_layers.extend([lin, nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(mlp_dropout)])\n",
        "            prev = h\n",
        "        mlp_layers.append(nn.Linear(prev, 1))\n",
        "        mlp_layers.append(nn.Sigmoid())\n",
        "        self.head = nn.Sequential(*mlp_layers)\n",
        "\n",
        "    def forward(self, x_cat: torch.LongTensor, x_cont: torch.FloatTensor = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x_cat: (B, n_cat) long\n",
        "            x_cont: (B, n_cont) float or None\n",
        "        Returns:\n",
        "            (B, 1) sigmoid probability\n",
        "        \"\"\"\n",
        "        z_cat = self.backbone(x_cat)  # (B, d_backbone)\n",
        "\n",
        "        if (x_cont is not None) and (self.n_cont > 0):\n",
        "            if x_cont.ndim == 1:\n",
        "                x_cont = x_cont.unsqueeze(1)\n",
        "            x_cont = self.cont_bn(x_cont)\n",
        "            x_cont = self.cont_linear(x_cont)\n",
        "            z = torch.cat([z_cat, x_cont], dim=1)\n",
        "        else:\n",
        "            z = z_cat\n",
        "\n",
        "        out = self.head(z)            # (B, 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Sklearn-Compatible Classifier\n",
        "# -----------------------------\n",
        "class TabTransformerBinaryClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    Example:\n",
        "        clf = TabTransformerBinaryClassifier(\n",
        "            cat_idx=[0, 1, 5],                # categorical column indices\n",
        "            cat_cardinalities=[10, 5, 20],    # cardinality per categorical column\n",
        "            cont_idx=[2, 3, 4],               # continuous column indices (or [])\n",
        "            d_token=32, n_heads=4, n_layers=2,\n",
        "            hidden_dims=(128, 64), lr=1e-3\n",
        "        )\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        cat_idx,\n",
        "        cat_cardinalities,\n",
        "        cont_idx=None,\n",
        "        d_token=32,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,\n",
        "        ff_dropout=0.1,\n",
        "        embedding_dropout=0.1,\n",
        "        add_cls=False,\n",
        "        pooling=\"concat\",\n",
        "        cont_proj=\"linear\",\n",
        "        hidden_dims=(128, 64),\n",
        "        mlp_dropout=0.2,\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-4,\n",
        "        loss_fn=\"logloss\",\n",
        "        device=None\n",
        "    ):\n",
        "        self.cat_idx = list(cat_idx)\n",
        "        self.cat_cardinalities = list(cat_cardinalities)\n",
        "        self.cont_idx = list(cont_idx) if cont_idx is not None else []\n",
        "\n",
        "        # Model / training hyperparameters\n",
        "        self.d_token = d_token\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.dim_feedforward = dim_feedforward\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.ff_dropout = ff_dropout\n",
        "        self.embedding_dropout = embedding_dropout\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "        self.cont_proj = cont_proj\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.mlp_dropout = mlp_dropout\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.loss_fn_name = loss_fn\n",
        "\n",
        "        # Internal state\n",
        "        self.model = None\n",
        "        self.best_model_weights = None\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # -------------------------\n",
        "    # Internals\n",
        "    # -------------------------\n",
        "    def _build_model(self):\n",
        "        n_cont = len(self.cont_idx)\n",
        "        model = TabTransformerModel(\n",
        "            cat_cardinalities=self.cat_cardinalities,\n",
        "            n_continuous=n_cont,\n",
        "            d_token=self.d_token,\n",
        "            n_heads=self.n_heads,\n",
        "            n_layers=self.n_layers,\n",
        "            dim_feedforward=self.dim_feedforward,\n",
        "            attn_dropout=self.attn_dropout,\n",
        "            ff_dropout=self.ff_dropout,\n",
        "            embedding_dropout=self.embedding_dropout,\n",
        "            add_cls=self.add_cls,\n",
        "            pooling=self.pooling,\n",
        "            cont_proj=self.cont_proj,\n",
        "            mlp_hidden_dims=self.hidden_dims,\n",
        "            mlp_dropout=self.mlp_dropout\n",
        "        )\n",
        "        return model.to(self.device)\n",
        "\n",
        "    def _define_loss_fn(self):\n",
        "        if self.loss_fn_name == \"logloss\":\n",
        "            return nn.BCELoss(reduction=\"none\")\n",
        "        else:\n",
        "            raise Exception(f\"{self.loss_fn_name} is not defined\")\n",
        "\n",
        "    def _split_X(self, X):\n",
        "        \"\"\"\n",
        "        Split input X (np.ndarray or torch.Tensor) into:\n",
        "        - x_cat: LongTensor (B, n_cat)\n",
        "        - x_cont: FloatTensor (B, n_cont) or None\n",
        "        \"\"\"\n",
        "        if isinstance(X, torch.Tensor):\n",
        "            X_np = X.detach().cpu().numpy()\n",
        "        else:\n",
        "            X_np = X\n",
        "\n",
        "        if len(self.cat_idx) > 0:\n",
        "            x_cat_np = X_np[:, self.cat_idx].astype(\"int64\")\n",
        "        else:\n",
        "            x_cat_np = np.zeros((X_np.shape[0], 0), dtype=\"int64\")\n",
        "\n",
        "        if len(self.cont_idx) > 0:\n",
        "            x_cont_np = X_np[:, self.cont_idx].astype(\"float32\")\n",
        "        else:\n",
        "            x_cont_np = None\n",
        "\n",
        "        x_cat = torch.tensor(x_cat_np, dtype=torch.long, device=self.device)\n",
        "        x_cont = torch.tensor(x_cont_np, dtype=torch.float32, device=self.device) if x_cont_np is not None else None\n",
        "        return x_cat, x_cont\n",
        "\n",
        "    # -------------------------\n",
        "    # Public API\n",
        "    # -------------------------\n",
        "    def fit(\n",
        "        self,\n",
        "        X,\n",
        "        y,\n",
        "        sample_weight=None,\n",
        "        eval_set=None,            # list of tuples: [(X_val, y_val)]\n",
        "        eval_metric=None,         # supports [\"logloss\"] only\n",
        "        max_epochs=10,\n",
        "        patience=None,\n",
        "        batch_size=32,\n",
        "        num_workers=0,\n",
        "        verbose=True,\n",
        "        pin_memory=None           # set True when using CUDA for faster host->device transfer\n",
        "    ):\n",
        "        if pin_memory is None:\n",
        "            pin_memory = (self.device == \"cuda\")\n",
        "\n",
        "        # Prepare tensors\n",
        "        x_cat, x_cont = self._split_X(X)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            w_tensor = torch.tensor(sample_weight, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "        else:\n",
        "            w_tensor = torch.ones_like(y_tensor, dtype=torch.float32, device=self.device)\n",
        "\n",
        "        if eval_set is not None:\n",
        "            X_val, y_val = eval_set[0]\n",
        "            x_cat_val, x_cont_val = self._split_X(X_val)\n",
        "            y_val_tensor = torch.tensor(y_val, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "        else:\n",
        "            x_cat_val = x_cont_val = y_val_tensor = None\n",
        "\n",
        "        # Build model\n",
        "        if self.model is None:\n",
        "            self.model = self._build_model()\n",
        "\n",
        "        loss_fn = self._define_loss_fn()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        # DataLoader\n",
        "        if x_cont is None:\n",
        "            train_dataset = TensorDataset(x_cat, y_tensor, w_tensor)\n",
        "        else:\n",
        "            train_dataset = TensorDataset(x_cat, x_cont, y_tensor, w_tensor)\n",
        "\n",
        "        def _make_train_loader():\n",
        "            return DataLoader(\n",
        "                train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                num_workers=num_workers, pin_memory=pin_memory\n",
        "            )\n",
        "\n",
        "        train_loader = _make_train_loader()\n",
        "\n",
        "        best_loss = float(\"inf\")\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            self.model.train()\n",
        "            epoch_loss = 0.0\n",
        "            n_steps = 0\n",
        "\n",
        "            if x_cont is None:\n",
        "                for Xc_b, y_b, w_b in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    y_pred = self.model(Xc_b, None)\n",
        "                    loss = loss_fn(y_pred, y_b)\n",
        "                    weighted_loss = (loss * w_b).sum() / w_b.sum()\n",
        "                    weighted_loss.backward()\n",
        "                    optimizer.step()\n",
        "                    epoch_loss += weighted_loss.item()\n",
        "                    n_steps += 1\n",
        "            else:\n",
        "                for Xc_b, Xn_b, y_b, w_b in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    y_pred = self.model(Xc_b, Xn_b)\n",
        "                    loss = loss_fn(y_pred, y_b)\n",
        "                    weighted_loss = (loss * w_b).sum() / w_b.sum()\n",
        "                    weighted_loss.backward()\n",
        "                    optimizer.step()\n",
        "                    epoch_loss += weighted_loss.item()\n",
        "                    n_steps += 1\n",
        "\n",
        "            train_avg = epoch_loss / max(1, n_steps)\n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch + 1}/{max_epochs} - train_loss: {train_avg:.6f}\")\n",
        "\n",
        "            # -------------------------\n",
        "            # Validation\n",
        "            # -------------------------\n",
        "            if eval_set is not None:\n",
        "                self.model.eval()\n",
        "                if eval_metric is not None:\n",
        "                    for m in eval_metric:\n",
        "                        if m != \"logloss\":\n",
        "                            raise Exception(f\"{eval_metric} is not defined\")\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    if x_cont_val is None:\n",
        "                        val_dataset = TensorDataset(x_cat_val, y_val_tensor)\n",
        "                        val_loader = DataLoader(\n",
        "                            val_dataset, batch_size=2048, shuffle=False,\n",
        "                            num_workers=num_workers, pin_memory=pin_memory\n",
        "                        )\n",
        "                        eval_loss = 0.0\n",
        "                        n_eval = 0\n",
        "                        for Xc_v, y_v in val_loader:\n",
        "                            y_pred_v = self.model(Xc_v, None)\n",
        "                            loss_v = loss_fn(y_pred_v, y_v)\n",
        "                            eval_loss += (loss_v.sum() / len(loss_v)).item()\n",
        "                            n_eval += 1\n",
        "                    else:\n",
        "                        val_dataset = TensorDataset(x_cat_val, x_cont_val, y_val_tensor)\n",
        "                        val_loader = DataLoader(\n",
        "                            val_dataset, batch_size=2048, shuffle=False,\n",
        "                            num_workers=num_workers, pin_memory=pin_memory\n",
        "                        )\n",
        "                        eval_loss = 0.0\n",
        "                        n_eval = 0\n",
        "                        for Xc_v, Xn_v, y_v in val_loader:\n",
        "                            y_pred_v = self.model(Xc_v, Xn_v)\n",
        "                            loss_v = loss_fn(y_pred_v, y_v)\n",
        "                            eval_loss += (loss_v.sum() / len(loss_v)).item()\n",
        "                            n_eval += 1\n",
        "\n",
        "                    eval_loss = eval_loss / max(1, n_eval)\n",
        "                    if verbose:\n",
        "                        print(f\"          val_loss: {eval_loss:.6f}\")\n",
        "\n",
        "                    if patience is not None:\n",
        "                        if eval_loss < best_loss:\n",
        "                            best_loss = eval_loss\n",
        "                            patience_counter = 0\n",
        "                            self.best_model_weights = {k: v.detach().cpu().clone() for k, v in self.model.state_dict().items()}\n",
        "                        else:\n",
        "                            patience_counter += 1\n",
        "                            if patience_counter >= patience:\n",
        "                                if verbose:\n",
        "                                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                                break\n",
        "\n",
        "        if self.best_model_weights is not None:\n",
        "            self.model.load_state_dict(self.best_model_weights)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        x_cat, x_cont = self._split_X(X)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            probs1 = self.model(x_cat, x_cont).detach().cpu().numpy()  # (B, 1) sigmoid\n",
        "        probs1 = probs1.astype(\"float\")\n",
        "        probs0 = 1.0 - probs1\n",
        "        return np.hstack([probs0, probs1])\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return probs.argmax(axis=1)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Synthetic Data Generator + Training Demo\n",
        "# ============================================================\n",
        "def make_synthetic_tabular(\n",
        "    n_samples=20000,\n",
        "    cat_cardinalities=(12, 7, 25),   # 3 categorical columns\n",
        "    n_cont=4,                        # 4 continuous columns\n",
        "    seed=42\n",
        "):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n_cat = len(cat_cardinalities)\n",
        "\n",
        "    # Generate categorical columns (each column 0..K-1)\n",
        "    X_cat = np.column_stack([\n",
        "        rng.randint(0, c, size=n_samples).astype(\"int64\") for c in cat_cardinalities\n",
        "    ])  # (N, n_cat)\n",
        "\n",
        "    # Generate continuous columns\n",
        "    X_cont = rng.randn(n_samples, n_cont).astype(\"float32\")  # (N, n_cont)\n",
        "\n",
        "    # Latent score: categorical contribution + continuous linear combo + noise\n",
        "    cat_weights = [rng.randn(c) * rng.uniform(0.3, 1.0) for c in cat_cardinalities]\n",
        "    score_cat = np.zeros(n_samples, dtype=\"float32\")\n",
        "    for j in range(n_cat):\n",
        "        score_cat += cat_weights[j][X_cat[:, j]]\n",
        "\n",
        "    w_cont = rng.randn(n_cont).astype(\"float32\")\n",
        "    score_cont = (X_cont * w_cont).sum(axis=1)\n",
        "\n",
        "    bias = 0.2\n",
        "    noise = rng.normal(scale=0.5, size=n_samples).astype(\"float32\")\n",
        "\n",
        "    logit = 0.6 * score_cat + 0.8 * score_cont + bias + noise\n",
        "    prob = 1 / (1 + np.exp(-logit))\n",
        "    y = (prob > 0.5).astype(\"int64\")\n",
        "\n",
        "    # Concatenate as [categorical | continuous]; later split by indices\n",
        "    X = np.concatenate([X_cat.astype(\"float32\"), X_cont], axis=1)\n",
        "    cat_idx = list(range(n_cat))\n",
        "    cont_idx = list(range(n_cat, n_cat + n_cont))\n",
        "\n",
        "    return X, y, cat_idx, cont_idx, list(cat_cardinalities)\n",
        "\n",
        "\n",
        "def train_and_evaluate_demo():\n",
        "    # Reproducibility\n",
        "    np.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # ---------------- Data ----------------\n",
        "    X, y, cat_idx, cont_idx, cat_cardinalities = make_synthetic_tabular(\n",
        "        n_samples=20000, cat_cardinalities=(12, 7, 25), n_cont=4, seed=13\n",
        "    )\n",
        "\n",
        "    # Train/Val/Test split\n",
        "    N = X.shape[0]\n",
        "    idx = np.arange(N)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    tr_end = int(N * 0.7)\n",
        "    va_end = int(N * 0.85)\n",
        "\n",
        "    tr_idx = idx[:tr_end]\n",
        "    va_idx = idx[tr_end:va_end]\n",
        "    te_idx = idx[va_end:]\n",
        "\n",
        "    X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
        "    X_va, y_va = X[va_idx], y[va_idx]\n",
        "    X_te, y_te = X[te_idx], y[te_idx]\n",
        "\n",
        "    # ---------------- Model ----------------\n",
        "    clf = TabTransformerBinaryClassifier(\n",
        "        cat_idx=cat_idx,\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        cont_idx=cont_idx,\n",
        "        d_token=32,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,\n",
        "        embedding_dropout=0.05,\n",
        "        pooling=\"concat\",          # or \"cls\" (with add_cls=True)\n",
        "        add_cls=False,\n",
        "        cont_proj=\"linear\",\n",
        "        hidden_dims=(128, 64),\n",
        "        mlp_dropout=0.2,\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-4,\n",
        "        loss_fn=\"logloss\",\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric=[\"logloss\"],\n",
        "        max_epochs=15,\n",
        "        patience=3,\n",
        "        batch_size=256,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # ---------------- Eval ----------------\n",
        "    proba_va = clf.predict_proba(X_va)[:, 1]\n",
        "    proba_te = clf.predict_proba(X_te)[:, 1]\n",
        "    pred_te = (proba_te >= 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_te, pred_te)\n",
        "    auc = roc_auc_score(y_te, proba_te)\n",
        "    ll  = log_loss(y_te, np.vstack([1 - proba_te, proba_te]).T)\n",
        "\n",
        "    print(\"\\n===== Test Metrics =====\")\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"ROC-AUC  : {auc:.4f}\")\n",
        "    print(f\"Logloss  : {ll:.4f}\")\n",
        "\n",
        "    # Show sample predictions\n",
        "    print(\"\\nSample predictions (first 10):\")\n",
        "    print(np.round(proba_te[:10], 4))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate_demo()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFxkZMZ_p5Nh",
        "outputId": "f59bdc1b-acb8-4029-920b-7bf7cc2fb6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 - train_loss: 0.345680\n",
            "          val_loss: 0.193235\n",
            "Epoch 2/15 - train_loss: 0.179509\n",
            "          val_loss: 0.147929\n",
            "Epoch 3/15 - train_loss: 0.149519\n",
            "          val_loss: 0.130593\n",
            "Epoch 4/15 - train_loss: 0.141352\n",
            "          val_loss: 0.126637\n",
            "Epoch 5/15 - train_loss: 0.138444\n",
            "          val_loss: 0.122914\n",
            "Epoch 6/15 - train_loss: 0.138383\n",
            "          val_loss: 0.123396\n",
            "Epoch 7/15 - train_loss: 0.133943\n",
            "          val_loss: 0.124393\n",
            "Epoch 8/15 - train_loss: 0.134532\n",
            "          val_loss: 0.124173\n",
            "Early stopping at epoch 8\n",
            "\n",
            "===== Test Metrics =====\n",
            "Accuracy : 0.9550\n",
            "ROC-AUC  : 0.9940\n",
            "Logloss  : 0.1048\n",
            "\n",
            "Sample predictions (first 10):\n",
            "[0.0577 0.0019 0.9995 0.     0.9976 0.0043 0.0666 0.9945 0.9992 0.9986]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (중략) 기존 import/모듈 정의 그대로 두고, 아래 부분만 추가/수정합니다.\n",
        "\n",
        "class TabTransformerBinaryClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cat_idx=None,\n",
        "        cat_cardinalities=None,\n",
        "        cont_idx=None,\n",
        "        d_token=32,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,\n",
        "        ff_dropout=0.1,\n",
        "        embedding_dropout=0.1,\n",
        "        add_cls=False,\n",
        "        pooling=\"concat\",\n",
        "        cont_proj=\"linear\",\n",
        "        hidden_dims=(128, 64),\n",
        "        mlp_dropout=0.2,\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-4,\n",
        "        loss_fn=\"logloss\",\n",
        "        device=None,\n",
        "        # --- 새 옵션 ---\n",
        "        auto_preprocess=True,\n",
        "        categorical_indices=None,   # 외부에서 주는 cat 인덱스\n",
        "        use_oov=True                # padding_idx=0 사용 여부\n",
        "    ):\n",
        "        # 기존 필드\n",
        "        self.cat_idx = [] if cat_idx is None else list(cat_idx)\n",
        "        self.cat_cardinalities = None if cat_cardinalities is None else list(cat_cardinalities)\n",
        "        self.cont_idx = [] if cont_idx is None else list(cont_idx)\n",
        "\n",
        "        self.d_token = d_token\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.dim_feedforward = dim_feedforward\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.ff_dropout = ff_dropout\n",
        "        self.embedding_dropout = embedding_dropout\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "        self.cont_proj = cont_proj\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.mlp_dropout = mlp_dropout\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.loss_fn_name = loss_fn\n",
        "\n",
        "        self.model = None\n",
        "        self.best_model_weights = None\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # --- 새 필드 ---\n",
        "        self.auto_preprocess = auto_preprocess\n",
        "        self.categorical_indices = categorical_indices\n",
        "        self.use_oov = use_oov\n",
        "        self.preproc = None  # TabularPreprocessor 인스턴스\n",
        "\n",
        "    def _build_model(self):\n",
        "        n_cont = len(self.cont_idx)\n",
        "        model = TabTransformerModel(\n",
        "            cat_cardinalities=self.cat_cardinalities,   # OOV 제외한 순수 K\n",
        "            n_continuous=n_cont,\n",
        "            d_token=self.d_token,\n",
        "            n_heads=self.n_heads,\n",
        "            n_layers=self.n_layers,\n",
        "            dim_feedforward=self.dim_feedforward,\n",
        "            attn_dropout=self.attn_dropout,\n",
        "            ff_dropout=self.ff_dropout,\n",
        "            embedding_dropout=self.embedding_dropout,\n",
        "            add_cls=self.add_cls,\n",
        "            pooling=self.pooling,\n",
        "            cont_proj=self.cont_proj,\n",
        "            mlp_hidden_dims=self.hidden_dims,\n",
        "            mlp_dropout=self.mlp_dropout\n",
        "        )\n",
        "        # ★ padding_idx=0을 백본에 전달\n",
        "        model.backbone.cat_embeddings = nn.ModuleList([\n",
        "            nn.Embedding(\n",
        "                num_embeddings=c + (1 if self.use_oov else 0),\n",
        "                embedding_dim=self.d_token,\n",
        "                padding_idx=0 if self.use_oov else None\n",
        "            )\n",
        "            for c in self.cat_cardinalities\n",
        "        ])\n",
        "        # column embedding 초기화 재적용\n",
        "        model.backbone.col_embedding = nn.Embedding(len(self.cat_cardinalities), self.d_token)\n",
        "        for emb in model.backbone.cat_embeddings:\n",
        "            nn.init.normal_(emb.weight, std=0.02)\n",
        "        nn.init.normal_(model.backbone.col_embedding.weight, std=0.02)\n",
        "\n",
        "        return model.to(self.device)\n",
        "\n",
        "    def _define_loss_fn(self):\n",
        "        if self.loss_fn_name == \"logloss\":\n",
        "            return nn.BCELoss(reduction=\"none\")\n",
        "        else:\n",
        "            raise Exception(f\"{self.loss_fn_name} is not defined\")\n",
        "\n",
        "    def _split_X(self, X):\n",
        "        \"\"\"\n",
        "        auto_preprocess=True 이면 self.preproc.transform(X) 사용.\n",
        "        아니면 기존 인덱스로 분리.\n",
        "        \"\"\"\n",
        "        if self.auto_preprocess:\n",
        "            assert self.preproc is not None and self.preproc.fitted_, \"Call fit() first.\"\n",
        "            x_cat_np, x_cont_np = self.preproc.transform(X)\n",
        "        else:\n",
        "            X_np = X.detach().cpu().numpy() if isinstance(X, torch.Tensor) else X\n",
        "            if len(self.cat_idx) > 0:\n",
        "                x_cat_np = X_np[:, self.cat_idx].astype(\"int64\")\n",
        "            else:\n",
        "                x_cat_np = np.zeros((X_np.shape[0], 0), dtype=\"int64\")\n",
        "            x_cont_np = (X_np[:, self.cont_idx].astype(\"float32\")\n",
        "                         if len(self.cont_idx) > 0 else None)\n",
        "\n",
        "        x_cat = torch.tensor(x_cat_np, dtype=torch.long, device=self.device)\n",
        "        x_cont = (torch.tensor(x_cont_np, dtype=torch.float32, device=self.device)\n",
        "                  if x_cont_np is not None else None)\n",
        "        return x_cat, x_cont\n",
        "\n",
        "    def fit(\n",
        "        self, X, y, sample_weight=None, eval_set=None, eval_metric=None,\n",
        "        max_epochs=10, patience=None, batch_size=32, num_workers=0,\n",
        "        verbose=True, pin_memory=None\n",
        "    ):\n",
        "        if pin_memory is None:\n",
        "            pin_memory = (self.device == \"cuda\")\n",
        "\n",
        "        # --- (새) 자동 전처리/카디널리티 추론 ---\n",
        "        if self.auto_preprocess:\n",
        "            self.preproc = TabularPreprocessor(\n",
        "                categorical_indices=self.categorical_indices,\n",
        "                use_oov=self.use_oov,\n",
        "                oov_token=0,\n",
        "                add_na_token=True\n",
        "            )\n",
        "            # cat_idx/cont_idx/cardinalities 자동 산출\n",
        "            self.preproc.fit(X)\n",
        "            self.cat_idx = self.preproc.cat_idx\n",
        "            self.cont_idx = self.preproc.cont_idx\n",
        "            self.cat_cardinalities = self.preproc.cardinalities\n",
        "\n",
        "        # 텐서 준비\n",
        "        x_cat, x_cont = self._split_X(X)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "        w_tensor = (torch.tensor(sample_weight, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "                    if sample_weight is not None else torch.ones_like(y_tensor))\n",
        "\n",
        "        # 검증셋 준비\n",
        "        if eval_set is not None:\n",
        "            X_val, y_val = eval_set[0]\n",
        "            x_cat_val, x_cont_val = self._split_X(X_val)\n",
        "            y_val_tensor = torch.tensor(y_val, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "        else:\n",
        "            x_cat_val = x_cont_val = y_val_tensor = None\n",
        "\n",
        "        # 모델 빌드 (padding_idx 포함)\n",
        "        if self.model is None:\n",
        "            self.model = self._build_model()\n",
        "\n",
        "        loss_fn = self._define_loss_fn()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        # DataLoader 구성\n",
        "        if x_cont is None:\n",
        "            train_dataset = TensorDataset(x_cat, y_tensor, w_tensor)\n",
        "            def _iter(loader):\n",
        "                for Xc_b, y_b, w_b in loader:\n",
        "                    yield Xc_b, None, y_b, w_b\n",
        "        else:\n",
        "            train_dataset = TensorDataset(x_cat, x_cont, y_tensor, w_tensor)\n",
        "            def _iter(loader):\n",
        "                for Xc_b, Xn_b, y_b, w_b in loader:\n",
        "                    yield Xc_b, Xn_b, y_b, w_b\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True,\n",
        "            num_workers=num_workers, pin_memory=pin_memory\n",
        "        )\n",
        "\n",
        "        best_loss, patience_counter = float(\"inf\"), 0\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            self.model.train()\n",
        "            epoch_loss, n_steps = 0.0, 0\n",
        "\n",
        "            for Xc_b, Xn_b, y_b, w_b in _iter(train_loader):\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = self.model(Xc_b, Xn_b)\n",
        "                loss = loss_fn(y_pred, y_b)\n",
        "                (loss * w_b).sum().div(w_b.sum()).backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += (loss * w_b).sum().div(w_b.sum()).item()\n",
        "                n_steps += 1\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch+1}/{max_epochs} - train_loss: {epoch_loss/max(1,n_steps):.6f}\")\n",
        "\n",
        "            # ---- Validation ----\n",
        "            if eval_set is not None:\n",
        "                self.model.eval()\n",
        "                with torch.no_grad():\n",
        "                    if x_cont_val is None:\n",
        "                        val_ds = TensorDataset(x_cat_val, y_val_tensor)\n",
        "                        val_loader = DataLoader(val_ds, batch_size=2048, shuffle=False,\n",
        "                                                num_workers=num_workers, pin_memory=pin_memory)\n",
        "                        eval_loss, n_eval = 0.0, 0\n",
        "                        for Xc_v, y_v in val_loader:\n",
        "                            y_p = self.model(Xc_v, None)\n",
        "                            lv = loss_fn(y_p, y_v)\n",
        "                            eval_loss += (lv.sum() / len(lv)).item()\n",
        "                            n_eval += 1\n",
        "                    else:\n",
        "                        val_ds = TensorDataset(x_cat_val, x_cont_val, y_val_tensor)\n",
        "                        val_loader = DataLoader(val_ds, batch_size=2048, shuffle=False,\n",
        "                                                num_workers=num_workers, pin_memory=pin_memory)\n",
        "                        eval_loss, n_eval = 0.0, 0\n",
        "                        for Xc_v, Xn_v, y_v in val_loader:\n",
        "                            y_p = self.model(Xc_v, Xn_v)\n",
        "                            lv = loss_fn(y_p, y_v)\n",
        "                            eval_loss += (lv.sum() / len(lv)).item()\n",
        "                            n_eval += 1\n",
        "                    eval_loss /= max(1, n_eval)\n",
        "                    if verbose:\n",
        "                        print(f\"          val_loss: {eval_loss:.6f}\")\n",
        "\n",
        "                    if patience is not None:\n",
        "                        if eval_loss < best_loss:\n",
        "                            best_loss = eval_loss\n",
        "                            patience_counter = 0\n",
        "                            self.best_model_weights = {k: v.detach().cpu().clone()\n",
        "                                                       for k, v in self.model.state_dict().items()}\n",
        "                        else:\n",
        "                            patience_counter += 1\n",
        "                            if patience_counter >= patience:\n",
        "                                if verbose:\n",
        "                                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                                break\n",
        "\n",
        "        if self.best_model_weights is not None:\n",
        "            self.model.load_state_dict(self.best_model_weights)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        x_cat, x_cont = self._split_X(X)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            probs1 = self.model(x_cat, x_cont).detach().cpu().numpy()  # (B, 1) sigmoid\n",
        "        probs1 = probs1.astype(\"float\")\n",
        "        probs0 = 1.0 - probs1\n",
        "        return np.hstack([probs0, probs1])\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return probs.argmax(axis=1)"
      ],
      "metadata": {
        "id": "B_PXz7vG42p5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto-preprocess categorical variables"
      ],
      "metadata": {
        "id": "Ns3h_G0D6iJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Complete, runnable TabTransformer pipeline with automatic preprocessing for string categorical columns.\n",
        "\n",
        "# Third Party\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 0) Preprocessor: string categorical -> integer IDs (with OOV=0)\n",
        "#    + auto cat_idx/cont_idx + cardinalities\n",
        "# ---------------------------------------------------------\n",
        "class TabularPreprocessor:\n",
        "    def __init__(self, categorical_indices=None, use_oov=True, oov_token=0, add_na_token=True):\n",
        "        self.categorical_indices = None if categorical_indices is None else list(categorical_indices)\n",
        "        self.use_oov = use_oov\n",
        "        self.oov_token = int(oov_token)  # 0 recommended\n",
        "        self.add_na_token = add_na_token\n",
        "        self.cat_maps = {}          # col_idx -> {category_value: int_id}\n",
        "        self.cardinalities = []     # per-categorical-column unique count (K, excluding OOV)\n",
        "        self.cat_idx = []\n",
        "        self.cont_idx = []\n",
        "        self.fitted_ = False\n",
        "\n",
        "    def _ensure_ndarray(self, X):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values\n",
        "        return np.asarray(X, dtype=object)  # safe for mixed types\n",
        "\n",
        "    def fit(self, X, categorical_indices=None):\n",
        "        X = self._ensure_ndarray(X)\n",
        "        n_cols = X.shape[1]\n",
        "\n",
        "        # lock categorical columns\n",
        "        if categorical_indices is not None:\n",
        "            self.categorical_indices = list(categorical_indices)\n",
        "\n",
        "        if self.categorical_indices is None:\n",
        "            # fallback: infer by dtype==object\n",
        "            self.categorical_indices = [j for j in range(n_cols) if X[:, j].dtype == object]\n",
        "\n",
        "        cat_set = set(self.categorical_indices)\n",
        "        self.cat_idx = sorted(list(cat_set))\n",
        "        self.cont_idx = [j for j in range(n_cols) if j not in cat_set]\n",
        "\n",
        "        # build per-column maps: real categories 1..K (0 reserved for OOV)\n",
        "        self.cat_maps = {}\n",
        "        self.cardinalities = []\n",
        "        for j in self.cat_idx:\n",
        "            col = X[:, j]\n",
        "            if self.add_na_token:\n",
        "                col = np.where(pd.isna(col), \"<NA>\", col)\n",
        "            uniques = pd.unique(col)\n",
        "            id_map = {val: i + 1 for i, val in enumerate(uniques)}  # 1..K\n",
        "            self.cat_maps[j] = id_map\n",
        "            self.cardinalities.append(len(uniques))  # exclude OOV\n",
        "\n",
        "        self.fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        assert self.fitted_, \"Call fit() before transform().\"\n",
        "        X = self._ensure_ndarray(X)\n",
        "\n",
        "        # categorical\n",
        "        if len(self.cat_idx) > 0:\n",
        "            x_cat = np.zeros((X.shape[0], len(self.cat_idx)), dtype=\"int64\")\n",
        "            for ti, j in enumerate(self.cat_idx):\n",
        "                col = X[:, j]\n",
        "                if self.add_na_token:\n",
        "                    col = np.where(pd.isna(col), \"<NA>\", col)\n",
        "                id_map = self.cat_maps[j]\n",
        "                x_cat[:, ti] = np.array([id_map.get(v, self.oov_token) for v in col], dtype=\"int64\")\n",
        "        else:\n",
        "            x_cat = np.zeros((X.shape[0], 0), dtype=\"int64\")\n",
        "\n",
        "        # continuous\n",
        "        if len(self.cont_idx) > 0:\n",
        "            x_cont = X[:, self.cont_idx].astype(\"float32\")\n",
        "        else:\n",
        "            x_cont = None\n",
        "\n",
        "        return x_cat, x_cont\n",
        "\n",
        "    def fit_transform(self, X, categorical_indices=None):\n",
        "        self.fit(X, categorical_indices)\n",
        "        return self.transform(X)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1) TabTransformer Backbone (categoricals only)\n",
        "#    - Correct handling of padding_idx (OOV slot)\n",
        "# ---------------------------------------------------------\n",
        "class TabTransformerBackbone(nn.Module):\n",
        "    \"\"\"\n",
        "    Contextualizes only categorical columns using a Transformer.\n",
        "    - Per-column embedding + column embedding (tabular positional embedding)\n",
        "    - Optional [CLS] token\n",
        "    - TransformerEncoder contextualization\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        cat_cardinalities,              # List[int], unique counts per categorical column (excluding OOV)\n",
        "        d_token=32,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,               # PyTorch single dropout\n",
        "        ff_dropout=0.1,                 # kept for API compatibility\n",
        "        embedding_dropout=0.1,\n",
        "        add_cls=False,\n",
        "        pooling=\"concat\",               # \"concat\" or \"cls\"\n",
        "        padding_idx=0,                  # Use 0 as OOV/pad index if not None\n",
        "        norm_first=True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert pooling in (\"concat\", \"cls\")\n",
        "        self.n_cat = len(cat_cardinalities)\n",
        "        self.d_token = d_token\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "\n",
        "        if self.n_cat == 0:\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "            self.col_embedding = None\n",
        "        else:\n",
        "            # FIXED: if padding_idx is not None, reserve +1 slot for it\n",
        "            self.cat_embeddings = nn.ModuleList([\n",
        "                nn.Embedding(\n",
        "                    num_embeddings=c + (1 if (padding_idx is not None) else 0),\n",
        "                    embedding_dim=d_token,\n",
        "                    padding_idx=padding_idx\n",
        "                )\n",
        "                for c in cat_cardinalities\n",
        "            ])\n",
        "            self.col_embedding = nn.Embedding(self.n_cat, d_token)\n",
        "\n",
        "        if self.add_cls:\n",
        "            self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n",
        "            nn.init.normal_(self.cls_token, std=0.02)\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout(embedding_dropout)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_token,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=attn_dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=norm_first\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        # init\n",
        "        for emb in self.cat_embeddings:\n",
        "            nn.init.normal_(emb.weight, std=0.02)\n",
        "        if self.col_embedding is not None:\n",
        "            nn.init.normal_(self.col_embedding.weight, std=0.02)\n",
        "\n",
        "    def forward(self, x_cat: torch.LongTensor):\n",
        "        \"\"\"\n",
        "        x_cat: LongTensor (B, n_cat)\n",
        "        returns:\n",
        "          pooling='concat' -> (B, n_cat*d)\n",
        "          pooling='cls'    -> (B, d)\n",
        "        \"\"\"\n",
        "        B = x_cat.size(0)\n",
        "        if self.n_cat == 0:\n",
        "            return torch.zeros(B, self.d_token if self.pooling == \"cls\" else 0,\n",
        "                               device=x_cat.device, dtype=torch.float32)\n",
        "\n",
        "        tok_list = []\n",
        "        for j, emb in enumerate(self.cat_embeddings):\n",
        "            tok = emb(x_cat[:, j])                         # (B, d)\n",
        "            if self.col_embedding is not None:\n",
        "                tok = tok + self.col_embedding.weight[j]   # (d,)\n",
        "            tok_list.append(tok.unsqueeze(1))              # (B, 1, d)\n",
        "\n",
        "        x_tok = torch.cat(tok_list, dim=1)                 # (B, n_cat, d)\n",
        "\n",
        "        if self.add_cls:\n",
        "            cls = self.cls_token.expand(B, -1, -1)         # (B, 1, d)\n",
        "            x_tok = torch.cat([cls, x_tok], dim=1)         # (B, 1+n_cat, d)\n",
        "\n",
        "        x_tok = self.embedding_dropout(x_tok)\n",
        "        z = self.transformer(x_tok)                        # (B, T, d)\n",
        "\n",
        "        if self.pooling == \"cls\" and self.add_cls:\n",
        "            out = z[:, 0, :]                               # (B, d)\n",
        "        elif self.pooling == \"cls\" and not self.add_cls:\n",
        "            out = z.mean(dim=1)                            # (B, d)\n",
        "        else:\n",
        "            if self.add_cls:\n",
        "                z = z[:, 1:, :]                            # drop CLS\n",
        "            out = z.reshape(B, -1)                         # (B, n_cat*d)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2) Full Model (Backbone + Continuous processing + Head)\n",
        "# ---------------------------------------------------------\n",
        "class TabTransformerModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Full TabTransformer:\n",
        "    - Transformer backbone for categorical features\n",
        "    - Continuous features normalized + linear proj (optional)\n",
        "    - Concatenate -> MLP head -> sigmoid\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        cat_cardinalities,          # List[int]\n",
        "        n_continuous=0,\n",
        "        d_token=32,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,\n",
        "        ff_dropout=0.1,\n",
        "        embedding_dropout=0.1,\n",
        "        add_cls=False,\n",
        "        pooling=\"concat\",\n",
        "        cont_proj=\"linear\",         # \"none\" or \"linear\"\n",
        "        mlp_hidden_dims=(128, 64),\n",
        "        mlp_dropout=0.2,\n",
        "        padding_idx=0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.n_cont = n_continuous\n",
        "        self.cont_proj = cont_proj\n",
        "\n",
        "        self.backbone = TabTransformerBackbone(\n",
        "            cat_cardinalities=cat_cardinalities,\n",
        "            d_token=d_token,\n",
        "            n_heads=n_heads,\n",
        "            n_layers=n_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            attn_dropout=attn_dropout,\n",
        "            ff_dropout=ff_dropout,\n",
        "            embedding_dropout=embedding_dropout,\n",
        "            add_cls=add_cls,\n",
        "            pooling=pooling,\n",
        "            padding_idx=padding_idx,\n",
        "            norm_first=True\n",
        "        )\n",
        "\n",
        "        # Continuous\n",
        "        if n_continuous > 0:\n",
        "            self.cont_bn = nn.BatchNorm1d(n_continuous)\n",
        "            if cont_proj == \"linear\":\n",
        "                self.cont_linear = nn.Linear(n_continuous, d_token)\n",
        "                nn.init.kaiming_uniform_(self.cont_linear.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "                cont_out_dim = d_token\n",
        "            else:\n",
        "                self.cont_linear = nn.Identity()\n",
        "                cont_out_dim = n_continuous\n",
        "        else:\n",
        "            self.cont_bn = None\n",
        "            self.cont_linear = None\n",
        "            cont_out_dim = 0\n",
        "\n",
        "        # Backbone output dim\n",
        "        backbone_out = (d_token if pooling == \"cls\" else len(cat_cardinalities) * d_token)\n",
        "        in_dim = backbone_out + cont_out_dim\n",
        "\n",
        "        # Head\n",
        "        mlp_layers = []\n",
        "        prev = in_dim\n",
        "        for h in mlp_hidden_dims:\n",
        "            lin = nn.Linear(prev, h)\n",
        "            nn.init.kaiming_uniform_(lin.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "            mlp_layers.extend([lin, nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(mlp_dropout)])\n",
        "            prev = h\n",
        "        mlp_layers.append(nn.Linear(prev, 1))\n",
        "        mlp_layers.append(nn.Sigmoid())\n",
        "        self.head = nn.Sequential(*mlp_layers)\n",
        "\n",
        "    def forward(self, x_cat: torch.LongTensor, x_cont: torch.FloatTensor = None):\n",
        "        z_cat = self.backbone(x_cat)\n",
        "        if (x_cont is not None) and (self.n_cont > 0):\n",
        "            if x_cont.ndim == 1:\n",
        "                x_cont = x_cont.unsqueeze(1)\n",
        "            x_cont = self.cont_bn(x_cont)\n",
        "            x_cont = self.cont_linear(x_cont)\n",
        "            z = torch.cat([z_cat, x_cont], dim=1)\n",
        "        else:\n",
        "            z = z_cat\n",
        "        out = self.head(z)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3) Sklearn-Compatible Classifier with Auto Preprocessing\n",
        "# ---------------------------------------------------------\n",
        "class TabTransformerBinaryClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    Example:\n",
        "        clf = TabTransformerBinaryClassifier(\n",
        "            auto_preprocess=True,\n",
        "            categorical_indices=[0, 3, 5],  # provided by you\n",
        "            d_token=32, n_heads=4, n_layers=2, ...\n",
        "        )\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        cat_idx=None,\n",
        "        cat_cardinalities=None,\n",
        "        cont_idx=None,\n",
        "        d_token=32,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,\n",
        "        ff_dropout=0.1,\n",
        "        embedding_dropout=0.1,\n",
        "        add_cls=False,\n",
        "        pooling=\"concat\",\n",
        "        cont_proj=\"linear\",\n",
        "        hidden_dims=(128, 64),\n",
        "        mlp_dropout=0.2,\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-4,\n",
        "        loss_fn=\"logloss\",\n",
        "        device=None,\n",
        "        # Auto preprocessing\n",
        "        auto_preprocess=True,\n",
        "        categorical_indices=None,\n",
        "        use_oov=True\n",
        "    ):\n",
        "        # manual fields (kept for compatibility)\n",
        "        self.cat_idx = [] if cat_idx is None else list(cat_idx)\n",
        "        self.cat_cardinalities = None if cat_cardinalities is None else list(cat_cardinalities)\n",
        "        self.cont_idx = [] if cont_idx is None else list(cont_idx)\n",
        "\n",
        "        # model/training params\n",
        "        self.d_token = d_token\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.dim_feedforward = dim_feedforward\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.ff_dropout = ff_dropout\n",
        "        self.embedding_dropout = embedding_dropout\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "        self.cont_proj = cont_proj\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.mlp_dropout = mlp_dropout\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.loss_fn_name = loss_fn\n",
        "\n",
        "        self.model = None\n",
        "        self.best_model_weights = None\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # auto-preproc\n",
        "        self.auto_preprocess = auto_preprocess\n",
        "        self.categorical_indices = categorical_indices\n",
        "        self.use_oov = use_oov\n",
        "        self.preproc = None\n",
        "\n",
        "    def _build_model(self):\n",
        "        n_cont = len(self.cont_idx)\n",
        "        padding_idx = 0 if self.use_oov else None\n",
        "        model = TabTransformerModel(\n",
        "            cat_cardinalities=self.cat_cardinalities,\n",
        "            n_continuous=n_cont,\n",
        "            d_token=self.d_token,\n",
        "            n_heads=self.n_heads,\n",
        "            n_layers=self.n_layers,\n",
        "            dim_feedforward=self.dim_feedforward,\n",
        "            attn_dropout=self.attn_dropout,\n",
        "            ff_dropout=self.ff_dropout,\n",
        "            embedding_dropout=self.embedding_dropout,\n",
        "            add_cls=self.add_cls,\n",
        "            pooling=self.pooling,\n",
        "            cont_proj=self.cont_proj,\n",
        "            mlp_hidden_dims=self.hidden_dims,\n",
        "            mlp_dropout=self.mlp_dropout,\n",
        "            padding_idx=padding_idx\n",
        "        )\n",
        "        return model.to(self.device)\n",
        "\n",
        "    def _define_loss_fn(self):\n",
        "        if self.loss_fn_name == \"logloss\":\n",
        "            return nn.BCELoss(reduction=\"none\")\n",
        "        else:\n",
        "            raise Exception(f\"{self.loss_fn_name} is not defined\")\n",
        "\n",
        "    def _split_X(self, X):\n",
        "        \"\"\"\n",
        "        If auto_preprocess=True -> use preproc.transform(X).\n",
        "        Else -> use provided indices.\n",
        "        \"\"\"\n",
        "        if self.auto_preprocess:\n",
        "            assert self.preproc is not None and self.preproc.fitted_, \"Call fit() first.\"\n",
        "            x_cat_np, x_cont_np = self.preproc.transform(X)\n",
        "        else:\n",
        "            X_np = X.detach().cpu().numpy() if isinstance(X, torch.Tensor) else X\n",
        "            if len(self.cat_idx) > 0:\n",
        "                x_cat_np = X_np[:, self.cat_idx].astype(\"int64\")\n",
        "            else:\n",
        "                x_cat_np = np.zeros((X_np.shape[0], 0), dtype=\"int64\")\n",
        "            x_cont_np = (X_np[:, self.cont_idx].astype(\"float32\") if len(self.cont_idx) > 0 else None)\n",
        "\n",
        "        x_cat = torch.tensor(x_cat_np, dtype=torch.long, device=self.device)\n",
        "        x_cont = torch.tensor(x_cont_np, dtype=torch.float32, device=self.device) if x_cont_np is not None else None\n",
        "        return x_cat, x_cont\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        X,\n",
        "        y,\n",
        "        sample_weight=None,\n",
        "        eval_set=None,            # list of tuples: [(X_val, y_val)]\n",
        "        eval_metric=None,         # supports [\"logloss\"]\n",
        "        max_epochs=10,\n",
        "        patience=None,\n",
        "        batch_size=32,\n",
        "        num_workers=0,\n",
        "        verbose=True,\n",
        "        pin_memory=None\n",
        "    ):\n",
        "        if pin_memory is None:\n",
        "            pin_memory = (self.device == \"cuda\")\n",
        "\n",
        "        # Auto-preprocessing: infer indices + cardinalities from data\n",
        "        if self.auto_preprocess:\n",
        "            self.preproc = TabularPreprocessor(\n",
        "                categorical_indices=self.categorical_indices,\n",
        "                use_oov=self.use_oov,\n",
        "                oov_token=0,\n",
        "                add_na_token=True\n",
        "            )\n",
        "            self.preproc.fit(X)  # derive cat_idx/cont_idx/cardinalities\n",
        "            self.cat_idx = self.preproc.cat_idx\n",
        "            self.cont_idx = self.preproc.cont_idx\n",
        "            self.cat_cardinalities = self.preproc.cardinalities\n",
        "\n",
        "        # Tensors\n",
        "        x_cat, x_cont = self._split_X(X)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "        w_tensor = (torch.tensor(sample_weight, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "                    if sample_weight is not None else torch.ones_like(y_tensor))\n",
        "\n",
        "        # Validation\n",
        "        if eval_set is not None:\n",
        "            X_val, y_val = eval_set[0]\n",
        "            x_cat_val, x_cont_val = self._split_X(X_val)\n",
        "            y_val_tensor = torch.tensor(y_val, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "        else:\n",
        "            x_cat_val = x_cont_val = y_val_tensor = None\n",
        "\n",
        "        # Build model\n",
        "        if self.model is None:\n",
        "            self.model = self._build_model()\n",
        "\n",
        "        loss_fn = self._define_loss_fn()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        # DataLoaders\n",
        "        if x_cont is None:\n",
        "            train_dataset = TensorDataset(x_cat, y_tensor, w_tensor)\n",
        "            def _iter(loader):\n",
        "                for Xc_b, y_b, w_b in loader:\n",
        "                    yield Xc_b, None, y_b, w_b\n",
        "        else:\n",
        "            train_dataset = TensorDataset(x_cat, x_cont, y_tensor, w_tensor)\n",
        "            def _iter(loader):\n",
        "                for Xc_b, Xn_b, y_b, w_b in loader:\n",
        "                    yield Xc_b, Xn_b, y_b, w_b\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True,\n",
        "            num_workers=num_workers, pin_memory=pin_memory\n",
        "        )\n",
        "\n",
        "        best_loss, patience_counter = float(\"inf\"), 0\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            self.model.train()\n",
        "            epoch_loss, n_steps = 0.0, 0\n",
        "\n",
        "            for Xc_b, Xn_b, y_b, w_b in _iter(train_loader):\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = self.model(Xc_b, Xn_b)\n",
        "                loss = loss_fn(y_pred, y_b)\n",
        "                (loss * w_b).sum().div(w_b.sum()).backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += (loss * w_b).sum().div(w_b.sum()).item()\n",
        "                n_steps += 1\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch + 1}/{max_epochs} - train_loss: {epoch_loss / max(1, n_steps):.6f}\")\n",
        "\n",
        "            # ---- Validation ----\n",
        "            if eval_set is not None:\n",
        "                self.model.eval()\n",
        "                with torch.no_grad():\n",
        "                    if x_cont_val is None:\n",
        "                        val_ds = TensorDataset(x_cat_val, y_val_tensor)\n",
        "                        val_loader = DataLoader(val_ds, batch_size=2048, shuffle=False,\n",
        "                                                num_workers=num_workers, pin_memory=pin_memory)\n",
        "                        eval_loss, n_eval = 0.0, 0\n",
        "                        for Xc_v, y_v in val_loader:\n",
        "                            y_p = self.model(Xc_v, None)\n",
        "                            lv = loss_fn(y_p, y_v)\n",
        "                            eval_loss += (lv.sum() / len(lv)).item()\n",
        "                            n_eval += 1\n",
        "                    else:\n",
        "                        val_ds = TensorDataset(x_cat_val, x_cont_val, y_val_tensor)\n",
        "                        val_loader = DataLoader(val_ds, batch_size=2048, shuffle=False,\n",
        "                                                num_workers=num_workers, pin_memory=pin_memory)\n",
        "                        eval_loss, n_eval = 0.0, 0\n",
        "                        for Xc_v, Xn_v, y_v in val_loader:\n",
        "                            y_p = self.model(Xc_v, Xn_v)\n",
        "                            lv = loss_fn(y_p, y_v)\n",
        "                            eval_loss += (lv.sum() / len(lv)).item()\n",
        "                            n_eval += 1\n",
        "                    eval_loss /= max(1, n_eval)\n",
        "                    if verbose:\n",
        "                        print(f\"          val_loss: {eval_loss:.6f}\")\n",
        "\n",
        "                    if patience is not None:\n",
        "                        if eval_loss < best_loss:\n",
        "                            best_loss = eval_loss\n",
        "                            patience_counter = 0\n",
        "                            self.best_model_weights = {k: v.detach().cpu().clone()\n",
        "                                                       for k, v in self.model.state_dict().items()}\n",
        "                        else:\n",
        "                            patience_counter += 1\n",
        "                            if patience_counter >= patience:\n",
        "                                if verbose:\n",
        "                                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                                break\n",
        "\n",
        "        if self.best_model_weights is not None:\n",
        "            self.model.load_state_dict(self.best_model_weights)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        x_cat, x_cont = self._split_X(X)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            probs1 = self.model(x_cat, x_cont).detach().cpu().numpy()  # (B, 1)\n",
        "        probs1 = probs1.astype(\"float\")\n",
        "        probs0 = 1.0 - probs1\n",
        "        return np.hstack([probs0, probs1])\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return probs.argmax(axis=1)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4) Sample mixed-type dataset generator (with strings)\n",
        "# ---------------------------------------------------------\n",
        "def make_mixed_sample(\n",
        "    n_samples=40000,\n",
        "    seed=7\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a mixed dataset:\n",
        "      - Categorical string cols: gender, city, device\n",
        "      - Continuous cols: 10 numerical features\n",
        "      - Label generated from cat weights + linear cont + noise\n",
        "    Returns:\n",
        "      X (object ndarray), y (int64), categorical_feature_indices (list[int])\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "\n",
        "    genders = np.array([\"남성\", \"여성\", \"기타\"], dtype=object)\n",
        "    cities = np.array([\"서울\", \"부산\", \"대구\", \"인천\", \"수원\", \"고양\"], dtype=object)\n",
        "    devices = np.array([\"ios\", \"android\", \"web\"], dtype=object)\n",
        "\n",
        "    # categorical columns\n",
        "    gender_col = rng.choice(genders, size=n_samples, p=[0.48, 0.48, 0.04])\n",
        "    city_col   = rng.choice(cities, size=n_samples)\n",
        "    device_col = rng.choice(devices, size=n_samples, p=[0.35, 0.55, 0.10])\n",
        "\n",
        "    # continuous columns\n",
        "    n_cont = 10\n",
        "    X_cont = rng.randn(n_samples, n_cont).astype(\"float32\")\n",
        "\n",
        "    # latent score\n",
        "    # assign random weights per category\n",
        "    w_gender = {g: w for g, w in zip(genders, rng.uniform(-0.8, 0.8, size=len(genders)))}\n",
        "    w_city   = {c: w for c, w in zip(cities, rng.uniform(-0.6, 1.0, size=len(cities)))}\n",
        "    w_device = {d: w for d, w in zip(devices, rng.uniform(-0.5, 0.9, size=len(devices)))}\n",
        "    w_cont   = rng.randn(n_cont).astype(\"float32\")\n",
        "\n",
        "    score_cat = (np.vectorize(lambda v: w_gender[v])(gender_col) +\n",
        "                 np.vectorize(lambda v: w_city[v])(city_col) +\n",
        "                 np.vectorize(lambda v: w_device[v])(device_col)).astype(\"float32\")\n",
        "    score_cont = (X_cont * w_cont).sum(axis=1).astype(\"float32\")\n",
        "\n",
        "    bias = 0.1\n",
        "    noise = rng.normal(scale=0.5, size=n_samples).astype(\"float32\")\n",
        "    logit = 0.7 * score_cat + 0.8 * score_cont + bias + noise\n",
        "    prob = 1.0 / (1.0 + np.exp(-logit))\n",
        "    y = (prob > 0.5).astype(\"int64\")\n",
        "\n",
        "    # combine into feature array (object dtype)\n",
        "    # layout: [gender, city, device, cont...]\n",
        "    X = np.empty((n_samples, 3 + n_cont), dtype=object)\n",
        "    X[:, 0] = gender_col\n",
        "    X[:, 1] = city_col\n",
        "    X[:, 2] = device_col\n",
        "    X[:, 3:] = X_cont\n",
        "\n",
        "    categorical_feature_indices = [0, 1, 2]\n",
        "    return X, y, categorical_feature_indices\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5) Train & Evaluate Demo\n",
        "# ---------------------------------------------------------\n",
        "def train_and_evaluate_demo():\n",
        "    np.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # ----- Data -----\n",
        "    X, y, categorical_feature_indices = make_mixed_sample(n_samples=30000, seed=123)\n",
        "\n",
        "    # train/val/test split\n",
        "    N = X.shape[0]\n",
        "    idx = np.arange(N)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    tr_end = int(N * 0.7)\n",
        "    va_end = int(N * 0.85)\n",
        "    tr_idx, va_idx, te_idx = idx[:tr_end], idx[tr_end:va_end], idx[va_end:]\n",
        "\n",
        "    X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
        "    X_va, y_va = X[va_idx], y[va_idx]\n",
        "    X_te, y_te = X[te_idx], y[te_idx]\n",
        "\n",
        "    # ----- Model -----\n",
        "    clf = TabTransformerBinaryClassifier(\n",
        "        # auto preprocessing with your categorical indices\n",
        "        auto_preprocess=True,\n",
        "        categorical_indices=categorical_feature_indices,\n",
        "        use_oov=True,                # reserve 0 for OOV/pad\n",
        "        # model hyperparams\n",
        "        d_token=32,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,\n",
        "        embedding_dropout=0.05,\n",
        "        pooling=\"concat\",\n",
        "        add_cls=False,\n",
        "        cont_proj=\"linear\",\n",
        "        hidden_dims=(128, 64),\n",
        "        mlp_dropout=0.2,\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-4,\n",
        "        loss_fn=\"logloss\",\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric=[\"logloss\"],\n",
        "        max_epochs=10,\n",
        "        patience=2,\n",
        "        batch_size=1024,     # adjust per GPU/CPU memory\n",
        "        num_workers=0,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # ----- Eval -----\n",
        "    proba_te = clf.predict_proba(X_te)[:, 1]\n",
        "    pred_te = (proba_te >= 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_te, pred_te)\n",
        "    auc = roc_auc_score(y_te, proba_te)\n",
        "    ll  = log_loss(y_te, np.vstack([1 - proba_te, proba_te]).T)\n",
        "\n",
        "    print(\"\\n===== Test Metrics =====\")\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"ROC-AUC  : {auc:.4f}\")\n",
        "    print(f\"Logloss  : {ll:.4f}\")\n",
        "\n",
        "    print(\"\\nSample predictions (first 10):\")\n",
        "    print(np.round(proba_te[:10], 4))\n",
        "\n",
        "    # sanity check: learned indices & cardinalities\n",
        "    if clf.auto_preprocess:\n",
        "        print(\"\\n[Info] Derived categorical indices:\", clf.cat_idx)\n",
        "        print(\"[Info] Derived continuous indices :\", clf.cont_idx[:10], \"... (total:\", len(clf.cont_idx), \")\")\n",
        "        print(\"[Info] Cardinalities (per cat col):\", clf.cat_cardinalities)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate_demo()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iasQFY3o6UC0",
        "outputId": "7bffd851-3dee-434b-bc08-0081d69c1405"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - train_loss: 0.551576\n",
            "          val_loss: 0.393735\n",
            "Epoch 2/10 - train_loss: 0.331714\n",
            "          val_loss: 0.250838\n",
            "Epoch 3/10 - train_loss: 0.229794\n",
            "          val_loss: 0.192343\n",
            "Epoch 4/10 - train_loss: 0.182760\n",
            "          val_loss: 0.161463\n",
            "Epoch 5/10 - train_loss: 0.156629\n",
            "          val_loss: 0.152338\n",
            "Epoch 6/10 - train_loss: 0.142234\n",
            "          val_loss: 0.136741\n",
            "Epoch 7/10 - train_loss: 0.135480\n",
            "          val_loss: 0.137253\n",
            "Epoch 8/10 - train_loss: 0.131664\n",
            "          val_loss: 0.134651\n",
            "Epoch 9/10 - train_loss: 0.130729\n",
            "          val_loss: 0.130683\n",
            "Epoch 10/10 - train_loss: 0.127579\n",
            "          val_loss: 0.129336\n",
            "\n",
            "===== Test Metrics =====\n",
            "Accuracy : 0.9444\n",
            "ROC-AUC  : 0.9896\n",
            "Logloss  : 0.1315\n",
            "\n",
            "Sample predictions (first 10):\n",
            "[1.041e-01 4.200e-03 9.889e-01 9.904e-01 2.000e-04 4.210e-02 6.000e-04\n",
            " 9.880e-01 9.840e-01 9.998e-01]\n",
            "\n",
            "[Info] Derived categorical indices: [0, 1, 2]\n",
            "[Info] Derived continuous indices : [3, 4, 5, 6, 7, 8, 9, 10, 11, 12] ... (total: 10 )\n",
            "[Info] Cardinalities (per cat col): [3, 6, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# ===== 전처리기 (문자 범주 자동 인코딩 + cardinalities 산출) =====\n",
        "class TabularPreprocessor:\n",
        "    def __init__(self, categorical_indices=None, use_oov=True, oov_token=0, add_na_token=True):\n",
        "        self.categorical_indices = None if categorical_indices is None else list(categorical_indices)\n",
        "        self.use_oov = use_oov\n",
        "        self.oov_token = int(oov_token)\n",
        "        self.add_na_token = add_na_token\n",
        "        self.cat_maps = {}\n",
        "        self.cardinalities = []\n",
        "        self.cat_idx = []\n",
        "        self.cont_idx = []\n",
        "        self.fitted_ = False\n",
        "\n",
        "    def _ensure_ndarray(self, X):\n",
        "        import pandas as pd\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values\n",
        "        return np.asarray(X, dtype=object)\n",
        "\n",
        "    def fit(self, X, categorical_indices=None):\n",
        "        import pandas as pd\n",
        "        X = self._ensure_ndarray(X)\n",
        "        n_cols = X.shape[1]\n",
        "        if categorical_indices is not None:\n",
        "            self.categorical_indices = list(categorical_indices)\n",
        "\n",
        "        # cat 인덱스가 없으면 dtype==object 기반 추론\n",
        "        if self.categorical_indices is None:\n",
        "            self.categorical_indices = [j for j in range(n_cols) if X[:, j].dtype == object]\n",
        "\n",
        "        cat_set = set(self.categorical_indices)\n",
        "        self.cat_idx = sorted(list(cat_set))\n",
        "        self.cont_idx = [j for j in range(n_cols) if j not in cat_set]\n",
        "\n",
        "        self.cat_maps = {}\n",
        "        self.cardinalities = []\n",
        "        for j in self.cat_idx:\n",
        "            col = X[:, j]\n",
        "            if self.add_na_token:\n",
        "                col = np.where(pd.isna(col), \"<NA>\", col)\n",
        "            uniques = pd.unique(col)\n",
        "            self.cat_maps[j] = {val: i + 1 for i, val in enumerate(uniques)}  # 1..K\n",
        "            self.cardinalities.append(len(uniques))\n",
        "        self.fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        import pandas as pd\n",
        "        assert self.fitted_, \"Call fit() before transform().\"\n",
        "        X = self._ensure_ndarray(X)\n",
        "\n",
        "        if len(self.cat_idx) > 0:\n",
        "            x_cat = np.zeros((X.shape[0], len(self.cat_idx)), dtype=\"int64\")\n",
        "            for out_i, j in enumerate(self.cat_idx):\n",
        "                col = X[:, j]\n",
        "                if self.add_na_token:\n",
        "                    col = np.where(pd.isna(col), \"<NA>\", col)\n",
        "                id_map = self.cat_maps[j]\n",
        "                x_cat[:, out_i] = np.array([id_map.get(v, self.oov_token) for v in col], dtype=\"int64\")\n",
        "        else:\n",
        "            x_cat = np.zeros((X.shape[0], 0), dtype=\"int64\")\n",
        "\n",
        "        if len(self.cont_idx) > 0:\n",
        "            # 혼합형일 수 있어 안전 캐스팅\n",
        "            x_cont = X[:, self.cont_idx].astype(\"float32\")\n",
        "        else:\n",
        "            x_cont = None\n",
        "\n",
        "        return x_cat, x_cont\n",
        "\n",
        "# ===== TabTransformer 구현 =====\n",
        "class TabTransformerBackbone(nn.Module):\n",
        "    def __init__(\n",
        "        self, cat_cardinalities, d_token=32, n_heads=4, n_layers=2, dim_feedforward=128,\n",
        "        attn_dropout=0.1, ff_dropout=0.1, embedding_dropout=0.1,\n",
        "        add_cls=False, pooling=\"concat\", padding_idx=0, norm_first=True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert pooling in (\"concat\",\"cls\")\n",
        "        self.n_cat = len(cat_cardinalities)\n",
        "        self.d_token = d_token\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "\n",
        "        if self.n_cat == 0:\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "            self.col_embedding = None\n",
        "        else:\n",
        "            self.cat_embeddings = nn.ModuleList([\n",
        "                nn.Embedding(\n",
        "                    num_embeddings=c + (1 if (padding_idx is not None) else 0),\n",
        "                    embedding_dim=d_token,\n",
        "                    padding_idx=padding_idx\n",
        "                )\n",
        "                for c in cat_cardinalities\n",
        "            ])\n",
        "            self.col_embedding = nn.Embedding(self.n_cat, d_token)\n",
        "\n",
        "        if self.add_cls:\n",
        "            self.cls_token = nn.Parameter(torch.zeros(1,1,d_token))\n",
        "            nn.init.normal_(self.cls_token, std=0.02)\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout(embedding_dropout)\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_token, nhead=n_heads, dim_feedforward=dim_feedforward,\n",
        "            dropout=attn_dropout, batch_first=True, norm_first=norm_first\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "\n",
        "        for emb in self.cat_embeddings:\n",
        "            nn.init.normal_(emb.weight, std=0.02)\n",
        "        if self.col_embedding is not None:\n",
        "            nn.init.normal_(self.col_embedding.weight, std=0.02)\n",
        "\n",
        "    def forward(self, x_cat: torch.LongTensor):\n",
        "        B = x_cat.size(0)\n",
        "        if self.n_cat == 0:\n",
        "            if self.pooling == \"cls\":\n",
        "                return torch.zeros(B, self.d_token, device=x_cat.device, dtype=torch.float32)\n",
        "            return torch.zeros(B, 0, device=x_cat.device, dtype=torch.float32)\n",
        "\n",
        "        toks = []\n",
        "        for j, emb in enumerate(self.cat_embeddings):\n",
        "            tok = emb(x_cat[:, j])\n",
        "            if self.col_embedding is not None:\n",
        "                tok = tok + self.col_embedding.weight[j]\n",
        "            toks.append(tok.unsqueeze(1))\n",
        "        x_tok = torch.cat(toks, dim=1)\n",
        "\n",
        "        if self.add_cls:\n",
        "            cls = self.cls_token.expand(B, -1, -1)\n",
        "            x_tok = torch.cat([cls, x_tok], dim=1)\n",
        "\n",
        "        x_tok = self.embedding_dropout(x_tok)\n",
        "        z = self.transformer(x_tok)\n",
        "\n",
        "        if self.pooling == \"cls\" and self.add_cls:\n",
        "            out = z[:, 0, :]\n",
        "        elif self.pooling == \"cls\":\n",
        "            out = z.mean(dim=1)\n",
        "        else:\n",
        "            if self.add_cls:\n",
        "                z = z[:, 1:, :]\n",
        "            out = z.reshape(B, -1)\n",
        "        return out\n",
        "\n",
        "class TabTransformerModel(nn.Module):\n",
        "    def __init__(\n",
        "        self, cat_cardinalities, n_continuous=0, d_token=32, n_heads=4, n_layers=2,\n",
        "        dim_feedforward=128, attn_dropout=0.1, ff_dropout=0.1, embedding_dropout=0.1,\n",
        "        add_cls=False, pooling=\"concat\", cont_proj=\"linear\",\n",
        "        mlp_hidden_dims=(128,64), mlp_dropout=0.2, padding_idx=0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.n_cont = n_continuous\n",
        "        self.cont_proj = cont_proj\n",
        "\n",
        "        self.backbone = TabTransformerBackbone(\n",
        "            cat_cardinalities=cat_cardinalities, d_token=d_token, n_heads=n_heads, n_layers=n_layers,\n",
        "            dim_feedforward=dim_feedforward, attn_dropout=attn_dropout, ff_dropout=ff_dropout,\n",
        "            embedding_dropout=embedding_dropout, add_cls=add_cls, pooling=pooling,\n",
        "            padding_idx=padding_idx, norm_first=True\n",
        "        )\n",
        "\n",
        "        if n_continuous > 0:\n",
        "            self.cont_bn = nn.BatchNorm1d(n_continuous)\n",
        "            if cont_proj == \"linear\":\n",
        "                self.cont_linear = nn.Linear(n_continuous, d_token)\n",
        "                nn.init.kaiming_uniform_(self.cont_linear.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "                cont_dim = d_token\n",
        "            else:\n",
        "                self.cont_linear = nn.Identity()\n",
        "                cont_dim = n_continuous\n",
        "        else:\n",
        "            self.cont_bn = None\n",
        "            self.cont_linear = None\n",
        "            cont_dim = 0\n",
        "\n",
        "        back_dim = (d_token if pooling==\"cls\" else len(cat_cardinalities)*d_token)\n",
        "        in_dim = back_dim + cont_dim\n",
        "\n",
        "        layers = []\n",
        "        prev = in_dim\n",
        "        for h in mlp_hidden_dims:\n",
        "            lin = nn.Linear(prev, h)\n",
        "            nn.init.kaiming_uniform_(lin.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "            layers += [lin, nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(mlp_dropout)]\n",
        "            prev = h\n",
        "        layers += [nn.Linear(prev, 1), nn.Sigmoid()]\n",
        "        self.head = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x_cat, x_cont=None):\n",
        "        z_cat = self.backbone(x_cat)\n",
        "        if (x_cont is not None) and (self.n_cont > 0):\n",
        "            if x_cont.ndim == 1:\n",
        "                x_cont = x_cont.unsqueeze(1)\n",
        "            x_cont = self.cont_bn(x_cont)\n",
        "            x_cont = self.cont_linear(x_cont)\n",
        "            z = torch.cat([z_cat, x_cont], dim=1)\n",
        "        else:\n",
        "            z = z_cat\n",
        "        return self.head(z)\n",
        "\n",
        "# ===== Custom-호환 래퍼 =====\n",
        "class TabTransformerBinaryClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    MLPBinaryClassifier 와 호환:\n",
        "      - __init__(..., lr=1e-3, loss_fn=\"logloss\")\n",
        "      - fit(X, y, sample_weight=None, eval_set=None, eval_metric=None, max_epochs=10, patience=None, batch_size=32)\n",
        "      - predict_proba, predict\n",
        "    추가:\n",
        "      - auto_preprocess=True + categorical_feature_indices: 혼합형 입력 지원\n",
        "      - 이미 전처리된 float32 입력도 지원(범주형 0개로 동작)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        # MLP와의 겉 시그니처 맞춤 (input_dim은 무시 가능)\n",
        "        input_dim=None,\n",
        "        hidden_dims=(128,64),\n",
        "        lr=1e-3,\n",
        "        loss_fn=\"logloss\",\n",
        "\n",
        "        # TabTransformer 하이퍼파라미터\n",
        "        d_token=32, n_heads=4, n_layers=2, dim_feedforward=128,\n",
        "        attn_dropout=0.1, embedding_dropout=0.1,\n",
        "        add_cls=False, pooling=\"concat\", cont_proj=\"linear\",\n",
        "        mlp_dropout=0.2, weight_decay=1e-4, device=None,\n",
        "\n",
        "        # 자동 전처리 옵션\n",
        "        auto_preprocess=True,\n",
        "        categorical_feature_indices=None,  # 혼합형이면 필수\n",
        "        use_oov=True\n",
        "    ):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.lr = lr\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "        self.d_token = d_token\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.dim_feedforward = dim_feedforward\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.embedding_dropout = embedding_dropout\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "        self.cont_proj = cont_proj\n",
        "        self.mlp_dropout = mlp_dropout\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = None\n",
        "        self.best_model_weights = None\n",
        "\n",
        "        # 전처리 상태\n",
        "        self.auto_preprocess = auto_preprocess\n",
        "        self.categorical_feature_indices = categorical_feature_indices\n",
        "        self.use_oov = use_oov\n",
        "        self.preproc = None\n",
        "        self.cat_idx = []\n",
        "        self.cont_idx = []\n",
        "        self.cat_cardinalities = []\n",
        "\n",
        "    def _define_loss_fn(self):\n",
        "        if self.loss_fn == \"logloss\":\n",
        "            return nn.BCELoss(reduction=\"none\")\n",
        "        else:\n",
        "            raise Exception(f\"{self.loss_fn} is not defined\")\n",
        "\n",
        "    def _build_model(self):\n",
        "        n_cont = len(self.cont_idx)\n",
        "        padding_idx = 0 if self.use_oov else None\n",
        "        model = TabTransformerModel(\n",
        "            cat_cardinalities=self.cat_cardinalities,\n",
        "            n_continuous=n_cont,\n",
        "            d_token=self.d_token,\n",
        "            n_heads=self.n_heads,\n",
        "            n_layers=self.n_layers,\n",
        "            dim_feedforward=self.dim_feedforward,\n",
        "            attn_dropout=self.attn_dropout,\n",
        "            ff_dropout=self.attn_dropout,   # 동일 파라미터로 둠\n",
        "            embedding_dropout=self.embedding_dropout,\n",
        "            add_cls=self.add_cls,\n",
        "            pooling=self.pooling,\n",
        "            cont_proj=self.cont_proj,\n",
        "            mlp_hidden_dims=self.hidden_dims,\n",
        "            mlp_dropout=self.mlp_dropout,\n",
        "            padding_idx=padding_idx\n",
        "        )\n",
        "        return model.to(self.device)\n",
        "\n",
        "    def _split_X(self, X):\n",
        "        if self.auto_preprocess:\n",
        "            assert self.preproc is not None and self.preproc.fitted_, \"Call fit() first.\"\n",
        "            x_cat_np, x_cont_np = self.preproc.transform(X)\n",
        "        else:\n",
        "            # 이미 전처리된 float32 입력을 모두 연속형으로 취급\n",
        "            X_np = X.detach().cpu().numpy() if isinstance(X, torch.Tensor) else X\n",
        "            # 범주형 없음\n",
        "            x_cat_np = np.zeros((X_np.shape[0], 0), dtype=\"int64\")\n",
        "            x_cont_np = X_np.astype(\"float32\")\n",
        "\n",
        "        x_cat = torch.tensor(x_cat_np, dtype=torch.long, device=self.device)\n",
        "        x_cont = torch.tensor(x_cont_np, dtype=torch.float32, device=self.device) if x_cont_np is not None else None\n",
        "        return x_cat, x_cont\n",
        "\n",
        "    def fit(\n",
        "        self, X, y, sample_weight=None, eval_set=None, eval_metric=None,\n",
        "        max_epochs=10, patience=None, batch_size=32\n",
        "    ):\n",
        "        pin_memory = (self.device == \"cuda\")\n",
        "\n",
        "        # 자동 전처리: 혼합형 처리 + cat_cardinalities 산출\n",
        "        if self.auto_preprocess:\n",
        "            self.preproc = TabularPreprocessor(\n",
        "                categorical_indices=self.categorical_feature_indices if hasattr(self, \"categorical_feature_indices\") else self.categorical_feature_indices,\n",
        "                use_oov=self.use_oov, oov_token=0, add_na_token=True\n",
        "            )\n",
        "            self.preproc.fit(X, categorical_indices=self.categorical_feature_indices)\n",
        "            self.cat_idx = self.preproc.cat_idx\n",
        "            self.cont_idx = self.preproc.cont_idx\n",
        "            self.cat_cardinalities = self.preproc.cardinalities\n",
        "        else:\n",
        "            # 전처리 안 쓰면: 범주형 0개, 연속형=전부\n",
        "            X_np = X.detach().cpu().numpy() if isinstance(X, torch.Tensor) else X\n",
        "            self.cat_idx = []\n",
        "            self.cont_idx = list(range(X_np.shape[1]))\n",
        "            self.cat_cardinalities = []\n",
        "\n",
        "        x_cat, x_cont = self._split_X(X)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "        w_tensor = (torch.tensor(sample_weight, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "                    if sample_weight is not None else torch.ones_like(y_tensor))\n",
        "\n",
        "        if eval_set is not None:\n",
        "            X_val, y_val = eval_set[0]\n",
        "            x_cat_val, x_cont_val = self._split_X(X_val)\n",
        "            y_val_tensor = torch.tensor(y_val, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "        else:\n",
        "            x_cat_val = x_cont_val = y_val_tensor = None\n",
        "\n",
        "        if self.model is None:\n",
        "            self.model = self._build_model()\n",
        "\n",
        "        loss_fn = self._define_loss_fn()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        # DataLoader (범주형 유무에 따라 튜플 수가 달라지므로 공통 이터레이터 제공)\n",
        "        if x_cont is None:\n",
        "            train_dataset = TensorDataset(x_cat, y_tensor, w_tensor)\n",
        "            def _iter(loader):\n",
        "                for Xc_b, y_b, w_b in loader:\n",
        "                    yield Xc_b, None, y_b, w_b\n",
        "        else:\n",
        "            train_dataset = TensorDataset(x_cat, x_cont, y_tensor, w_tensor)\n",
        "            def _iter(loader):\n",
        "                for Xc_b, Xn_b, y_b, w_b in loader:\n",
        "                    yield Xc_b, Xn_b, y_b, w_b\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=pin_memory)\n",
        "\n",
        "        best_loss, patience_counter = float(\"inf\"), 0\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            self.model.train()\n",
        "            running, steps = 0.0, 0\n",
        "            for Xc_b, Xn_b, y_b, w_b in _iter(train_loader):\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = self.model(Xc_b, Xn_b)\n",
        "                loss = loss_fn(y_pred, y_b)\n",
        "                (loss * w_b).sum().div(w_b.sum()).backward()\n",
        "                optimizer.step()\n",
        "                running += (loss * w_b).sum().div(w_b.sum()).item()\n",
        "                steps += 1\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{max_epochs}, Loss: {running / max(1,steps):.6f}\")\n",
        "\n",
        "            # ---- Validation ----\n",
        "            if eval_set is not None:\n",
        "                self.model.eval()\n",
        "                with torch.no_grad():\n",
        "                    if x_cont_val is None:\n",
        "                        val_ds = TensorDataset(x_cat_val, y_val_tensor)\n",
        "                        val_loader = DataLoader(val_ds, batch_size=2048, shuffle=False, pin_memory=pin_memory)\n",
        "                        eval_loss, n_eval = 0.0, 0\n",
        "                        for Xc_v, y_v in val_loader:\n",
        "                            y_p = self.model(Xc_v, None)\n",
        "                            lv = loss_fn(y_p, y_v)\n",
        "                            eval_loss += (lv.sum() / len(lv)).item()\n",
        "                            n_eval += 1\n",
        "                    else:\n",
        "                        val_ds = TensorDataset(x_cat_val, x_cont_val, y_val_tensor)\n",
        "                        val_loader = DataLoader(val_ds, batch_size=2048, shuffle=False, pin_memory=pin_memory)\n",
        "                        eval_loss, n_eval = 0.0, 0\n",
        "                        for Xc_v, Xn_v, y_v in val_loader:\n",
        "                            y_p = self.model(Xc_v, Xn_v)\n",
        "                            lv = loss_fn(y_p, y_v)\n",
        "                            eval_loss += (lv.sum() / len(lv)).item()\n",
        "                            n_eval += 1\n",
        "                    eval_loss /= max(1, n_eval)\n",
        "                    if patience is not None:\n",
        "                        print(f\"-- eval_loss: {eval_loss:.6f} (best: {best_loss:.6f})\")\n",
        "                        if eval_loss < best_loss:\n",
        "                            best_loss = eval_loss\n",
        "                            patience_counter = 0\n",
        "                            self.best_model_weights = {k: v.detach().cpu().clone()\n",
        "                                                       for k, v in self.model.state_dict().items()}\n",
        "                        else:\n",
        "                            patience_counter += 1\n",
        "                            if patience_counter >= patience:\n",
        "                                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                                break\n",
        "\n",
        "        if self.best_model_weights is not None:\n",
        "            self.model.load_state_dict(self.best_model_weights)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        x_cat, x_cont = self._split_X(X)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            probs1 = self.model(x_cat, x_cont).detach().cpu().numpy()\n",
        "        probs1 = probs1.astype(\"float\")\n",
        "        probs0 = 1.0 - probs1\n",
        "        return np.hstack([probs0, probs1])\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return probs.argmax(axis=1)\n"
      ],
      "metadata": {
        "id": "15EgVcO27hZ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}