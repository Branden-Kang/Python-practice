{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy4vhJPnzpV5E5YOp9xAYk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Third Party\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# -----------------------------\n",
        "# TabTransformer Backbone\n",
        "# -----------------------------\n",
        "class TabTransformerBackbone(nn.Module):\n",
        "    \"\"\"\n",
        "    Backbone that contextualizes only categorical columns using a Transformer.\n",
        "    - Sum of per-column embedding and column embedding (tabular positional embedding)\n",
        "    - (Optional) prepend a [CLS] token for pooling\n",
        "    - Contextualize with TransformerEncoder\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        cat_cardinalities,              # List[int], number of unique values per categorical column\n",
        "        d_token=32,                     # token embedding dimension (Transformer d_model)\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,               # PyTorch uses one dropout (applies in both self-attn and FFN)\n",
        "        ff_dropout=0.1,                 # kept for API compatibility (treated same as attn_dropout)\n",
        "        embedding_dropout=0.1,\n",
        "        add_cls=False,                  # if True, prepend a [CLS] token\n",
        "        pooling=\"concat\",               # \"concat\" or \"cls\"\n",
        "        padding_idx=None,               # specify padding index if needed for embeddings\n",
        "        norm_first=True                 # Pre-LN architecture\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert pooling in (\"concat\", \"cls\")\n",
        "        self.n_cat = len(cat_cardinalities)\n",
        "        self.d_token = d_token\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "\n",
        "        if self.n_cat == 0:\n",
        "            # No categorical columns -> act as a dummy pass-through\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "            self.col_embedding = None\n",
        "        else:\n",
        "            # Per-column embeddings\n",
        "            self.cat_embeddings = nn.ModuleList([\n",
        "                nn.Embedding(\n",
        "                    num_embeddings=c + (1 if (padding_idx is None) else 0),\n",
        "                    embedding_dim=d_token,\n",
        "                    padding_idx=None if (padding_idx is None) else padding_idx\n",
        "                )\n",
        "                for c in cat_cardinalities\n",
        "            ])\n",
        "            # Column embeddings (tabular counterpart of positional encoding)\n",
        "            self.col_embedding = nn.Embedding(self.n_cat, d_token)\n",
        "\n",
        "        # (Optional) CLS token\n",
        "        if self.add_cls:\n",
        "            self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n",
        "            nn.init.normal_(self.cls_token, std=0.02)\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout(embedding_dropout)\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_token,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=attn_dropout,   # single dropout parameter in PyTorch\n",
        "            batch_first=True,\n",
        "            norm_first=norm_first\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        # Weight initialization\n",
        "        for emb in self.cat_embeddings:\n",
        "            nn.init.normal_(emb.weight, std=0.02)\n",
        "        if self.col_embedding is not None:\n",
        "            nn.init.normal_(self.col_embedding.weight, std=0.02)\n",
        "\n",
        "    def forward(self, x_cat: torch.LongTensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x_cat: LongTensor of shape (B, n_cat)\n",
        "        Returns:\n",
        "            If pooling='concat' -> FloatTensor of shape (B, n_cat * d_token)\n",
        "            If pooling='cls'    -> FloatTensor of shape (B, d_token)\n",
        "        \"\"\"\n",
        "        B = x_cat.size(0)\n",
        "        if self.n_cat == 0:\n",
        "            # No categorical columns\n",
        "            if self.pooling == \"cls\":\n",
        "                out = torch.zeros(B, self.d_token, device=x_cat.device, dtype=torch.float32)\n",
        "            else:\n",
        "                out = torch.zeros(B, 0, device=x_cat.device, dtype=torch.float32)\n",
        "            return out\n",
        "\n",
        "        tok_list = []\n",
        "        for j, emb in enumerate(self.cat_embeddings):\n",
        "            tok = emb(x_cat[:, j])                         # (B, d)\n",
        "            if self.col_embedding is not None:\n",
        "                tok = tok + self.col_embedding.weight[j]   # add column embedding (d,)\n",
        "            tok_list.append(tok.unsqueeze(1))              # (B, 1, d)\n",
        "\n",
        "        x_tok = torch.cat(tok_list, dim=1)                 # (B, n_cat, d)\n",
        "\n",
        "        if self.add_cls:\n",
        "            cls = self.cls_token.expand(B, -1, -1)         # (B, 1, d)\n",
        "            x_tok = torch.cat([cls, x_tok], dim=1)         # (B, 1 + n_cat, d)\n",
        "\n",
        "        x_tok = self.embedding_dropout(x_tok)              # (B, T, d)\n",
        "        z = self.transformer(x_tok)                        # (B, T, d)\n",
        "\n",
        "        if self.pooling == \"cls\" and self.add_cls:\n",
        "            out = z[:, 0, :]                               # (B, d)\n",
        "        elif self.pooling == \"cls\" and not self.add_cls:\n",
        "            out = z.mean(dim=1)                            # (B, d)\n",
        "        else:\n",
        "            if self.add_cls:\n",
        "                z = z[:, 1:, :]                            # (B, n_cat, d) â€” drop CLS\n",
        "            out = z.reshape(B, -1)                         # (B, n_cat * d)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Full Model (Backbone + Head)\n",
        "# -----------------------------\n",
        "class TabTransformerModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Full TabTransformer:\n",
        "    - Transformer backbone contextualizes categorical features\n",
        "    - Continuous features are normalized + optionally projected\n",
        "    - Concatenate -> MLP head -> single logit -> sigmoid\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        cat_cardinalities,          # List[int]\n",
        "        n_continuous=0,\n",
        "        d_token=32,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,\n",
        "        ff_dropout=0.1,             # kept for API compatibility\n",
        "        embedding_dropout=0.1,\n",
        "        add_cls=False,\n",
        "        pooling=\"concat\",\n",
        "        cont_proj=\"linear\",         # \"none\" or \"linear\"\n",
        "        mlp_hidden_dims=(128, 64),  # head MLP sizes\n",
        "        mlp_dropout=0.2\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.n_cont = n_continuous\n",
        "        self.cont_proj = cont_proj\n",
        "\n",
        "        # Backbone\n",
        "        self.backbone = TabTransformerBackbone(\n",
        "            cat_cardinalities=cat_cardinalities,\n",
        "            d_token=d_token,\n",
        "            n_heads=n_heads,\n",
        "            n_layers=n_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            attn_dropout=attn_dropout,\n",
        "            ff_dropout=ff_dropout,\n",
        "            embedding_dropout=embedding_dropout,\n",
        "            add_cls=add_cls,\n",
        "            pooling=pooling,\n",
        "            norm_first=True\n",
        "        )\n",
        "\n",
        "        # Continuous feature processing\n",
        "        if n_continuous > 0:\n",
        "            self.cont_bn = nn.BatchNorm1d(n_continuous)\n",
        "            if cont_proj == \"linear\":\n",
        "                self.cont_linear = nn.Linear(n_continuous, d_token)\n",
        "                nn.init.kaiming_uniform_(self.cont_linear.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "                cont_out_dim = d_token\n",
        "            else:\n",
        "                self.cont_linear = nn.Identity()\n",
        "                cont_out_dim = n_continuous\n",
        "        else:\n",
        "            self.cont_bn = None\n",
        "            self.cont_linear = None\n",
        "            cont_out_dim = 0\n",
        "\n",
        "        # Backbone output dimension\n",
        "        if pooling == \"cls\":\n",
        "            backbone_out = d_token\n",
        "        else:\n",
        "            backbone_out = len(cat_cardinalities) * d_token\n",
        "\n",
        "        in_dim = backbone_out + cont_out_dim\n",
        "\n",
        "        # Head MLP\n",
        "        mlp_layers = []\n",
        "        prev = in_dim\n",
        "        for h in mlp_hidden_dims:\n",
        "            lin = nn.Linear(prev, h)\n",
        "            nn.init.kaiming_uniform_(lin.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "            mlp_layers.extend([lin, nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(mlp_dropout)])\n",
        "            prev = h\n",
        "        mlp_layers.append(nn.Linear(prev, 1))\n",
        "        mlp_layers.append(nn.Sigmoid())\n",
        "        self.head = nn.Sequential(*mlp_layers)\n",
        "\n",
        "    def forward(self, x_cat: torch.LongTensor, x_cont: torch.FloatTensor = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x_cat: (B, n_cat) long\n",
        "            x_cont: (B, n_cont) float or None\n",
        "        Returns:\n",
        "            (B, 1) sigmoid probability\n",
        "        \"\"\"\n",
        "        z_cat = self.backbone(x_cat)  # (B, d_backbone)\n",
        "\n",
        "        if (x_cont is not None) and (self.n_cont > 0):\n",
        "            if x_cont.ndim == 1:\n",
        "                x_cont = x_cont.unsqueeze(1)\n",
        "            x_cont = self.cont_bn(x_cont)\n",
        "            x_cont = self.cont_linear(x_cont)\n",
        "            z = torch.cat([z_cat, x_cont], dim=1)\n",
        "        else:\n",
        "            z = z_cat\n",
        "\n",
        "        out = self.head(z)            # (B, 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Sklearn-Compatible Classifier\n",
        "# -----------------------------\n",
        "class TabTransformerBinaryClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    Example:\n",
        "        clf = TabTransformerBinaryClassifier(\n",
        "            cat_idx=[0, 1, 5],                # categorical column indices\n",
        "            cat_cardinalities=[10, 5, 20],    # cardinality per categorical column\n",
        "            cont_idx=[2, 3, 4],               # continuous column indices (or [])\n",
        "            d_token=32, n_heads=4, n_layers=2,\n",
        "            hidden_dims=(128, 64), lr=1e-3\n",
        "        )\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        cat_idx,\n",
        "        cat_cardinalities,\n",
        "        cont_idx=None,\n",
        "        d_token=32,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,\n",
        "        ff_dropout=0.1,\n",
        "        embedding_dropout=0.1,\n",
        "        add_cls=False,\n",
        "        pooling=\"concat\",\n",
        "        cont_proj=\"linear\",\n",
        "        hidden_dims=(128, 64),\n",
        "        mlp_dropout=0.2,\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-4,\n",
        "        loss_fn=\"logloss\",\n",
        "        device=None\n",
        "    ):\n",
        "        self.cat_idx = list(cat_idx)\n",
        "        self.cat_cardinalities = list(cat_cardinalities)\n",
        "        self.cont_idx = list(cont_idx) if cont_idx is not None else []\n",
        "\n",
        "        # Model / training hyperparameters\n",
        "        self.d_token = d_token\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.dim_feedforward = dim_feedforward\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.ff_dropout = ff_dropout\n",
        "        self.embedding_dropout = embedding_dropout\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "        self.cont_proj = cont_proj\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.mlp_dropout = mlp_dropout\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.loss_fn_name = loss_fn\n",
        "\n",
        "        # Internal state\n",
        "        self.model = None\n",
        "        self.best_model_weights = None\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # -------------------------\n",
        "    # Internals\n",
        "    # -------------------------\n",
        "    def _build_model(self):\n",
        "        n_cont = len(self.cont_idx)\n",
        "        model = TabTransformerModel(\n",
        "            cat_cardinalities=self.cat_cardinalities,\n",
        "            n_continuous=n_cont,\n",
        "            d_token=self.d_token,\n",
        "            n_heads=self.n_heads,\n",
        "            n_layers=self.n_layers,\n",
        "            dim_feedforward=self.dim_feedforward,\n",
        "            attn_dropout=self.attn_dropout,\n",
        "            ff_dropout=self.ff_dropout,\n",
        "            embedding_dropout=self.embedding_dropout,\n",
        "            add_cls=self.add_cls,\n",
        "            pooling=self.pooling,\n",
        "            cont_proj=self.cont_proj,\n",
        "            mlp_hidden_dims=self.hidden_dims,\n",
        "            mlp_dropout=self.mlp_dropout\n",
        "        )\n",
        "        return model.to(self.device)\n",
        "\n",
        "    def _define_loss_fn(self):\n",
        "        if self.loss_fn_name == \"logloss\":\n",
        "            return nn.BCELoss(reduction=\"none\")\n",
        "        else:\n",
        "            raise Exception(f\"{self.loss_fn_name} is not defined\")\n",
        "\n",
        "    def _split_X(self, X):\n",
        "        \"\"\"\n",
        "        Split input X (np.ndarray or torch.Tensor) into:\n",
        "        - x_cat: LongTensor (B, n_cat)\n",
        "        - x_cont: FloatTensor (B, n_cont) or None\n",
        "        \"\"\"\n",
        "        if isinstance(X, torch.Tensor):\n",
        "            X_np = X.detach().cpu().numpy()\n",
        "        else:\n",
        "            X_np = X\n",
        "\n",
        "        if len(self.cat_idx) > 0:\n",
        "            x_cat_np = X_np[:, self.cat_idx].astype(\"int64\")\n",
        "        else:\n",
        "            x_cat_np = np.zeros((X_np.shape[0], 0), dtype=\"int64\")\n",
        "\n",
        "        if len(self.cont_idx) > 0:\n",
        "            x_cont_np = X_np[:, self.cont_idx].astype(\"float32\")\n",
        "        else:\n",
        "            x_cont_np = None\n",
        "\n",
        "        x_cat = torch.tensor(x_cat_np, dtype=torch.long, device=self.device)\n",
        "        x_cont = torch.tensor(x_cont_np, dtype=torch.float32, device=self.device) if x_cont_np is not None else None\n",
        "        return x_cat, x_cont\n",
        "\n",
        "    # -------------------------\n",
        "    # Public API\n",
        "    # -------------------------\n",
        "    def fit(\n",
        "        self,\n",
        "        X,\n",
        "        y,\n",
        "        sample_weight=None,\n",
        "        eval_set=None,            # list of tuples: [(X_val, y_val)]\n",
        "        eval_metric=None,         # supports [\"logloss\"] only\n",
        "        max_epochs=10,\n",
        "        patience=None,\n",
        "        batch_size=32,\n",
        "        num_workers=0,\n",
        "        verbose=True,\n",
        "        pin_memory=None           # set True when using CUDA for faster host->device transfer\n",
        "    ):\n",
        "        if pin_memory is None:\n",
        "            pin_memory = (self.device == \"cuda\")\n",
        "\n",
        "        # Prepare tensors\n",
        "        x_cat, x_cont = self._split_X(X)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            w_tensor = torch.tensor(sample_weight, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "        else:\n",
        "            w_tensor = torch.ones_like(y_tensor, dtype=torch.float32, device=self.device)\n",
        "\n",
        "        if eval_set is not None:\n",
        "            X_val, y_val = eval_set[0]\n",
        "            x_cat_val, x_cont_val = self._split_X(X_val)\n",
        "            y_val_tensor = torch.tensor(y_val, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "        else:\n",
        "            x_cat_val = x_cont_val = y_val_tensor = None\n",
        "\n",
        "        # Build model\n",
        "        if self.model is None:\n",
        "            self.model = self._build_model()\n",
        "\n",
        "        loss_fn = self._define_loss_fn()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        # DataLoader\n",
        "        if x_cont is None:\n",
        "            train_dataset = TensorDataset(x_cat, y_tensor, w_tensor)\n",
        "        else:\n",
        "            train_dataset = TensorDataset(x_cat, x_cont, y_tensor, w_tensor)\n",
        "\n",
        "        def _make_train_loader():\n",
        "            return DataLoader(\n",
        "                train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                num_workers=num_workers, pin_memory=pin_memory\n",
        "            )\n",
        "\n",
        "        train_loader = _make_train_loader()\n",
        "\n",
        "        best_loss = float(\"inf\")\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            self.model.train()\n",
        "            epoch_loss = 0.0\n",
        "            n_steps = 0\n",
        "\n",
        "            if x_cont is None:\n",
        "                for Xc_b, y_b, w_b in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    y_pred = self.model(Xc_b, None)\n",
        "                    loss = loss_fn(y_pred, y_b)\n",
        "                    weighted_loss = (loss * w_b).sum() / w_b.sum()\n",
        "                    weighted_loss.backward()\n",
        "                    optimizer.step()\n",
        "                    epoch_loss += weighted_loss.item()\n",
        "                    n_steps += 1\n",
        "            else:\n",
        "                for Xc_b, Xn_b, y_b, w_b in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    y_pred = self.model(Xc_b, Xn_b)\n",
        "                    loss = loss_fn(y_pred, y_b)\n",
        "                    weighted_loss = (loss * w_b).sum() / w_b.sum()\n",
        "                    weighted_loss.backward()\n",
        "                    optimizer.step()\n",
        "                    epoch_loss += weighted_loss.item()\n",
        "                    n_steps += 1\n",
        "\n",
        "            train_avg = epoch_loss / max(1, n_steps)\n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch + 1}/{max_epochs} - train_loss: {train_avg:.6f}\")\n",
        "\n",
        "            # -------------------------\n",
        "            # Validation\n",
        "            # -------------------------\n",
        "            if eval_set is not None:\n",
        "                self.model.eval()\n",
        "                if eval_metric is not None:\n",
        "                    for m in eval_metric:\n",
        "                        if m != \"logloss\":\n",
        "                            raise Exception(f\"{eval_metric} is not defined\")\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    if x_cont_val is None:\n",
        "                        val_dataset = TensorDataset(x_cat_val, y_val_tensor)\n",
        "                        val_loader = DataLoader(\n",
        "                            val_dataset, batch_size=2048, shuffle=False,\n",
        "                            num_workers=num_workers, pin_memory=pin_memory\n",
        "                        )\n",
        "                        eval_loss = 0.0\n",
        "                        n_eval = 0\n",
        "                        for Xc_v, y_v in val_loader:\n",
        "                            y_pred_v = self.model(Xc_v, None)\n",
        "                            loss_v = loss_fn(y_pred_v, y_v)\n",
        "                            eval_loss += (loss_v.sum() / len(loss_v)).item()\n",
        "                            n_eval += 1\n",
        "                    else:\n",
        "                        val_dataset = TensorDataset(x_cat_val, x_cont_val, y_val_tensor)\n",
        "                        val_loader = DataLoader(\n",
        "                            val_dataset, batch_size=2048, shuffle=False,\n",
        "                            num_workers=num_workers, pin_memory=pin_memory\n",
        "                        )\n",
        "                        eval_loss = 0.0\n",
        "                        n_eval = 0\n",
        "                        for Xc_v, Xn_v, y_v in val_loader:\n",
        "                            y_pred_v = self.model(Xc_v, Xn_v)\n",
        "                            loss_v = loss_fn(y_pred_v, y_v)\n",
        "                            eval_loss += (loss_v.sum() / len(loss_v)).item()\n",
        "                            n_eval += 1\n",
        "\n",
        "                    eval_loss = eval_loss / max(1, n_eval)\n",
        "                    if verbose:\n",
        "                        print(f\"          val_loss: {eval_loss:.6f}\")\n",
        "\n",
        "                    if patience is not None:\n",
        "                        if eval_loss < best_loss:\n",
        "                            best_loss = eval_loss\n",
        "                            patience_counter = 0\n",
        "                            self.best_model_weights = {k: v.detach().cpu().clone() for k, v in self.model.state_dict().items()}\n",
        "                        else:\n",
        "                            patience_counter += 1\n",
        "                            if patience_counter >= patience:\n",
        "                                if verbose:\n",
        "                                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                                break\n",
        "\n",
        "        if self.best_model_weights is not None:\n",
        "            self.model.load_state_dict(self.best_model_weights)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        x_cat, x_cont = self._split_X(X)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            probs1 = self.model(x_cat, x_cont).detach().cpu().numpy()  # (B, 1) sigmoid\n",
        "        probs1 = probs1.astype(\"float\")\n",
        "        probs0 = 1.0 - probs1\n",
        "        return np.hstack([probs0, probs1])\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return probs.argmax(axis=1)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Synthetic Data Generator + Training Demo\n",
        "# ============================================================\n",
        "def make_synthetic_tabular(\n",
        "    n_samples=20000,\n",
        "    cat_cardinalities=(12, 7, 25),   # 3 categorical columns\n",
        "    n_cont=4,                        # 4 continuous columns\n",
        "    seed=42\n",
        "):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n_cat = len(cat_cardinalities)\n",
        "\n",
        "    # Generate categorical columns (each column 0..K-1)\n",
        "    X_cat = np.column_stack([\n",
        "        rng.randint(0, c, size=n_samples).astype(\"int64\") for c in cat_cardinalities\n",
        "    ])  # (N, n_cat)\n",
        "\n",
        "    # Generate continuous columns\n",
        "    X_cont = rng.randn(n_samples, n_cont).astype(\"float32\")  # (N, n_cont)\n",
        "\n",
        "    # Latent score: categorical contribution + continuous linear combo + noise\n",
        "    cat_weights = [rng.randn(c) * rng.uniform(0.3, 1.0) for c in cat_cardinalities]\n",
        "    score_cat = np.zeros(n_samples, dtype=\"float32\")\n",
        "    for j in range(n_cat):\n",
        "        score_cat += cat_weights[j][X_cat[:, j]]\n",
        "\n",
        "    w_cont = rng.randn(n_cont).astype(\"float32\")\n",
        "    score_cont = (X_cont * w_cont).sum(axis=1)\n",
        "\n",
        "    bias = 0.2\n",
        "    noise = rng.normal(scale=0.5, size=n_samples).astype(\"float32\")\n",
        "\n",
        "    logit = 0.6 * score_cat + 0.8 * score_cont + bias + noise\n",
        "    prob = 1 / (1 + np.exp(-logit))\n",
        "    y = (prob > 0.5).astype(\"int64\")\n",
        "\n",
        "    # Concatenate as [categorical | continuous]; later split by indices\n",
        "    X = np.concatenate([X_cat.astype(\"float32\"), X_cont], axis=1)\n",
        "    cat_idx = list(range(n_cat))\n",
        "    cont_idx = list(range(n_cat, n_cat + n_cont))\n",
        "\n",
        "    return X, y, cat_idx, cont_idx, list(cat_cardinalities)\n",
        "\n",
        "\n",
        "def train_and_evaluate_demo():\n",
        "    # Reproducibility\n",
        "    np.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # ---------------- Data ----------------\n",
        "    X, y, cat_idx, cont_idx, cat_cardinalities = make_synthetic_tabular(\n",
        "        n_samples=20000, cat_cardinalities=(12, 7, 25), n_cont=4, seed=13\n",
        "    )\n",
        "\n",
        "    # Train/Val/Test split\n",
        "    N = X.shape[0]\n",
        "    idx = np.arange(N)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    tr_end = int(N * 0.7)\n",
        "    va_end = int(N * 0.85)\n",
        "\n",
        "    tr_idx = idx[:tr_end]\n",
        "    va_idx = idx[tr_end:va_end]\n",
        "    te_idx = idx[va_end:]\n",
        "\n",
        "    X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
        "    X_va, y_va = X[va_idx], y[va_idx]\n",
        "    X_te, y_te = X[te_idx], y[te_idx]\n",
        "\n",
        "    # ---------------- Model ----------------\n",
        "    clf = TabTransformerBinaryClassifier(\n",
        "        cat_idx=cat_idx,\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        cont_idx=cont_idx,\n",
        "        d_token=32,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,\n",
        "        embedding_dropout=0.05,\n",
        "        pooling=\"concat\",          # or \"cls\" (with add_cls=True)\n",
        "        add_cls=False,\n",
        "        cont_proj=\"linear\",\n",
        "        hidden_dims=(128, 64),\n",
        "        mlp_dropout=0.2,\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-4,\n",
        "        loss_fn=\"logloss\",\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric=[\"logloss\"],\n",
        "        max_epochs=15,\n",
        "        patience=3,\n",
        "        batch_size=256,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # ---------------- Eval ----------------\n",
        "    proba_va = clf.predict_proba(X_va)[:, 1]\n",
        "    proba_te = clf.predict_proba(X_te)[:, 1]\n",
        "    pred_te = (proba_te >= 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_te, pred_te)\n",
        "    auc = roc_auc_score(y_te, proba_te)\n",
        "    ll  = log_loss(y_te, np.vstack([1 - proba_te, proba_te]).T)\n",
        "\n",
        "    print(\"\\n===== Test Metrics =====\")\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"ROC-AUC  : {auc:.4f}\")\n",
        "    print(f\"Logloss  : {ll:.4f}\")\n",
        "\n",
        "    # Show sample predictions\n",
        "    print(\"\\nSample predictions (first 10):\")\n",
        "    print(np.round(proba_te[:10], 4))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate_demo()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFxkZMZ_p5Nh",
        "outputId": "f59bdc1b-acb8-4029-920b-7bf7cc2fb6d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 - train_loss: 0.345680\n",
            "          val_loss: 0.193235\n",
            "Epoch 2/15 - train_loss: 0.179509\n",
            "          val_loss: 0.147929\n",
            "Epoch 3/15 - train_loss: 0.149519\n",
            "          val_loss: 0.130593\n",
            "Epoch 4/15 - train_loss: 0.141352\n",
            "          val_loss: 0.126637\n",
            "Epoch 5/15 - train_loss: 0.138444\n",
            "          val_loss: 0.122914\n",
            "Epoch 6/15 - train_loss: 0.138383\n",
            "          val_loss: 0.123396\n",
            "Epoch 7/15 - train_loss: 0.133943\n",
            "          val_loss: 0.124393\n",
            "Epoch 8/15 - train_loss: 0.134532\n",
            "          val_loss: 0.124173\n",
            "Early stopping at epoch 8\n",
            "\n",
            "===== Test Metrics =====\n",
            "Accuracy : 0.9550\n",
            "ROC-AUC  : 0.9940\n",
            "Logloss  : 0.1048\n",
            "\n",
            "Sample predictions (first 10):\n",
            "[0.0577 0.0019 0.9995 0.     0.9976 0.0043 0.0666 0.9945 0.9992 0.9986]\n"
          ]
        }
      ]
    }
  ]
}