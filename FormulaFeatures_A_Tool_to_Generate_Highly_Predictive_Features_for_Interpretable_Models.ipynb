{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPI1DBOk9Sjq6xRE1aG9466"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://towardsdatascience.com/formulafeatures-a-tool-to-generate-highly-predictive-features-for-interpretable-models-e18aab45e96d)"
      ],
      "metadata": {
        "id": "cRWzU9FoMRgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Uncomment to debug warnings\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"error\")\n",
        "\n",
        "\n",
        "class FormulaFeatures:\n",
        "    def __init__(self,\n",
        "                 base_model=None,\n",
        "                 metric=None,\n",
        "                 max_iterations=None,\n",
        "                 max_iterations_no_gain=None,\n",
        "                 max_original_features=20,\n",
        "                 target_type='regression',\n",
        "                 test_square=False,\n",
        "                 test_sqrt=False,\n",
        "                 test_log=False,\n",
        "                 verbose=0):\n",
        "        \"\"\"\n",
        "        A feature engineering tool to efficiently create effective, arbitrarily-complex arithmetic combinations of\n",
        "        numeric features.\n",
        "\n",
        "         base_model\n",
        "            Currently unimplemented\n",
        "\n",
        "         metric\n",
        "            Currently unimplemented\n",
        "\n",
        "         max_iterations (int)\n",
        "            The maximum iterations the fit process is allowed to execute. This limits the number and complexity of the\n",
        "            features engineered\n",
        "\n",
        "         max_iterations_no_gain\n",
        "            Currently unimplemented\n",
        "\n",
        "         max_original_features (int)\n",
        "            Where the input dataframe contains more than this number of columns, only the max_original_features features\n",
        "            with the highest scores will be combined with other features\n",
        "\n",
        "         target_type (string)\n",
        "            Must be \"classification\" or \"regression\"\n",
        "\n",
        "         test_square\n",
        "            Currently unimplemented\n",
        "\n",
        "         test_sqrt\n",
        "            Currently unimplemented\n",
        "\n",
        "         test_log\n",
        "            Currently unimplemented\n",
        "\n",
        "         verbose (int)\n",
        "            Either 0, 1, or 2. Indicates the amount of information displayed during the fit process.\n",
        "        \"\"\"\n",
        "\n",
        "        self.base_model = base_model\n",
        "        self.metric = metric\n",
        "        self.max_iterations = np.inf if max_iterations is None else max_iterations\n",
        "        self.max_iterations_no_gain = np.inf if max_iterations_no_gain is None else max_iterations_no_gain\n",
        "        self.max_original_features = max_original_features\n",
        "        self.target_type = target_type\n",
        "        self.test_square = test_square\n",
        "        self.test_sqrt = test_sqrt\n",
        "        self.test_log = test_log\n",
        "        self.verbose = verbose\n",
        "        self.x_train, self.x_test, self.y_train, self.y_test = None, None, None, None\n",
        "\n",
        "        # All original features and the useful engineered features\n",
        "        self.features_arr = None\n",
        "\n",
        "        # Parallel to features_arr. Contains the formula to create the feature.\n",
        "        self.feature_definitions = None\n",
        "\n",
        "        # Parallel to features_arr. Contains the accuracy of the feature in a 1d model.\n",
        "        self.feature_scores = None\n",
        "\n",
        "        # Parallel to features_arr. Binary indicator if the feature is to be considered for future combinations\n",
        "        self.use_feature = None\n",
        "\n",
        "        # Parallel to features_arr. Each element contains a tuple with two elements: the values for the the feature in\n",
        "        # the train data and the values for the feature in the test data.\n",
        "        self.feature_values = None\n",
        "\n",
        "        # The best score of any single original or engineered feature created to date.\n",
        "        self.best_score_overall = -np.inf\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        \"\"\"\n",
        "        This is a supervised feature engineering process, and as such requires the y column.\n",
        "        \"\"\"\n",
        "\n",
        "        # Remove all Null and inf values to start\n",
        "        x = self.__clean_data(x)\n",
        "\n",
        "        # Create a train-test split within the passed data, which may itself be\n",
        "        # a portion of the full data available\n",
        "        self.x_train, self.x_test, self.y_train, self.y_test = \\\n",
        "            train_test_split(x, y, test_size=0.33, random_state=42)\n",
        "\n",
        "        # For efficiency, limit the size of the training and testing data\n",
        "        if len(self.x_train) > 50_000:\n",
        "            self.x_train = self.x_train.sample(n=50_000)\n",
        "            self.y_train = self.y_train.loc[self.x_train.index]\n",
        "        if len(self.x_test) > 10_000:\n",
        "            self.x_test = self.x_test.sample(n=10_000)\n",
        "            self.y_test = self.y_test.loc[self.x_test.index]\n",
        "\n",
        "        # Get the initial set of features, which are the features passed in.\n",
        "        self.features_arr = x.columns.tolist()\n",
        "\n",
        "        # Define the initial set of feature definitions. These are set as None, as they original features have no\n",
        "        # definition to create them.\n",
        "        self.feature_definitions = [None]*len(self.features_arr)\n",
        "\n",
        "        # Store the values in each original & engineered column. At this point, we save the values of the original\n",
        "        # columns. Each element is a tuple with the train values and the test values\n",
        "        self.feature_values = []\n",
        "        for col_idx, col_name in enumerate(x.columns):\n",
        "            self.feature_values.append((self.x_train[col_name].values, self.x_test[col_name].values))\n",
        "\n",
        "        # To start, we use all original features.\n",
        "        self.use_feature = [True]*len(self.features_arr)\n",
        "\n",
        "        # Examine the original columns\n",
        "        self.__get_metrics_orig_features()\n",
        "        self.__display_features_metrics(0)\n",
        "        self.__get_best_feature_score()\n",
        "        self.__reduce_original_features()\n",
        "\n",
        "        # Get the initial engineered features based on pairs of original features.\n",
        "        iteration_number = 1\n",
        "        prev_num_features = len(self.features_arr)\n",
        "        self.__combine_features(starting_idx=0)\n",
        "        self.__display_features_metrics(iteration_number)\n",
        "        self.__get_best_feature_score()\n",
        "\n",
        "        # Loop, creating new combinations of features\n",
        "        num_features = len(self.features_arr)\n",
        "        iteration_number += 1\n",
        "        while iteration_number <= self.max_iterations:\n",
        "            self.__combine_features(starting_idx=prev_num_features)\n",
        "            self.__display_features_metrics(iteration_number)\n",
        "            self.__get_best_feature_score()\n",
        "\n",
        "            new_num_features = len(self.features_arr)\n",
        "            if new_num_features == num_features:\n",
        "                break\n",
        "            prev_num_features = num_features\n",
        "            num_features = new_num_features\n",
        "            iteration_number += 1\n",
        "\n",
        "        self.__display_features_metrics(\"Final\")\n",
        "\n",
        "    def fit_transform(self, x):\n",
        "        \"\"\"\n",
        "        Equivalent to calling fit() and transform()\n",
        "        \"\"\"\n",
        "\n",
        "        self.fit(x)\n",
        "        x = x.copy()\n",
        "        for feature_idx, feature_name in enumerate(self.features_arr):\n",
        "            if feature_name not in x.columns:\n",
        "                train_vals = pd.Series(self.feature_values[feature_idx][0], index=self.x_train.index)\n",
        "                test_vals = pd.Series(self.feature_values[feature_idx][1], index=self.x_test.index)\n",
        "                x[feature_name] = train_vals.append(test_vals)\n",
        "        x = self.__clean_data(x)\n",
        "        return x\n",
        "\n",
        "    def transform(self, x):\n",
        "        \"\"\"\n",
        "        Given a dataframe with same features as used passed to fit(), return the same dataframe with the additional\n",
        "        features determined in the fit process.\n",
        "        \"\"\"\n",
        "        x = x.copy()\n",
        "        for feature_idx, feature_name in enumerate(self.features_arr):\n",
        "            if feature_name not in x.columns:\n",
        "                feature_definition = self.feature_definitions[feature_idx]\n",
        "                feat_1 = self.features_arr[feature_definition[0]]\n",
        "                op = feature_definition[1]\n",
        "                feat_2 = self.features_arr[feature_definition[2]]\n",
        "                if op == 'add':\n",
        "                    new_col = pd.DataFrame({feature_name: x[feat_1] + x[feat_2]})\n",
        "                elif op == 'multiply':\n",
        "                    new_col = pd.DataFrame({feature_name: x[feat_1] * x[feat_2]})\n",
        "                elif op == 'subtract':\n",
        "                    new_col = pd.DataFrame({feature_name: x[feat_1] - x[feat_2]})\n",
        "                elif op == 'divide':\n",
        "                    new_col = pd.DataFrame({feature_name: x[feat_1] / x[feat_2]})\n",
        "                x = pd.concat([x, new_col], axis=1)\n",
        "\n",
        "        x.columns = [str(col_name) for col_name in x.columns]\n",
        "        x = self.__clean_data(x)\n",
        "        return x\n",
        "\n",
        "    def __get_metrics_orig_features(self):\n",
        "        \"\"\"\n",
        "        Determine how strong each feature is individually in a 1D model.\n",
        "        \"\"\"\n",
        "\n",
        "        self.feature_scores = []\n",
        "\n",
        "        for col_name in self.features_arr:\n",
        "            if self.target_type == 'regression':\n",
        "                dt = DecisionTreeRegressor()\n",
        "                dt.fit(self.x_train[[col_name]], self.y_train)\n",
        "                y_pred = dt.predict(self.x_test[[col_name]])\n",
        "                r2 = r2_score(self.y_test, y_pred)\n",
        "                self.feature_scores.append(r2)\n",
        "            else:\n",
        "                dt = DecisionTreeClassifier()\n",
        "                dt.fit(self.x_train[[col_name]], self.y_train)\n",
        "                y_pred = dt.predict(self.x_test[[col_name]])\n",
        "                f1 = f1_score(self.y_test, y_pred, average='macro')\n",
        "                self.feature_scores.append(f1)\n",
        "\n",
        "    def __display_features_metrics(self, iteration_number):\n",
        "        if self.verbose >= 1:\n",
        "            print()\n",
        "            print(\"*********************************************************************************\")\n",
        "            print(f\"After Iteration {iteration_number}. Features:\")\n",
        "            lst = list(zip(range(len(self.features_arr)), self.features_arr, self.feature_scores))\n",
        "            best_score = -1\n",
        "            for e in lst:\n",
        "                if e[2] > best_score:\n",
        "                    best_score = e[2]\n",
        "                print(f\"{e[0]:>4}: {round(e[2], 3):>8}, {e[1]}\")\n",
        "            print(f\"Best Score: {best_score}\")\n",
        "\n",
        "    def display_features(self):\n",
        "        lst = list(zip(range(len(self.features_arr)), self.features_arr, self.feature_scores, self.feature_definitions))\n",
        "        for e in lst:\n",
        "            print(f\"{e[0]:>4}: {round(e[2], 3):>8}, {e[1]}\")\n",
        "\n",
        "    def __get_best_feature_score(self):\n",
        "        best_score = -1\n",
        "        for e in self.feature_scores:\n",
        "            if e > best_score:\n",
        "                best_score = e\n",
        "        self.best_score_overall = best_score\n",
        "\n",
        "    def __add_feature(self, best_score, best_operation, i, j, best_train_values, best_test_values):\n",
        "        self.feature_scores.append(best_score)\n",
        "        self.feature_values.append((best_train_values, best_test_values))\n",
        "        self.feature_definitions.append((i, best_operation, j))\n",
        "        self.use_feature.append(True)\n",
        "\n",
        "    def __combine_features(self, starting_idx):\n",
        "        \"\"\"\n",
        "        Each call to this method represents one iteration. It tries combining all features from starting_idx up to all\n",
        "        other features, taking the strongest of these features.\n",
        "        \"\"\"\n",
        "\n",
        "        new_elements = []\n",
        "        best_score = -1\n",
        "        best_operation = \"\"\n",
        "        best_train_values = None\n",
        "        best_test_values = None\n",
        "        train_vals_i, train_vals_j, test_vals_i, test_vals_j = None, None, None, None\n",
        "\n",
        "        def test_feat(operation):\n",
        "            nonlocal best_score, best_operation, best_train_values, best_test_values\n",
        "            nonlocal train_vals_i, train_vals_j, test_vals_i, test_vals_j\n",
        "\n",
        "            temp_x_train = self.x_train.copy()\n",
        "            temp_x_test = self.x_test.copy()\n",
        "\n",
        "            if operation == 'add':\n",
        "                temp_x_train['TEST'] = train_vals_i + train_vals_j\n",
        "                temp_x_test['TEST']  = test_vals_i + test_vals_j\n",
        "            elif operation == 'multiply':\n",
        "                temp_x_train['TEST'] = train_vals_i * train_vals_j\n",
        "                temp_x_test['TEST']  = test_vals_i  * test_vals_j\n",
        "            elif operation == 'subtract':\n",
        "                temp_x_train['TEST'] = train_vals_i - train_vals_j\n",
        "                temp_x_test['TEST']  = test_vals_i  - test_vals_j\n",
        "            elif operation == 'divide':\n",
        "                temp_x_train['TEST'] = np.divide(train_vals_i, train_vals_j, out=np.zeros_like(train_vals_i), where=train_vals_j!=0)\n",
        "                temp_x_test['TEST']  = np.divide(test_vals_i,  test_vals_j,  out=np.zeros_like(test_vals_i),  where=test_vals_j!=0)\n",
        "            else:\n",
        "                assert False\n",
        "\n",
        "            temp_x_train = self.__clean_data(temp_x_train)\n",
        "            temp_x_test  = self.__clean_data(temp_x_test)\n",
        "\n",
        "            if self.target_type == 'regression':\n",
        "                dt = DecisionTreeRegressor()\n",
        "                dt.fit(temp_x_train[['TEST']], self.y_train)\n",
        "                y_pred = dt.predict(temp_x_test[['TEST']])\n",
        "                score = r2_score(self.y_test, y_pred)\n",
        "            else:\n",
        "                dt = DecisionTreeClassifier()\n",
        "                dt.fit(temp_x_train[['TEST']], self.y_train)\n",
        "                y_pred = dt.predict(temp_x_test[['TEST']])\n",
        "                score = f1_score(self.y_test, y_pred, average='macro')\n",
        "\n",
        "            score_parent_i = self.feature_scores[i]\n",
        "            score_parent_j = self.feature_scores[j]\n",
        "            if self.verbose >= 2:\n",
        "                print(f\"Columns: {col_i} and {col_j} -- {operation:<8} -- Score: {score}, parent score: {score_parent_i} and {score_parent_j}\")\n",
        "            if ((score > 0.1) or (score > self.best_score_overall)) and (score > score_parent_i) and (score > score_parent_j) and (score > best_score):\n",
        "                best_score = score\n",
        "                best_operation = operation\n",
        "                best_train_values = temp_x_train['TEST'].values\n",
        "                best_test_values = temp_x_test['TEST'].values\n",
        "\n",
        "        unary_functions = ['none']\n",
        "        if self.test_square:\n",
        "            unary_functions.append('square')\n",
        "        if self.test_sqrt:\n",
        "            unary_functions.append('sqrt')\n",
        "        if self.test_log:\n",
        "            unary_functions.append('log')\n",
        "\n",
        "        # Match all the new elements will all (old & new) elements. To do this, we match each new feature with all\n",
        "        # features before it.\n",
        "        for i in range(starting_idx, len(self.features_arr)):\n",
        "            col_i = self.features_arr[i]\n",
        "            for j in range(i):\n",
        "                col_j = self.features_arr[j]\n",
        "                if self.verbose >= 2:\n",
        "                    print()\n",
        "                    print(\"Testing columns: \", i, j)\n",
        "                best_score = -1\n",
        "                best_operation = \"\"\n",
        "                best_train_values = []\n",
        "                best_test_values = []\n",
        "\n",
        "                for unary_i in unary_functions:\n",
        "                    for unary_j in unary_functions:\n",
        "                        train_vals_i = np.array(self.feature_values[i][0])\n",
        "                        train_vals_j = np.array(self.feature_values[j][0])\n",
        "                        test_vals_i  = np.array(self.feature_values[i][1])\n",
        "                        test_vals_j  = np.array(self.feature_values[j][1])\n",
        "\n",
        "                        test_feat(\"add\")\n",
        "                        test_feat('multiply')\n",
        "                        test_feat(\"subtract\")\n",
        "                        test_feat('divide')\n",
        "                if best_operation != '':\n",
        "                    new_elements.append((self.features_arr[i], best_operation, self.features_arr[j]))\n",
        "                    self.__add_feature(best_score, best_operation, i, j, best_train_values, best_test_values)\n",
        "\n",
        "        start_current_iteration = len(self.features_arr)\n",
        "        self.features_arr.extend(new_elements)\n",
        "        self.__assess_new_features(new_elements, start_current_iteration)\n",
        "        self.__check_arrays()\n",
        "\n",
        "    def __assess_new_features(self, new_elements, starting_idx):\n",
        "        new_cols_df = pd.DataFrame({x: self.feature_values[starting_idx + x_idx][0]\n",
        "                                    for x, x_idx in zip(new_elements, range(len(new_elements)))})\n",
        "        corr_matrix = new_cols_df.corr(method='spearman')\n",
        "        arr_x, arr_y = np.where(np.triu(corr_matrix) > 0.95)\n",
        "\n",
        "        remove_indexes_arr = []\n",
        "        for e in list(zip(arr_x, arr_y)):\n",
        "            if e[0] == e[1]:\n",
        "                continue\n",
        "            col_i = starting_idx + e[0]\n",
        "            col_j = starting_idx + e[1]\n",
        "            if self.feature_scores[col_i] > self.feature_scores[col_j]:\n",
        "                remove_indexes_arr.append(col_j)\n",
        "            else:\n",
        "                remove_indexes_arr.append(col_i)\n",
        "\n",
        "        self.__remove_features(remove_indexes_arr,\n",
        "                               msg=f\"Removing {len(remove_indexes_arr)} redundant features created during iteration\")\n",
        "\n",
        "    def __remove_features(self, remove_indexes_arr, msg):\n",
        "        remove_indexes_arr = list(set(remove_indexes_arr))\n",
        "        remove_indexes_arr.sort(reverse=True)\n",
        "        if self.verbose >= 2:\n",
        "            print()\n",
        "            print(msg)\n",
        "        for i in remove_indexes_arr:\n",
        "            del(self.features_arr[i])\n",
        "            del(self.feature_values[i])\n",
        "            del(self.feature_definitions[i])\n",
        "            del(self.feature_scores[i])\n",
        "\n",
        "    def __check_arrays(self):\n",
        "        assert len(self.features_arr) == \\\n",
        "               len(self.feature_values) == \\\n",
        "               len(self.feature_definitions) == \\\n",
        "               len(self.feature_scores)\n",
        "\n",
        "    def __reduce_original_features(self):\n",
        "        if len(self.features_arr) <= self.max_original_features:\n",
        "            return\n",
        "        bottom_features = np.argsort(self.feature_scores)[ : len(self.features_arr) - self.max_original_features]\n",
        "        self.__remove_features(bottom_features,\n",
        "                               msg=f\"Excluding the least predictive {len(bottom_features)} features from examination\")\n",
        "\n",
        "    def __clean_data(self, df):\n",
        "        df = df.fillna(0)\n",
        "        df = df.replace([-np.inf, np.inf], 0)\n",
        "        return df\n",
        "\n",
        "    def plot_features(self):\n",
        "        for feat_idx, feat_name in enumerate(self.features_arr):\n",
        "            if self.target_type == 'regression':\n",
        "                plt.scatter(x=self.feature_values[feat_idx][0], y=self.y_train)\n",
        "            else:\n",
        "                s = sns.boxplot(x=self.feature_values[feat_idx][0], y=self.y_train)\n",
        "            s.xlabel = feat_name\n",
        "            s.ylabel = 'Target'\n",
        "            plt.title(f\"Relationship of {feat_name} to Target\")\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "######################################################################################################################\n",
        "# Methods to generate X data. These are similar to sklearn's make_classification() and make_regression(), but have\n",
        "# a known f(x)\n",
        "\n",
        "\n",
        "def generate_synthetic_x_data(num_rows=100_000, num_noise_cols=0, num_redundant_cols=0, seed=0):\n",
        "    # The r2 on the full set of features and the individual features can vary greatly depending on the seed.\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    a = np.random.random(num_rows)\n",
        "    b = np.random.random(num_rows)\n",
        "    d = np.random.random(num_rows)\n",
        "    c = np.random.random(num_rows)\n",
        "\n",
        "    # Ensure there are at least 4 columns\n",
        "    data = {\n",
        "        \"a\": a,\n",
        "        \"b\": b,\n",
        "        \"c\": c,\n",
        "        \"d\": d,\n",
        "    }\n",
        "\n",
        "    for i in range(num_noise_cols):\n",
        "        data[f\"Noise_{i}\"] = np.random.random(num_rows)\n",
        "\n",
        "    for i in range(num_redundant_cols):\n",
        "        noise = (np.random.random(num_rows) / 10.0)\n",
        "        if i % 4 == 0:\n",
        "            data[f\"Redundant_A_{i}\"] = data['a'] + noise\n",
        "        if i % 4 == 1:\n",
        "            data[f\"Redundant_B_{i}\"] = data['b'] + noise\n",
        "        if i % 4 == 2:\n",
        "            data[f\"Redundant_C_{i}\"] = data['c'] + noise\n",
        "        if i % 4 == 3:\n",
        "            data[f\"Redundant_D_{i}\"] = data['d'] + noise\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "######################################################################################################################\n",
        "# Methods to generate y column. These are similar to sklearn's make_classification() and make_regression(), but have\n",
        "# a known f(x), relating the x columns to the y column\n",
        "\n",
        "def generate_synthetic_y_formula_4_0(df):\n",
        "    y = df['a'] + df['b']\n",
        "    return y\n",
        "\n",
        "\n",
        "def generate_synthetic_y_formula_4_1(df):\n",
        "    y = ((((5.3 * df['a']) + df['b']) * df['c']) / df['d']) - ((5.4 * df['b']) - (2.1 * df['c']))\n",
        "    return y\n",
        "\n",
        "\n",
        "def generate_synthetic_y_formula_4_2(df):\n",
        "    y = df['a'] * df['b'] * df['c'] + df['d']\n",
        "    return y\n",
        "\n",
        "\n",
        "def generate_synthetic_y_formula_4_3(df):\n",
        "    y = df['a'] * df['b'] * df['c'] * df['d']\n",
        "    return y\n",
        "\n",
        "\n",
        "def generate_synthetic_y_formula_4_4(df):\n",
        "    y = pd.Series(np.where(df['a'] > df['a'].median(),\n",
        "                           df['a'] * df['b'],\n",
        "                           df['c'] / df['d']))\n",
        "    return y\n",
        "\n",
        "\n",
        "def generate_synthetic_y_formula_4_5(df):\n",
        "    # 'C' is used in both cases\n",
        "    y = pd.Series(np.where(df['a'] > df['a'].median(),\n",
        "                           df['a'] * df['b'] * df['c'],\n",
        "                           df['c'] / df['d']))\n",
        "    return y\n",
        "\n",
        "\n",
        "def generate_synthetic_y_formula_4_6(df):\n",
        "    # 'A^2' is used, and sqrt of 'C'\n",
        "    y = pd.Series(np.where(df['a'] > df['a'].median(),\n",
        "                           df['a'] * df['a'] * df['c'],\n",
        "                           np.sqrt(df['c']) / df['d']))\n",
        "    return y"
      ],
      "metadata": {
        "id": "IiDaZkf1M0Uu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b5NG_Q_gMNB1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the data\n",
        "iris = load_iris()\n",
        "x, y = iris.data, iris.target\n",
        "x = pd.DataFrame(x, columns=iris.feature_names)\n",
        "\n",
        "# Split the data into train and test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
        "\n",
        "# Engineer new features\n",
        "ff = FormulaFeatures()\n",
        "ff.fit(x_train, y_train)\n",
        "x_train_extended = ff.transform(x_train)\n",
        "x_test_extended = ff.transform(x_test)\n",
        "\n",
        "# Train a decision tree and make predictions\n",
        "dt = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
        "dt.fit(x_train_extended, y_train)\n",
        "y_pred = dt.predict(x_test_extended)"
      ]
    }
  ]
}