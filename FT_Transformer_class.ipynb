{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPgf1nZg/+TmT6nv4cg72D"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Tl8Fsqu4Jn_e"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# FT-Transformer 구현 (TabTransformerModel 스타일과 통일)\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# BaseNNModel (이미 있으시면 아래 클래스만 가져가시면 됩니다)\n",
        "# -----------------------------------------------------------\n",
        "class BaseNNModel(nn.Module, ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self, **kwargs):\n",
        "        super(BaseNNModel, self).__init__()\n",
        "\n",
        "    @abstractmethod\n",
        "    def build_network(self) -> nn.Module:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        pass\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# FT-Transformer Model\n",
        "# -----------------------------------------------------------\n",
        "class FTTransformerModel(BaseNNModel):\n",
        "    \"\"\"\n",
        "    FT-Transformer (Feature Tokenizer + Transformer) 구현.\n",
        "\n",
        "    - input_dim: 전체 feature 개수\n",
        "    - cat_features: 범주형 feature 인덱스 리스트 (X의 column index)\n",
        "    - cat_dims: 각 범주형 feature의 cardinality (고유값 개수)\n",
        "        * 각 범주형 값은 0 ~ (cardinality-1)의 int 인덱스로 들어온다고 가정.\n",
        "    - 나머지 feature는 모두 numerical feature로 처리.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        cat_features: list[int] | None = None,\n",
        "        cat_dims: list[int] | None = None,\n",
        "        d_token: int = 32,\n",
        "        n_heads: int = 4,\n",
        "        n_layers: int = 3,\n",
        "        dim_feedforward: int | None = None,\n",
        "        attn_dropout: float = 0.1,\n",
        "        token_dropout: float = 0.0,\n",
        "        mlp_dropout: float = 0.2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.cat_features = cat_features or []\n",
        "        self.cat_dims = cat_dims or []\n",
        "        self.d_token = d_token\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.dim_feedforward = dim_feedforward or (4 * d_token)  # 논문 스타일: 4 * d\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.token_dropout_p = token_dropout\n",
        "        self.mlp_dropout = mlp_dropout\n",
        "\n",
        "        self.build_network()\n",
        "\n",
        "    def build_network(self) -> nn.Module:\n",
        "        # --- feature 개수 세기 ---\n",
        "        assert len(self.cat_features) == len(\n",
        "            self.cat_dims\n",
        "        ), \"cat_features 개수와 cat_dims 개수가 다릅니다.\"\n",
        "\n",
        "        self.n_cat = len(self.cat_features)\n",
        "        self.n_num = self.input_dim - self.n_cat\n",
        "\n",
        "        # ===== feature 인덱스를 buffer로 저장 (forward에서 재사용) =====\n",
        "        if self.n_cat > 0:\n",
        "            cat_idx = torch.tensor(self.cat_features, dtype=torch.long)\n",
        "            self.register_buffer(\"cat_idx_tensor\", cat_idx, persistent=False)\n",
        "        else:\n",
        "            self.register_buffer(\n",
        "                \"cat_idx_tensor\",\n",
        "                torch.zeros(0, dtype=torch.long),\n",
        "                persistent=False,\n",
        "            )\n",
        "\n",
        "        num_idx = [i for i in range(self.input_dim) if i not in self.cat_features]\n",
        "        if len(num_idx) != self.n_num:\n",
        "            raise ValueError(\"numerical feature 개수 계산이 잘못되었습니다.\")\n",
        "        num_idx = torch.tensor(num_idx, dtype=torch.long)\n",
        "        self.register_buffer(\"num_idx_tensor\", num_idx, persistent=False)\n",
        "\n",
        "        # ===== Feature Tokenizer =====\n",
        "        # 1) Numerical features: T_j = b_j + x_j * W_j\n",
        "        if self.n_num > 0:\n",
        "            # shape: (1, n_num, d_token)로 두고 broadcasting 사용\n",
        "            self.num_weight = nn.Parameter(\n",
        "                torch.empty(1, self.n_num, self.d_token)\n",
        "            )\n",
        "            self.num_bias = nn.Parameter(\n",
        "                torch.empty(1, self.n_num, self.d_token)\n",
        "            )\n",
        "            nn.init.normal_(self.num_weight, std=0.02)\n",
        "            nn.init.normal_(self.num_bias, std=0.02)\n",
        "        else:\n",
        "            self.num_weight = None\n",
        "            self.num_bias = None\n",
        "\n",
        "        # 2) Categorical features: T_j = b_j + Embedding(x_j)\n",
        "        if self.n_cat > 0:\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "            for c in self.cat_dims:\n",
        "                emb = nn.Embedding(num_embeddings=c, embedding_dim=self.d_token)\n",
        "                nn.init.normal_(emb.weight, std=0.02)\n",
        "                self.cat_embeddings.append(emb)\n",
        "\n",
        "            # 각 범주형 feature별 bias (1, n_cat, d)\n",
        "            self.cat_bias = nn.Parameter(\n",
        "                torch.empty(1, self.n_cat, self.d_token)\n",
        "            )\n",
        "            nn.init.normal_(self.cat_bias, std=0.02)\n",
        "        else:\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "            self.cat_bias = None\n",
        "\n",
        "        # CLS 토큰\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.d_token))\n",
        "        nn.init.normal_(self.cls_token, std=0.02)\n",
        "\n",
        "        # 토큰 dropout (feature-level)\n",
        "        self.token_dropout = nn.Dropout(self.token_dropout_p)\n",
        "\n",
        "        # ===== Transformer Encoder (PreNorm) =====\n",
        "        # FT-Transformer는 PreNorm 구조를 사용 → norm_first=True\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.d_token,\n",
        "            nhead=self.n_heads,\n",
        "            dim_feedforward=self.dim_feedforward,\n",
        "            dropout=self.attn_dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True,\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=self.n_layers,\n",
        "        )\n",
        "\n",
        "        # ===== Head (Prediction) =====\n",
        "        # 최종: ŷ = Linear(ReLU(LayerNorm(T_CLS^L)))  (binary classification logit)\n",
        "        self.final_norm = nn.LayerNorm(self.d_token)\n",
        "        self.final_relu = nn.ReLU()\n",
        "        self.final_dropout = nn.Dropout(self.mlp_dropout)\n",
        "        self.head = nn.Linear(self.d_token, 1)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _tokenize(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, input_dim)\n",
        "        return: tokens T (B, k, d_token)  -- k = n_num + n_cat\n",
        "        \"\"\"\n",
        "        B, D = x.shape\n",
        "\n",
        "        tokens_list = []\n",
        "\n",
        "        # ----- numerical -----\n",
        "        if self.n_num > 0:\n",
        "            # (B, n_num)\n",
        "            x_num = x.index_select(1, self.num_idx_tensor).unsqueeze(-1)  # (B, n_num, 1)\n",
        "            # (1, n_num, d) broadcast + (B, n_num, 1) -> (B, n_num, d)\n",
        "            T_num = self.num_bias + x_num * self.num_weight\n",
        "            tokens_list.append(T_num)\n",
        "\n",
        "        # ----- categorical -----\n",
        "        if self.n_cat > 0:\n",
        "            x_cat = x.index_select(1, self.cat_idx_tensor).long()  # (B, n_cat)\n",
        "            cat_tokens = []\n",
        "            for j, emb in enumerate(self.cat_embeddings):\n",
        "                tj = emb(x_cat[:, j])  # (B, d_token)\n",
        "                cat_tokens.append(tj.unsqueeze(1))  # (B, 1, d_token)\n",
        "            T_cat = torch.cat(cat_tokens, dim=1)  # (B, n_cat, d_token)\n",
        "            T_cat = T_cat + self.cat_bias  # broadcast: (1, n_cat, d)\n",
        "            tokens_list.append(T_cat)\n",
        "\n",
        "        if len(tokens_list) == 0:\n",
        "            raise ValueError(\"No features (numerical or categorical) were provided.\")\n",
        "\n",
        "        # feature dimension 기준으로 concat\n",
        "        T = torch.cat(tokens_list, dim=1)  # (B, k, d_token)\n",
        "        return T\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, input_dim), float32 (범주형도 float->long 캐스팅 예정)\n",
        "        return: (B, 1) logits (sigmoid 이전)\n",
        "        \"\"\"\n",
        "        B, D = x.shape\n",
        "\n",
        "        # 1) Feature Tokenizer\n",
        "        T = self._tokenize(x)  # (B, k, d)\n",
        "        T = self.token_dropout(T)\n",
        "\n",
        "        # 2) [CLS] 토큰 붙이기\n",
        "        cls = self.cls_token.expand(B, -1, -1)  # (B, 1, d)\n",
        "        T = torch.cat([cls, T], dim=1)  # (B, 1 + k, d)\n",
        "\n",
        "        # 3) Transformer 인코더\n",
        "        Z = self.transformer(T)  # (B, 1 + k, d)\n",
        "\n",
        "        # 4) [CLS] 벡터에서 예측\n",
        "        cls_rep = Z[:, 0, :]  # (B, d)\n",
        "        h = self.final_norm(cls_rep)\n",
        "        h = self.final_relu(h)\n",
        "        h = self.final_dropout(h)\n",
        "        logits = self.head(h)  # (B, 1)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ===========================================================\n",
        "# MLP + TabTransformer + FT-Transformer 전체 데모 코드\n",
        "# ===========================================================\n",
        "\n",
        "# Standard Library\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Third Party\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import average_precision_score, log_loss, roc_auc_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Base Neural Network Model\n",
        "# -----------------------------------------------------------\n",
        "class BaseNNModel(nn.Module, ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self, **kwargs):\n",
        "        super(BaseNNModel, self).__init__()\n",
        "\n",
        "    @abstractmethod\n",
        "    def build_network(self) -> nn.Module:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        pass\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# MLP Model\n",
        "# -----------------------------------------------------------\n",
        "class MLPModel(BaseNNModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dims: list[int],\n",
        "        cat_features: list[int] | None = None,\n",
        "        cat_dims: list[int] | None = None,\n",
        "        emb_dim: int = 8,\n",
        "    ):\n",
        "        super(MLPModel, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.cat_features = cat_features or []\n",
        "        self.cat_dims = cat_dims or []\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.embeddings = None\n",
        "        self.network = self.build_network()\n",
        "\n",
        "    def build_network(self) -> nn.Sequential:\n",
        "        # categorical embedding layer\n",
        "        if len(self.cat_dims) > 0:\n",
        "            self.embeddings = nn.ModuleList(\n",
        "                [nn.Embedding(cat_dim, self.emb_dim) for cat_dim in self.cat_dims]\n",
        "            )\n",
        "\n",
        "        combined_input_dim = (\n",
        "            self.input_dim - len(self.cat_features)\n",
        "            + len(self.cat_features) * self.emb_dim\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        dims = [combined_input_dim] + self.hidden_dims\n",
        "\n",
        "        for i in range(len(dims) - 1):\n",
        "            hidden_layer = nn.Linear(dims[i], dims[i + 1])\n",
        "            nn.init.kaiming_normal_(\n",
        "                hidden_layer.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
        "            )\n",
        "\n",
        "            layers.append(hidden_layer)\n",
        "            layers.append(nn.BatchNorm1d(dims[i + 1]))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        # output layer (logit)\n",
        "        layers.append(nn.Linear(dims[-1], 1))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        continuous_features = []\n",
        "        embedded_features = []\n",
        "        cat_idx = 0\n",
        "\n",
        "        for i in range(self.input_dim):\n",
        "            if i in self.cat_features:\n",
        "                cat_values = x[:, i].long()\n",
        "                embedded = self.embeddings[cat_idx](cat_values)\n",
        "                embedded_features.append(embedded)\n",
        "                cat_idx += 1\n",
        "            else:\n",
        "                continuous_features.append(x[:, i : i + 1])\n",
        "\n",
        "        if continuous_features:\n",
        "            continuous_features = torch.cat(continuous_features, dim=1)\n",
        "        else:\n",
        "            continuous_features = None\n",
        "\n",
        "        if embedded_features:\n",
        "            embedded_features = torch.cat(embedded_features, dim=1)\n",
        "        else:\n",
        "            embedded_features = None\n",
        "\n",
        "        if continuous_features is not None and embedded_features is not None:\n",
        "            combined_features = torch.cat(\n",
        "                [continuous_features, embedded_features], dim=1\n",
        "            )\n",
        "        elif embedded_features is not None:\n",
        "            combined_features = embedded_features\n",
        "        elif continuous_features is not None:\n",
        "            combined_features = continuous_features\n",
        "        else:\n",
        "            raise ValueError(\"No features found for forward pass.\")\n",
        "\n",
        "        logits = self.network(combined_features)\n",
        "        return logits  # (B, 1)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# TabTransformer Model (개선 버전)\n",
        "# -----------------------------------------------------------\n",
        "class TabTransformerModel(BaseNNModel):\n",
        "    \"\"\"\n",
        "    - input_dim: 전체 feature 개수\n",
        "    - cat_features: 범주형 feature 인덱스 리스트 (X의 column index)\n",
        "    - cat_dims: 각 범주형 feature의 cardinality (고유값 개수)\n",
        "      * 각 범주형 값은 0 ~ (cardinality-1)의 int 인덱스로 들어온다고 가정.\n",
        "    - 나머지 feature는 모두 continuous feature로 처리.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        cat_features: list[int] | None = None,\n",
        "        cat_dims: list[int] | None = None,\n",
        "        d_token: int = 32,\n",
        "        n_heads: int = 4,\n",
        "        n_layers: int = 2,\n",
        "        dim_feedforward: int | None = None,\n",
        "        attn_dropout: float = 0.1,\n",
        "        embedding_dropout: float = 0.1,\n",
        "        add_cls: bool = False,\n",
        "        pooling: str = \"concat\",  # \"concat\" or \"cls\"\n",
        "        cont_proj: str = \"linear\",  # \"none\" or \"linear\"\n",
        "        mlp_hidden_dims: tuple[int, ...] = (128, 64),\n",
        "        mlp_dropout: float = 0.2,\n",
        "        use_missing_category: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.cat_features = cat_features or []\n",
        "        self.cat_dims = cat_dims or []\n",
        "        self.d_token = d_token\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.dim_feedforward = dim_feedforward or (4 * d_token)\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.embedding_dropout_p = embedding_dropout\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "        self.cont_proj = cont_proj\n",
        "        self.mlp_hidden_dims = mlp_hidden_dims\n",
        "        self.mlp_dropout = mlp_dropout\n",
        "        self.use_missing_category = use_missing_category\n",
        "\n",
        "        self.build_network()\n",
        "\n",
        "    def build_network(self) -> nn.Module:\n",
        "        assert len(self.cat_features) == len(\n",
        "            self.cat_dims\n",
        "        ), \"cat_features 개수와 cat_dims 개수가 다릅니다.\"\n",
        "\n",
        "        # 연속형 / 범주형 개수\n",
        "        self.n_cat = len(self.cat_features)\n",
        "        self.n_cont = self.input_dim - self.n_cat\n",
        "\n",
        "        # ----- 인덱스 buffer 등록 -----\n",
        "        if self.n_cat > 0:\n",
        "            cat_idx = torch.tensor(self.cat_features, dtype=torch.long)\n",
        "            self.register_buffer(\"cat_idx_tensor\", cat_idx, persistent=False)\n",
        "        else:\n",
        "            self.register_buffer(\n",
        "                \"cat_idx_tensor\",\n",
        "                torch.zeros(0, dtype=torch.long),\n",
        "                persistent=False,\n",
        "            )\n",
        "\n",
        "        cont_idx = [i for i in range(self.input_dim) if i not in self.cat_features]\n",
        "        if len(cont_idx) != self.n_cont:\n",
        "            raise ValueError(\"continuous feature 개수 계산이 잘못되었습니다.\")\n",
        "        cont_idx = torch.tensor(cont_idx, dtype=torch.long)\n",
        "        self.register_buffer(\"cont_idx_tensor\", cont_idx, persistent=False)\n",
        "\n",
        "        # ====== Categorical path ======\n",
        "        if self.n_cat == 0:\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "            self.col_embedding = None\n",
        "        else:\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "            for c in self.cat_dims:\n",
        "                n_embeddings = c + (1 if self.use_missing_category else 0)\n",
        "                emb = nn.Embedding(\n",
        "                    num_embeddings=n_embeddings,\n",
        "                    embedding_dim=self.d_token,\n",
        "                )\n",
        "                nn.init.normal_(emb.weight, std=0.02)\n",
        "                self.cat_embeddings.append(emb)\n",
        "\n",
        "            # column embedding (선택적 token-type 역할)\n",
        "            self.col_embedding = nn.Embedding(self.n_cat, self.d_token)\n",
        "            nn.init.normal_(self.col_embedding.weight, std=0.02)\n",
        "\n",
        "        if self.add_cls:\n",
        "            self.cls_token = nn.Parameter(torch.zeros(1, 1, self.d_token))\n",
        "            nn.init.normal_(self.cls_token, std=0.02)\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout(self.embedding_dropout_p)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.d_token,\n",
        "            nhead=self.n_heads,\n",
        "            dim_feedforward=self.dim_feedforward,\n",
        "            dropout=self.attn_dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=False,  # 논문 스타일(Post-LN)에 가깝게\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=self.n_layers)\n",
        "\n",
        "        # ====== 연속형 path ======\n",
        "        if self.n_cont > 0:\n",
        "            self.cont_bn = nn.BatchNorm1d(self.n_cont)\n",
        "            if self.cont_proj == \"linear\":\n",
        "                self.cont_linear = nn.Linear(self.n_cont, self.d_token)\n",
        "                nn.init.kaiming_uniform_(\n",
        "                    self.cont_linear.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
        "                )\n",
        "                cont_out_dim = self.d_token\n",
        "            else:\n",
        "                self.cont_linear = nn.Identity()\n",
        "                cont_out_dim = self.n_cont\n",
        "        else:\n",
        "            self.cont_bn = None\n",
        "            self.cont_linear = None\n",
        "            cont_out_dim = 0\n",
        "\n",
        "        # ====== Head (logit 출력) ======\n",
        "        backbone_out = (\n",
        "            self.d_token if self.pooling == \"cls\" else self.n_cat * self.d_token\n",
        "        )\n",
        "        in_dim = backbone_out + cont_out_dim\n",
        "\n",
        "        layers = []\n",
        "        prev = in_dim\n",
        "        for h in self.mlp_hidden_dims:\n",
        "            lin = nn.Linear(prev, h)\n",
        "            nn.init.kaiming_uniform_(lin.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "            layers.extend(\n",
        "                [lin, nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(self.mlp_dropout)]\n",
        "            )\n",
        "            prev = h\n",
        "        layers.append(nn.Linear(prev, 1))  # logits\n",
        "        self.head = nn.Sequential(*layers)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _encode_categoricals(self, x_cat: torch.LongTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x_cat: (B, n_cat) -> contextualized embedding\n",
        "        - use_missing_category=True 인 경우, 전처리에서 missing을 마지막 인덱스로 매핑했다고 가정.\n",
        "        \"\"\"\n",
        "        B = x_cat.size(0)\n",
        "        if self.n_cat == 0:\n",
        "            return torch.zeros(\n",
        "                B,\n",
        "                self.d_token if self.pooling == \"cls\" else 0,\n",
        "                device=x_cat.device,\n",
        "                dtype=torch.float32,\n",
        "            )\n",
        "\n",
        "        tok_list = []\n",
        "        for j, emb in enumerate(self.cat_embeddings):\n",
        "            tok = emb(x_cat[:, j])  # (B, d)\n",
        "            if self.col_embedding is not None:\n",
        "                tok = tok + self.col_embedding.weight[j]  # (d,)\n",
        "            tok_list.append(tok.unsqueeze(1))  # (B, 1, d)\n",
        "\n",
        "        x_tok = torch.cat(tok_list, dim=1)  # (B, n_cat, d)\n",
        "\n",
        "        if self.add_cls:\n",
        "            cls = self.cls_token.expand(B, -1, -1)  # (B, 1, d)\n",
        "            x_tok = torch.cat([cls, x_tok], dim=1)  # (B, 1 + n_cat, d)\n",
        "\n",
        "        x_tok = self.embedding_dropout(x_tok)\n",
        "        z = self.transformer(x_tok)  # (B, T, d)\n",
        "\n",
        "        if self.pooling == \"cls\" and self.add_cls:\n",
        "            out = z[:, 0, :]  # (B, d)\n",
        "        elif self.pooling == \"cls\":\n",
        "            out = z.mean(dim=1)  # (B, d)\n",
        "        else:\n",
        "            if self.add_cls:\n",
        "                z = z[:, 1:, :]  # CLS 제거\n",
        "            out = z.reshape(B, -1)  # (B, n_cat*d)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, input_dim)\n",
        "        - self.cat_features 인덱스에 해당하는 column은 categorical, 나머지는 continuous로 사용.\n",
        "        - 출력: (B, 1) logits (sigmoid 전)\n",
        "        \"\"\"\n",
        "        B, D = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        # ----- categorical split -----\n",
        "        if self.n_cat > 0:\n",
        "            x_cat = x.index_select(1, self.cat_idx_tensor).long()\n",
        "        else:\n",
        "            x_cat = torch.zeros(B, 0, dtype=torch.long, device=device)\n",
        "\n",
        "        # ----- continuous split -----\n",
        "        if self.n_cont > 0:\n",
        "            x_cont = x.index_select(1, self.cont_idx_tensor).float()\n",
        "        else:\n",
        "            x_cont = None\n",
        "\n",
        "        # categorical path\n",
        "        z_cat = self._encode_categoricals(x_cat)\n",
        "\n",
        "        # continuous path\n",
        "        if (x_cont is not None) and (self.n_cont > 0):\n",
        "            if x_cont.ndim == 1:\n",
        "                x_cont = x_cont.unsqueeze(1)\n",
        "            x_cont = self.cont_bn(x_cont)\n",
        "            x_cont = self.cont_linear(x_cont)\n",
        "            z = torch.cat([z_cat, x_cont], dim=1)\n",
        "        else:\n",
        "            z = z_cat\n",
        "\n",
        "        logits = self.head(z)  # (B, 1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# FT-Transformer Model (단일 클래스)\n",
        "# -----------------------------------------------------------\n",
        "class FTTransformerModel(BaseNNModel):\n",
        "    \"\"\"\n",
        "    FT-Transformer (Feature Tokenizer + Transformer) 구현.\n",
        "\n",
        "    - input_dim: 전체 feature 개수\n",
        "    - cat_features: 범주형 feature 인덱스 리스트 (X의 column index)\n",
        "    - cat_dims: 각 범주형 feature의 cardinality (고유값 개수)\n",
        "        * 각 범주형 값은 0 ~ (cardinality-1)의 int 인덱스로 들어온다고 가정.\n",
        "    - 나머지 feature는 모두 numerical feature로 처리.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        cat_features: list[int] | None = None,\n",
        "        cat_dims: list[int] | None = None,\n",
        "        d_token: int = 32,\n",
        "        n_heads: int = 4,\n",
        "        n_layers: int = 3,\n",
        "        dim_feedforward: int | None = None,\n",
        "        attn_dropout: float = 0.1,\n",
        "        token_dropout: float = 0.0,\n",
        "        mlp_dropout: float = 0.2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.cat_features = cat_features or []\n",
        "        self.cat_dims = cat_dims or []\n",
        "        self.d_token = d_token\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.dim_feedforward = dim_feedforward or (4 * d_token)\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.token_dropout_p = token_dropout\n",
        "        self.mlp_dropout = mlp_dropout\n",
        "\n",
        "        self.build_network()\n",
        "\n",
        "    def build_network(self) -> nn.Module:\n",
        "        # --- feature 개수 세기 ---\n",
        "        assert len(self.cat_features) == len(\n",
        "            self.cat_dims\n",
        "        ), \"cat_features와 cat_dims 길이가 다릅니다.\"\n",
        "\n",
        "        self.n_cat = len(self.cat_features)\n",
        "        self.n_num = self.input_dim - self.n_cat\n",
        "\n",
        "        # ===== feature 인덱스를 buffer로 저장 =====\n",
        "        if self.n_cat > 0:\n",
        "            cat_idx = torch.tensor(self.cat_features, dtype=torch.long)\n",
        "            self.register_buffer(\"cat_idx_tensor\", cat_idx, persistent=False)\n",
        "        else:\n",
        "            self.register_buffer(\n",
        "                \"cat_idx_tensor\",\n",
        "                torch.zeros(0, dtype=torch.long),\n",
        "                persistent=False,\n",
        "            )\n",
        "\n",
        "        num_idx = [i for i in range(self.input_dim) if i not in self.cat_features]\n",
        "        if len(num_idx) != self.n_num:\n",
        "            raise ValueError(\"numerical feature 개수 계산이 잘못되었습니다.\")\n",
        "        num_idx = torch.tensor(num_idx, dtype=torch.long)\n",
        "        self.register_buffer(\"num_idx_tensor\", num_idx, persistent=False)\n",
        "\n",
        "        # ===== Feature Tokenizer =====\n",
        "        # 1) Numerical features: T_j = b_j + x_j * W_j\n",
        "        if self.n_num > 0:\n",
        "            self.num_weight = nn.Parameter(\n",
        "                torch.empty(1, self.n_num, self.d_token)\n",
        "            )\n",
        "            self.num_bias = nn.Parameter(\n",
        "                torch.empty(1, self.n_num, self.d_token)\n",
        "            )\n",
        "            nn.init.normal_(self.num_weight, std=0.02)\n",
        "            nn.init.normal_(self.num_bias, std=0.02)\n",
        "        else:\n",
        "            self.num_weight = None\n",
        "            self.num_bias = None\n",
        "\n",
        "        # 2) Categorical features: T_j = b_j + Embedding(x_j)\n",
        "        if self.n_cat > 0:\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "            for c in self.cat_dims:\n",
        "                emb = nn.Embedding(num_embeddings=c, embedding_dim=self.d_token)\n",
        "                nn.init.normal_(emb.weight, std=0.02)\n",
        "                self.cat_embeddings.append(emb)\n",
        "\n",
        "            self.cat_bias = nn.Parameter(\n",
        "                torch.empty(1, self.n_cat, self.d_token)\n",
        "            )\n",
        "            nn.init.normal_(self.cat_bias, std=0.02)\n",
        "        else:\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "            self.cat_bias = None\n",
        "\n",
        "        # CLS 토큰\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.d_token))\n",
        "        nn.init.normal_(self.cls_token, std=0.02)\n",
        "\n",
        "        # 토큰 dropout\n",
        "        self.token_dropout = nn.Dropout(self.token_dropout_p)\n",
        "\n",
        "        # ===== Transformer Encoder (PreNorm) =====\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.d_token,\n",
        "            nhead=self.n_heads,\n",
        "            dim_feedforward=self.dim_feedforward,\n",
        "            dropout=self.attn_dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True,  # FT-Transformer: PreNorm\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=self.n_layers,\n",
        "        )\n",
        "\n",
        "        # ===== Head (Prediction) =====\n",
        "        self.final_norm = nn.LayerNorm(self.d_token)\n",
        "        self.final_relu = nn.ReLU()\n",
        "        self.final_dropout = nn.Dropout(self.mlp_dropout)\n",
        "        self.head = nn.Linear(self.d_token, 1)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _tokenize(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, input_dim)\n",
        "        return: tokens T (B, k, d_token)  -- k = n_num + n_cat\n",
        "        \"\"\"\n",
        "        B, D = x.shape\n",
        "\n",
        "        tokens_list = []\n",
        "\n",
        "        # ----- numerical -----\n",
        "        if self.n_num > 0:\n",
        "            x_num = x.index_select(1, self.num_idx_tensor).unsqueeze(-1)  # (B, n_num, 1)\n",
        "            T_num = self.num_bias + x_num * self.num_weight  # (B, n_num, d)\n",
        "            tokens_list.append(T_num)\n",
        "\n",
        "        # ----- categorical -----\n",
        "        if self.n_cat > 0:\n",
        "            x_cat = x.index_select(1, self.cat_idx_tensor).long()  # (B, n_cat)\n",
        "            cat_tokens = []\n",
        "            for j, emb in enumerate(self.cat_embeddings):\n",
        "                tj = emb(x_cat[:, j])  # (B, d_token)\n",
        "                cat_tokens.append(tj.unsqueeze(1))  # (B, 1, d_token)\n",
        "            T_cat = torch.cat(cat_tokens, dim=1)  # (B, n_cat, d_token)\n",
        "            T_cat = T_cat + self.cat_bias  # (B, n_cat, d_token)\n",
        "            tokens_list.append(T_cat)\n",
        "\n",
        "        if len(tokens_list) == 0:\n",
        "            raise ValueError(\"No features (numerical or categorical) were provided.\")\n",
        "\n",
        "        T = torch.cat(tokens_list, dim=1)  # (B, k, d_token)\n",
        "        return T\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, input_dim), float32 (범주형도 float->long 캐스팅 예정)\n",
        "        return: (B, 1) logits\n",
        "        \"\"\"\n",
        "        B, D = x.shape\n",
        "\n",
        "        # 1) Feature Tokenizer\n",
        "        T = self._tokenize(x)  # (B, k, d)\n",
        "        T = self.token_dropout(T)\n",
        "\n",
        "        # 2) [CLS] 토큰 붙이기\n",
        "        cls = self.cls_token.expand(B, -1, -1)  # (B, 1, d)\n",
        "        T = torch.cat([cls, T], dim=1)  # (B, 1 + k, d)\n",
        "\n",
        "        # 3) Transformer\n",
        "        Z = self.transformer(T)  # (B, 1 + k, d)\n",
        "\n",
        "        # 4) [CLS]로 예측\n",
        "        cls_rep = Z[:, 0, :]  # (B, d)\n",
        "        h = self.final_norm(cls_rep)\n",
        "        h = self.final_relu(h)\n",
        "        h = self.final_dropout(h)\n",
        "        logits = self.head(h)  # (B, 1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Deep Learning Binary Classifier\n",
        "# -----------------------------------------------------------\n",
        "class DeepLearningBinaryClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_type: str = \"mlp\",\n",
        "        model_params: dict | None = None,\n",
        "    ):\n",
        "        self.model_type = model_type\n",
        "        self.model_params = model_params or {}\n",
        "        self.model = None\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def _build_model(self) -> BaseNNModel:\n",
        "        model_registry = {\n",
        "            \"mlp\": MLPModel,\n",
        "            \"tabtransformer\": TabTransformerModel,\n",
        "            \"fttransformer\": FTTransformerModel,\n",
        "        }\n",
        "\n",
        "        if self.model_type not in model_registry:\n",
        "            raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
        "\n",
        "        valid_params = {\n",
        "            k: v for k, v in self.model_params.items() if k not in [\"loss_fn\", \"lr\"]\n",
        "        }\n",
        "\n",
        "        model_class = model_registry[self.model_type](**valid_params)\n",
        "        return model_class\n",
        "\n",
        "    def _get_loss_fn(self) -> nn.Module:\n",
        "        loss_name = self.model_params.get(\"loss_fn\", \"logloss\")\n",
        "        if loss_name == \"logloss\":\n",
        "            return nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss function: {loss_name}\")\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        X: np.ndarray,\n",
        "        y: np.ndarray,\n",
        "        sample_weight: np.ndarray | None = None,\n",
        "        eval_set: list[tuple[np.ndarray, np.ndarray]] | None = None,\n",
        "        eval_metric: list[str] | None = None,\n",
        "        max_epochs: int = 10,\n",
        "        patience: int | None = None,\n",
        "        batch_size: int = 128,\n",
        "        verbose: bool = True,\n",
        "    ) -> \"DeepLearningBinaryClassifier\":\n",
        "\n",
        "        lr = self.model_params.get(\"lr\", 0.001)\n",
        "        eval_metric = eval_metric or [\"logloss\"]\n",
        "\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(self.device)\n",
        "\n",
        "        if eval_set is not None:\n",
        "            x_eval_tensor = torch.tensor(\n",
        "                eval_set[0][0], dtype=torch.float32\n",
        "            ).to(self.device)\n",
        "            y_eval_true = eval_set[0][1]\n",
        "        else:\n",
        "            x_eval_tensor = None\n",
        "            y_eval_true = None\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight_tensor = torch.tensor(\n",
        "                sample_weight, dtype=torch.float32\n",
        "            ).to(self.device)\n",
        "        else:\n",
        "            sample_weight_tensor = torch.ones_like(y_tensor, dtype=torch.float32)\n",
        "\n",
        "        train_dataset = TensorDataset(X_tensor, y_tensor, sample_weight_tensor)\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True\n",
        "        )\n",
        "\n",
        "        if self.model is None:\n",
        "            self.model = self._build_model().to(self.device)\n",
        "\n",
        "        loss_fn = self._get_loss_fn()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "        patience_counter = 0\n",
        "        best_metric = float(\"inf\")\n",
        "        best_model_weights = None\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            self.model.train()\n",
        "            epoch_loss = 0.0\n",
        "            n_batches = 0\n",
        "\n",
        "            for x_batch, y_batch, weight_batch in train_dataloader:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                y_pred_logits = self.model(x_batch)\n",
        "                loss = loss_fn(y_pred_logits, y_batch)\n",
        "                weighted_loss = (loss * weight_batch).sum() / weight_batch.sum()\n",
        "\n",
        "                weighted_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += weighted_loss.item()\n",
        "                n_batches += 1\n",
        "\n",
        "            if verbose:\n",
        "                print(\n",
        "                    f\"Epoch {epoch + 1}/{max_epochs} \"\n",
        "                    f\"- [train] loss: {epoch_loss / max(1, n_batches):.6f}\"\n",
        "                )\n",
        "\n",
        "            # evaluation\n",
        "            if eval_set is not None:\n",
        "                self.model.eval()\n",
        "                with torch.no_grad():\n",
        "                    y_eval_logits = self.model(x_eval_tensor)\n",
        "                    y_eval_pred = torch.sigmoid(y_eval_logits).cpu().numpy().ravel()\n",
        "\n",
        "                eval_metrics = {}\n",
        "                for metric in eval_metric:\n",
        "                    if metric == \"logloss\":\n",
        "                        eval_metrics[\"logloss\"] = log_loss(y_eval_true, y_eval_pred)\n",
        "                    elif metric == \"average_precision\":\n",
        "                        eval_metrics[\"average_precision\"] = -average_precision_score(\n",
        "                            y_eval_true, y_eval_pred\n",
        "                        )\n",
        "                    elif metric == \"auc\":\n",
        "                        eval_metrics[\"auc\"] = -roc_auc_score(y_eval_true, y_eval_pred)\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unknown metric: {metric}\")\n",
        "\n",
        "                if verbose:\n",
        "                    metrics_str = \", \".join(\n",
        "                        [f\"{k}: {v:.4f}\" for k, v in eval_metrics.items()]\n",
        "                    )\n",
        "                    print(f\"  - [eval] {metrics_str}\")\n",
        "\n",
        "                # early stopping (기준 metric은 리스트의 첫 번째)\n",
        "                main_metric_name = eval_metric[0]\n",
        "                current_metric = eval_metrics.get(\n",
        "                    main_metric_name, eval_metrics[\"logloss\"]\n",
        "                )\n",
        "\n",
        "                if verbose:\n",
        "                    print(\n",
        "                        f\"    -- (early_stopping) current_metric: {current_metric:.6f}, \"\n",
        "                        f\"best_metric: {best_metric:.6f}\"\n",
        "                    )\n",
        "\n",
        "                if current_metric < best_metric:\n",
        "                    best_metric = current_metric\n",
        "                    patience_counter = 0\n",
        "                    best_model_weights = self.model.state_dict()\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience is not None and patience_counter >= patience:\n",
        "                        if verbose:\n",
        "                            print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                        break\n",
        "\n",
        "        if best_model_weights is not None:\n",
        "            self.model.load_state_dict(best_model_weights)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        self.model = self.model.to(self.device)\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            logits = self.model(X_tensor)\n",
        "            probs1 = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "        if probs1.shape[1] == 1:\n",
        "            probs1 = probs1.reshape(-1, 1)\n",
        "\n",
        "        probs0 = 1.0 - probs1\n",
        "        probs = np.hstack((probs0, probs1))\n",
        "        return probs.astype(\"float\")\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        probs = self.predict_proba(X)\n",
        "        return probs.argmax(axis=1)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 한국형 피처 스키마(연령/성별/OS/점수형) 데모 데이터 생성\n",
        "# -----------------------------------------------------------\n",
        "def make_kor_feature_demo_data(\n",
        "    n_samples: int = 5000,\n",
        "    seed: int = 42,\n",
        "):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = n_samples\n",
        "\n",
        "    # --- 1) 각 피처 생성 ---\n",
        "    # 연령: 18~80 사이 normal 분포\n",
        "    age = np.clip(rng.normal(loc=40, scale=12, size=n), 18, 80).astype(\"float32\")\n",
        "\n",
        "    # 성별: 3개 카테고리 (0: 미상, 1: 남, 2: 여)\n",
        "    gender = rng.randint(0, 3, size=n).astype(\"int64\")\n",
        "\n",
        "    # 앱가입건수: 포아송\n",
        "    app_join_cnt = rng.poisson(lam=2.0, size=n).astype(\"float32\")\n",
        "\n",
        "    # 앱마지막접속경과일: 지수분포 (최근일수록 작게)\n",
        "    last_login_days = rng.exponential(scale=10.0, size=n).astype(\"float32\")\n",
        "\n",
        "    # OS: 4개 카테고리 (0: Android, 1: iOS, 2: Web, 3: 기타)\n",
        "    os_cat = rng.randint(0, 4, size=n).astype(\"int64\")\n",
        "\n",
        "    # 앱가입경과일: 평균 200일 정도로\n",
        "    app_join_days = rng.exponential(scale=200.0, size=n).astype(\"float32\")\n",
        "\n",
        "    # 점수형(0~1 근처의 score 형태로)\n",
        "    biz_owner_score = (rng.binomial(1, 0.3, size=n) * rng.uniform(0.5, 1.0, size=n)).astype(\"float32\")\n",
        "    home_owner_score = (rng.binomial(1, 0.4, size=n) * rng.uniform(0.5, 1.0, size=n)).astype(\"float32\")\n",
        "    car_owner_score = (rng.binomial(1, 0.5, size=n) * rng.uniform(0.5, 1.0, size=n)).astype(\"float32\")\n",
        "    married_score   = (rng.binomial(1, 0.5, size=n) * rng.uniform(0.5, 1.0, size=n)).astype(\"float32\")\n",
        "    child_score     = (rng.binomial(1, 0.4, size=n) * rng.uniform(0.5, 1.0, size=n)).astype(\"float32\")\n",
        "\n",
        "    # --- 2) 라벨 생성용 latent score ---\n",
        "    w_gender = rng.uniform(-0.3, 0.3, size=3)   # gender 0/1/2\n",
        "    w_os     = rng.uniform(-0.2, 0.2, size=4)   # os 0~3\n",
        "\n",
        "    score = np.zeros(n, dtype=\"float32\")\n",
        "\n",
        "    score += 0.05 * (age - 40) / 10.0\n",
        "    score += 0.25 * app_join_cnt\n",
        "    score -= 0.03 * last_login_days\n",
        "    score -= 0.002 * app_join_days\n",
        "    score += 0.8 * biz_owner_score\n",
        "    score += 0.6 * home_owner_score\n",
        "    score += 0.7 * car_owner_score\n",
        "    score += 0.9 * married_score\n",
        "    score += 1.0 * child_score\n",
        "\n",
        "    score += w_gender[gender]\n",
        "    score += w_os[os_cat]\n",
        "\n",
        "    noise = rng.normal(scale=0.5, size=n).astype(\"float32\")\n",
        "    bias = -0.2\n",
        "    logit = score + bias + noise\n",
        "    prob = 1.0 / (1.0 + np.exp(-logit))\n",
        "    y = (prob > 0.5).astype(\"int64\")\n",
        "\n",
        "    # --- 3) 최종 X 매트릭스 ---\n",
        "    X = np.column_stack(\n",
        "        [\n",
        "            age,                        # 0\n",
        "            gender.astype(\"float32\"),   # 1\n",
        "            app_join_cnt,               # 2\n",
        "            last_login_days,            # 3\n",
        "            os_cat.astype(\"float32\"),   # 4\n",
        "            app_join_days,              # 5\n",
        "            biz_owner_score,            # 6\n",
        "            home_owner_score,           # 7\n",
        "            car_owner_score,            # 8\n",
        "            married_score,              # 9\n",
        "            child_score,                # 10\n",
        "        ]\n",
        "    ).astype(\"float32\")\n",
        "\n",
        "    cat_feature_indices = [1, 4]   # 성별, OS\n",
        "    cat_dims = [3, 4]              # gender:3, os:4\n",
        "\n",
        "    return X, y, cat_feature_indices, cat_dims\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# TabTransformer 데모\n",
        "# -----------------------------------------------------------\n",
        "def demo_train_tabtransformer():\n",
        "    print(\"\\n===== TabTransformer Demo (KOR Feature Schema) =====\")\n",
        "\n",
        "    X, y, cat_features, cat_dims = make_kor_feature_demo_data(\n",
        "        n_samples=2000,\n",
        "        seed=123,\n",
        "    )\n",
        "\n",
        "    print(\"X shape:\", X.shape, \" | y shape:\", y.shape)\n",
        "    print(\"Categorical feature indices:\", cat_features)\n",
        "    print(\"Categorical dims (cardinality):\", cat_dims)\n",
        "\n",
        "    # train / val / test split\n",
        "    N = X.shape[0]\n",
        "    idx = np.arange(N)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    tr_end = int(N * 0.7)\n",
        "    va_end = int(N * 0.85)\n",
        "    tr_idx, va_idx, te_idx = idx[:tr_end], idx[tr_end:va_end], idx[va_end:]\n",
        "\n",
        "    X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
        "    X_va, y_va = X[va_idx], y[va_idx]\n",
        "    X_te, y_te = X[te_idx], y[te_idx]\n",
        "\n",
        "    sample_weight = np.ones_like(y_tr, dtype=\"float32\")\n",
        "\n",
        "    input_dim = X.shape[1]\n",
        "\n",
        "    model_params = {\n",
        "        \"input_dim\": input_dim,\n",
        "        \"cat_features\": cat_features,\n",
        "        \"cat_dims\": cat_dims,\n",
        "        \"d_token\": 32,\n",
        "        \"n_heads\": 4,\n",
        "        \"n_layers\": 2,\n",
        "        \"dim_feedforward\": None,   # None이면 4*d_token\n",
        "        \"attn_dropout\": 0.1,\n",
        "        \"embedding_dropout\": 0.05,\n",
        "        \"add_cls\": False,\n",
        "        \"pooling\": \"concat\",\n",
        "        \"cont_proj\": \"linear\",\n",
        "        \"mlp_hidden_dims\": (128, 64),\n",
        "        \"mlp_dropout\": 0.2,\n",
        "        \"use_missing_category\": False,\n",
        "        \"lr\": 1e-3,\n",
        "        \"loss_fn\": \"logloss\",\n",
        "    }\n",
        "\n",
        "    clf = DeepLearningBinaryClassifier(\n",
        "        model_type=\"tabtransformer\",\n",
        "        model_params=model_params,\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_tr,\n",
        "        y_tr,\n",
        "        sample_weight=sample_weight,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric=[\"logloss\"],\n",
        "        max_epochs=5,      # 데모용\n",
        "        patience=2,\n",
        "        batch_size=256,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    probs_te = clf.predict_proba(X_te)[:, 1]\n",
        "    preds_te = (probs_te >= 0.5).astype(\"int64\")\n",
        "\n",
        "    acc = (preds_te == y_te).mean()\n",
        "    auc = roc_auc_score(y_te, probs_te)\n",
        "    ll = log_loss(y_te, probs_te)\n",
        "\n",
        "    print(\"\\n===== TabTransformer Test Metrics =====\")\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"ROC-AUC  : {auc:.4f}\")\n",
        "    print(f\"Logloss  : {ll:.4f}\")\n",
        "    print(\"Sample probs (first 10):\", np.round(probs_te[:10], 4))\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# MLP 데모\n",
        "# -----------------------------------------------------------\n",
        "def demo_train_mlp():\n",
        "    print(\"\\n===== MLP Demo (KOR Feature Schema) =====\")\n",
        "\n",
        "    X, y, cat_features, cat_dims = make_kor_feature_demo_data(\n",
        "        n_samples=2000,\n",
        "        seed=321,\n",
        "    )\n",
        "\n",
        "    print(\"X shape:\", X.shape, \" | y shape:\", y.shape)\n",
        "    print(\"Categorical feature indices:\", cat_features)\n",
        "    print(\"Categorical dims (cardinality):\", cat_dims)\n",
        "\n",
        "    N = X.shape[0]\n",
        "    idx = np.arange(N)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    tr_end = int(N * 0.7)\n",
        "    va_end = int(N * 0.85)\n",
        "    tr_idx, va_idx, te_idx = idx[:tr_end], idx[tr_end:va_end], idx[va_end:]\n",
        "\n",
        "    X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
        "    X_va, y_va = X[va_idx], y[va_idx]\n",
        "    X_te, y_te = X[te_idx], y[te_idx]\n",
        "\n",
        "    sample_weight = np.ones_like(y_tr, dtype=\"float32\")\n",
        "\n",
        "    input_dim = X.shape[1]\n",
        "\n",
        "    model_params = {\n",
        "        \"input_dim\": input_dim,\n",
        "        \"hidden_dims\": [128, 64],\n",
        "        \"cat_features\": cat_features,\n",
        "        \"cat_dims\": cat_dims,\n",
        "        \"emb_dim\": 8,\n",
        "        \"lr\": 1e-3,\n",
        "        \"loss_fn\": \"logloss\",\n",
        "    }\n",
        "\n",
        "    clf = DeepLearningBinaryClassifier(\n",
        "        model_type=\"mlp\",\n",
        "        model_params=model_params,\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_tr,\n",
        "        y_tr,\n",
        "        sample_weight=sample_weight,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric=[\"logloss\"],\n",
        "        max_epochs=5,      # 데모용\n",
        "        patience=2,\n",
        "        batch_size=256,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    probs_te = clf.predict_proba(X_te)[:, 1]\n",
        "    preds_te = (probs_te >= 0.5).astype(\"int64\")\n",
        "\n",
        "    acc = (preds_te == y_te).mean()\n",
        "    auc = roc_auc_score(y_te, probs_te)\n",
        "    ll = log_loss(y_te, probs_te)\n",
        "\n",
        "    print(\"\\n===== MLP Test Metrics =====\")\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"ROC-AUC  : {auc:.4f}\")\n",
        "    print(f\"Logloss  : {ll:.4f}\")\n",
        "    print(\"Sample probs (first 10):\", np.round(probs_te[:10], 4))\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# FT-Transformer 데모\n",
        "# -----------------------------------------------------------\n",
        "def demo_train_fttransformer():\n",
        "    print(\"\\n===== FT-Transformer Demo (KOR Feature Schema) =====\")\n",
        "\n",
        "    X, y, cat_features, cat_dims = make_kor_feature_demo_data(\n",
        "        n_samples=2000,\n",
        "        seed=456,\n",
        "    )\n",
        "\n",
        "    print(\"X shape:\", X.shape, \" | y shape:\", y.shape)\n",
        "    print(\"Categorical feature indices:\", cat_features)\n",
        "    print(\"Categorical dims (cardinality):\", cat_dims)\n",
        "\n",
        "    # train / val / test split (70 / 15 / 15)\n",
        "    N = X.shape[0]\n",
        "    idx = np.arange(N)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    tr_end = int(N * 0.7)\n",
        "    va_end = int(N * 0.85)\n",
        "    tr_idx, va_idx, te_idx = idx[:tr_end], idx[tr_end:va_end], idx[va_end:]\n",
        "\n",
        "    X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
        "    X_va, y_va = X[va_idx], y[va_idx]\n",
        "    X_te, y_te = X[te_idx], y[te_idx]\n",
        "\n",
        "    sample_weight = np.ones_like(y_tr, dtype=\"float32\")\n",
        "\n",
        "    input_dim = X.shape[1]\n",
        "\n",
        "    # FT-Transformer용 하이퍼파라미터\n",
        "    model_params = {\n",
        "        \"input_dim\": input_dim,\n",
        "        \"cat_features\": cat_features,\n",
        "        \"cat_dims\": cat_dims,\n",
        "        \"d_token\": 32,\n",
        "        \"n_heads\": 4,\n",
        "        \"n_layers\": 3,\n",
        "        \"dim_feedforward\": None,  # None이면 4 * d_token\n",
        "        \"attn_dropout\": 0.1,\n",
        "        \"token_dropout\": 0.05,\n",
        "        \"mlp_dropout\": 0.2,\n",
        "        \"lr\": 1e-3,\n",
        "        \"loss_fn\": \"logloss\",\n",
        "    }\n",
        "\n",
        "    clf = DeepLearningBinaryClassifier(\n",
        "        model_type=\"fttransformer\",\n",
        "        model_params=model_params,\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_tr,\n",
        "        y_tr,\n",
        "        sample_weight=sample_weight,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric=[\"logloss\"],\n",
        "        max_epochs=5,      # 데모용\n",
        "        patience=2,\n",
        "        batch_size=256,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    # 평가\n",
        "    probs_te = clf.predict_proba(X_te)[:, 1]\n",
        "    preds_te = (probs_te >= 0.5).astype(\"int64\")\n",
        "\n",
        "    acc = (preds_te == y_te).mean()\n",
        "    auc = roc_auc_score(y_te, probs_te)\n",
        "    ll = log_loss(y_te, probs_te)\n",
        "\n",
        "    print(\"\\n===== FT-Transformer Test Metrics =====\")\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"ROC-AUC  : {auc:.4f}\")\n",
        "    print(f\"Logloss  : {ll:.4f}\")\n",
        "    print(\"Sample probs (first 10):\", np.round(probs_te[:10], 4))\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 메인\n",
        "# -----------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # demo_train_tabtransformer()\n",
        "    # demo_train_mlp()\n",
        "    demo_train_fttransformer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxWZFYhzJ5YO",
        "outputId": "2b662455-5cf1-43e7-a8c0-a54e91f2b9a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== FT-Transformer Demo (KOR Feature Schema) =====\n",
            "X shape: (2000, 11)  | y shape: (2000,)\n",
            "Categorical feature indices: [1, 4]\n",
            "Categorical dims (cardinality): [3, 4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - [train] loss: 140.687468\n",
            "  - [eval] logloss: 0.5333\n",
            "    -- (early_stopping) current_metric: 0.533306, best_metric: inf\n",
            "Epoch 2/5 - [train] loss: 128.496629\n",
            "  - [eval] logloss: 0.5203\n",
            "    -- (early_stopping) current_metric: 0.520328, best_metric: 0.533306\n",
            "Epoch 3/5 - [train] loss: 126.481900\n",
            "  - [eval] logloss: 0.4843\n",
            "    -- (early_stopping) current_metric: 0.484252, best_metric: 0.520328\n",
            "Epoch 4/5 - [train] loss: 117.148365\n",
            "  - [eval] logloss: 0.4436\n",
            "    -- (early_stopping) current_metric: 0.443629, best_metric: 0.484252\n",
            "Epoch 5/5 - [train] loss: 107.854777\n",
            "  - [eval] logloss: 0.4695\n",
            "    -- (early_stopping) current_metric: 0.469536, best_metric: 0.443629\n",
            "\n",
            "===== FT-Transformer Test Metrics =====\n",
            "Accuracy : 0.7200\n",
            "ROC-AUC  : 0.7987\n",
            "Logloss  : 0.4887\n",
            "Sample probs (first 10): [0.8738 0.6252 0.3958 0.8739 0.3938 0.4466 0.4032 0.8745 0.3937 0.873 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QqCva09GLJZH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}