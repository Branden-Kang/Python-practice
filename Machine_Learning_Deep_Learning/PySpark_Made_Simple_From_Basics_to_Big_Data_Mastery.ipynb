{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrsAwPLHiHW6AiMUs+Y1Fn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@nomannayeem/pyspark-made-simple-from-basics-to-big-data-mastery-cb1d702968be)"
      ],
      "metadata": {
        "id": "ZXnwbsREcdfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Your First PySpark Application"
      ],
      "metadata": {
        "id": "uZ51rr2LcnAJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fzT4g-s9cbk9"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SimpleApp\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "\n",
        "# Show the DataFrame content\n",
        "df.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up PySpark: Your First Step to Big Data Processing"
      ],
      "metadata": {
        "id": "pVLyvkTNcwT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Java (JDK)"
      ],
      "metadata": {
        "id": "8VfajE28dDFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "java version \"17.0.1\" // or the latest version\n",
        "```"
      ],
      "metadata": {
        "id": "z9_cHM96dE2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Install Apache Spark\n",
        "Go to the Apache Spark Downloads page and download it"
      ],
      "metadata": {
        "id": "79dQv9kwdH5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Install Python\n",
        "```\n",
        "python --version\n",
        "```"
      ],
      "metadata": {
        "id": "-SpYaiaTdNoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Set Up PySpark Environment"
      ],
      "metadata": {
        "id": "zvYhnI8EdSa-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Create a Virtual Environment:\n",
        "```\n",
        "python -m venv pyspark-env\n",
        "```"
      ],
      "metadata": {
        "id": "IRLUyJVddT10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Activate the Environment:"
      ],
      "metadata": {
        "id": "dQlEZ2zSdqzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Mac\n",
        "source pyspark-env/bin/activate\n",
        "\n",
        "# Windows\n",
        "pyspark-env\\Scripts\\activate\n",
        "```"
      ],
      "metadata": {
        "id": "vIxTCXpmdzjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Install PySpark:\n",
        "```\n",
        "pip install pyspark\n",
        "```"
      ],
      "metadata": {
        "id": "QX1aV_8Cd3tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Running PySpark with JupyterLab\n"
      ],
      "metadata": {
        "id": "kKSdzBnjd7mN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install JupyterLab:\n",
        "```\n",
        "Install JupyterLab:\n",
        "```"
      ],
      "metadata": {
        "id": "Z83mb7QYd89e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install JupyterLab:\n",
        "```\n",
        "jupyter-lab\n",
        "```"
      ],
      "metadata": {
        "id": "G4LSphpId_2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running Your First PySpark Code"
      ],
      "metadata": {
        "id": "8XOBXjz5eC0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TestSparkSetup\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Print Spark session info\n",
        "print(spark.version)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "E0ZKdB8Ucp8A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with Resilient Distributed Datasets (RDDs)\n"
      ],
      "metadata": {
        "id": "Qcc1EHZxeIfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating an RDD"
      ],
      "metadata": {
        "id": "4CjoOTthek6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDD Example\")\n",
        "\n",
        "# Create an RDD from a Python list\n",
        "numbers_rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "# Perform an action to collect the results\n",
        "collected_numbers = numbers_rdd.collect()\n",
        "\n",
        "# Print the result\n",
        "print(collected_numbers)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "BHiMjwa1eFmL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RDD Transformations and Actions"
      ],
      "metadata": {
        "id": "GCW2z25_euHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD from a list\n",
        "numbers_rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "# Multiply each element by 2\n",
        "doubled_rdd = numbers_rdd.map(lambda x: x * 2)\n",
        "\n",
        "# Collect and print the result\n",
        "print(doubled_rdd.collect())"
      ],
      "metadata": {
        "id": "3dNaGwIieoRV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of elements in the RDD\n",
        "print(numbers_rdd.count())"
      ],
      "metadata": {
        "id": "82KT-Ayrev2E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with DataFrames and DataFrame Operations"
      ],
      "metadata": {
        "id": "rhLfkd3be0NY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a DataFrame"
      ],
      "metadata": {
        "id": "kj-Bl18Ue3Ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DataFrame Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data: list of tuples\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
        "\n",
        "# Define column names\n",
        "columns = [\"Name\", \"Age\"]\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame content\n",
        "df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "R_Q7Jw3PeyYz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataFrame Operations"
      ],
      "metadata": {
        "id": "gNN1VOaBe7AH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the \"Name\" column\n",
        "df.select(\"Name\").show()\n",
        "\n",
        "# Select multiple columns\n",
        "df.select(\"Name\", \"Age\").show()\n",
        "\n",
        "# Filter rows where age is greater than 30\n",
        "df.filter(df.Age > 30).show()\n",
        "\n",
        "# Group by age and count occurrences\n",
        "df.groupBy(\"Age\").count().show()\n",
        "\n",
        "# Sort the DataFrame by age\n",
        "df.orderBy(\"Age\").show()\n",
        "\n",
        "# Sort by age in descending order\n",
        "df.orderBy(df.Age.desc()).show()"
      ],
      "metadata": {
        "id": "AVFNVycze41T"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark SQL and Querying DataFrames\n"
      ],
      "metadata": {
        "id": "v8E8u-_WfEuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark SQL Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "\n",
        "# Register the DataFrame as a temporary view\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# Run a SQL query\n",
        "result = spark.sql(\"SELECT Name, Age FROM people WHERE Age > 30\")\n",
        "\n",
        "# Show the result of the query\n",
        "result.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "xVadsXU4fDeH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common SQL Queries in Spark SQL"
      ],
      "metadata": {
        "id": "YFj3sZRafM4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = spark.sql(\"SELECT Name FROM people\")\n",
        "result.show()\n",
        "\n",
        "result = spark.sql(\"SELECT * FROM people WHERE Age < 30\")\n",
        "result.show()\n",
        "\n",
        "result = spark.sql(\"SELECT Age, COUNT(*) as count FROM people GROUP BY Age\")\n",
        "result.show()\n",
        "\n",
        "# Sample data\n",
        "data_jobs = [(\"Alice\", \"Engineer\"), (\"Bob\", \"Doctor\")]\n",
        "df_jobs = spark.createDataFrame(data_jobs, [\"Name\", \"Job\"])\n",
        "\n",
        "# Register jobs DataFrame as a temporary view\n",
        "df_jobs.createOrReplaceTempView(\"jobs\")\n",
        "\n",
        "# SQL join query\n",
        "result = spark.sql(\"\"\"\n",
        "    SELECT p.Name, p.Age, j.Job\n",
        "    FROM people p\n",
        "    JOIN jobs j ON p.Name = j.Name\n",
        "\"\"\")\n",
        "\n",
        "result.show()"
      ],
      "metadata": {
        "id": "7ruiJtszfJ7g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced DataFrame Operations"
      ],
      "metadata": {
        "id": "U2Lz-eLAfUbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Missing Data"
      ],
      "metadata": {
        "id": "OuA2QJIsfXJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dropping Rows with Missing Data"
      ],
      "metadata": {
        "id": "sPfBPNGHfcly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with any missing values\n",
        "df_cleaned = df.dropna()\n",
        "df_cleaned.show()"
      ],
      "metadata": {
        "id": "ULxHW6VafSy0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where the \"Age\" column has missing values\n",
        "df_cleaned = df.dropna(subset=[\"Age\"])\n",
        "df_cleaned.show()"
      ],
      "metadata": {
        "id": "WFCSGYsQfZVZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filling Missing Data"
      ],
      "metadata": {
        "id": "pApIYMxoffi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values in all columns with a default value\n",
        "df_filled = df.fillna(0)\n",
        "df_filled.show()\n",
        "\n",
        "# Fill missing values in specific columns\n",
        "df_filled = df.fillna({\"Age\": 0, \"Name\": \"Unknown\"})\n",
        "df_filled.show()"
      ],
      "metadata": {
        "id": "XmlBd833feCQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Window Functions in PySpark"
      ],
      "metadata": {
        "id": "V9S9mm_VflBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "\n",
        "# Define a window specification\n",
        "window_spec = Window.orderBy(\"Age\")\n",
        "\n",
        "# Add a rank column\n",
        "df_with_rank = df.withColumn(\"rank\", rank().over(window_spec))\n",
        "df_with_rank.show()"
      ],
      "metadata": {
        "id": "3VZyBmgDfd0I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizing DataFrame Queries\n"
      ],
      "metadata": {
        "id": "cdxfYOGNfqxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Caching DataFrames"
      ],
      "metadata": {
        "id": "GIJhOfKHfx_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache the DataFrame\n",
        "df.cache()\n",
        "\n",
        "# Perform operations on the cached DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "id": "HWugzO3ofoQ6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Partitioning"
      ],
      "metadata": {
        "id": "ApL9POjgf0gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Repartition the DataFrame\n",
        "df_repartitioned = df.repartition(4)\n",
        "df_repartitioned.show()"
      ],
      "metadata": {
        "id": "Io_LkspBfzgp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Topics in PySpark"
      ],
      "metadata": {
        "id": "X_z6sQYAf4x3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming Data in PySpark"
      ],
      "metadata": {
        "id": "b7fv7FY6f--o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StructuredNetworkWordCount\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read streaming data from a socket\n",
        "lines = spark.readStream \\\n",
        "    .format(\"socket\") \\\n",
        "    .option(\"host\", \"localhost\") \\\n",
        "    .option(\"port\", 9999) \\\n",
        "    .load()\n",
        "\n",
        "# Split the lines into words\n",
        "words = lines.select(\n",
        "   explode(split(lines.value, \" \")).alias(\"word\")\n",
        ")\n",
        "\n",
        "# Generate running word count\n",
        "word_counts = words.groupBy(\"word\").count()\n",
        "\n",
        "# Start running the query that prints the running counts to the console\n",
        "query = word_counts.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()"
      ],
      "metadata": {
        "id": "Fd2VptHDf2nU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a Machine Learning Pipeline with PySpark"
      ],
      "metadata": {
        "id": "OAukHmpJgFIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Sample data\n",
        "data = [(0, 1.0, 0.5), (1, 2.0, 1.5), (0, 0.5, 0.3), (1, 2.5, 1.7)]\n",
        "df = spark.createDataFrame(data, [\"label\", \"feature1\", \"feature2\"])\n",
        "\n",
        "# Assemble features into a vector\n",
        "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\n",
        "\n",
        "# Define a Logistic Regression model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Create a pipeline with the assembler and the logistic regression model\n",
        "pipeline = Pipeline(stages=[assembler, lr])\n",
        "\n",
        "# Train the model\n",
        "model = pipeline.fit(df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(df)\n",
        "\n",
        "# Show the predictions\n",
        "predictions.select(\"label\", \"features\", \"prediction\").show()"
      ],
      "metadata": {
        "id": "qCjCM7lfgB8v"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real-World Project: End-to-End Data Processing with PySpark"
      ],
      "metadata": {
        "id": "9X0TfD9hgRKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Load the Data"
      ],
      "metadata": {
        "id": "DDq0KLGYgVv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Customer Churn Prediction\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the customer churn dataset\n",
        "data = spark.read.csv(\"customer_churn.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show the data schema and first few rows\n",
        "data.printSchema()\n",
        "data.show(5)"
      ],
      "metadata": {
        "id": "IfdlbFMbgMzF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Data Cleaning and Preprocessing"
      ],
      "metadata": {
        "id": "1NqSHkm4gZHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values\n",
        "data_cleaned = data.dropna()\n",
        "\n",
        "# Convert categorical columns to numerical ones using StringIndexer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"PaymentMethod\", outputCol=\"PaymentMethodIndex\")\n",
        "data_indexed = indexer.fit(data_cleaned).transform(data_cleaned)\n",
        "\n",
        "# Show the processed data\n",
        "data_indexed.show(5)"
      ],
      "metadata": {
        "id": "YCZCw4b1gXlE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Feature Engineering"
      ],
      "metadata": {
        "id": "Vdhf8sO-gfKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Select the features and label column\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"Tenure\", \"MonthlyCharges\", \"TotalCharges\", \"PaymentMethodIndex\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# Apply the assembler to the DataFrame\n",
        "data_prepared = assembler.transform(data_indexed)\n",
        "\n",
        "# Show the prepared data\n",
        "data_prepared.select(\"features\", \"Churn\").show(5)"
      ],
      "metadata": {
        "id": "ABBirfU5gcWO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Build and Train the Model"
      ],
      "metadata": {
        "id": "gqoSNQs9gkF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Churn\")\n",
        "\n",
        "# Train the model\n",
        "model = lr.fit(data_prepared)\n",
        "\n",
        "# Make predictions on the dataset\n",
        "predictions = model.transform(data_prepared)\n",
        "\n",
        "# Show the predictions\n",
        "predictions.select(\"Churn\", \"prediction\", \"probability\").show(5)"
      ],
      "metadata": {
        "id": "Hi3oMMrZgiD6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Evaluate the Model"
      ],
      "metadata": {
        "id": "G6eXcl-ggoUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Initialize the evaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Churn\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Evaluate the model\n",
        "roc_auc = evaluator.evaluate(predictions)\n",
        "print(f\"Area under ROC curve: {roc_auc}\")"
      ],
      "metadata": {
        "id": "EhBriguRgmam"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}
