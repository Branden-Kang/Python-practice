{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkOb7ca3fsdvBMK9ZR6ZIa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@francescofranco_39234/k-fold-cross-validation-with-pytorch-4d57d6f623fc)"
      ],
      "metadata": {
        "id": "B4dWEumxLixk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s_OKkCS1LE3d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConvNet(nn.Module):\n",
        "  '''\n",
        "    Simple Convolutional Neural Network\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Conv2d(1, 10, kernel_size=3),\n",
        "      nn.ReLU(),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(26 * 26 * 10, 50),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(50, 20),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(20, 10)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "z3ZmV0ppL0DI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def reset_weights(m):\n",
        "  '''\n",
        "    Try resetting model weights to avoid\n",
        "    weight leakage.\n",
        "  '''\n",
        "  for layer in m.children():\n",
        "   if hasattr(layer, 'reset_parameters'):\n",
        "    print(f'Reset trainable parameters of layer = {layer}')\n",
        "    layer.reset_parameters()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # Configuration options\n",
        "  k_folds = 5\n",
        "  num_epochs = 1\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "  # For fold results\n",
        "  results = {}\n",
        "\n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "\n",
        "  # Prepare MNIST dataset by concatenating Train/Test part; we split later.\n",
        "  dataset_train_part = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor(), train=True)\n",
        "  dataset_test_part = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor(), train=False)\n",
        "  dataset = ConcatDataset([dataset_train_part, dataset_test_part])\n",
        "\n",
        "  # Define the K-fold Cross Validator\n",
        "  kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "  # Start print\n",
        "  print('--------------------------------')\n",
        "\n",
        "  # K-fold Cross Validation model evaluation\n",
        "  for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "\n",
        "    # Print\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "\n",
        "    # Define data loaders for training and testing data in this fold\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "                      dataset,\n",
        "                      batch_size=10, sampler=train_subsampler)\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "                      dataset,\n",
        "                      batch_size=10, sampler=test_subsampler)\n",
        "\n",
        "    # Init the neural network\n",
        "    network = SimpleConvNet()\n",
        "    network.apply(reset_weights)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
        "\n",
        "    # Run the training loop for defined number of epochs\n",
        "    for epoch in range(0, num_epochs):\n",
        "\n",
        "      # Print epoch\n",
        "      print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "      # Set current loss value\n",
        "      current_loss = 0.0\n",
        "\n",
        "      # Iterate over the DataLoader for training data\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "        # Get inputs\n",
        "        inputs, targets = data\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform forward pass\n",
        "        outputs = network(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        # Perform backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Perform optimization\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        current_loss += loss.item()\n",
        "        if i % 500 == 499:\n",
        "            print('Loss after mini-batch %5d: %.3f' %\n",
        "                  (i + 1, current_loss / 500))\n",
        "            current_loss = 0.0\n",
        "\n",
        "    # Process is complete.\n",
        "    print('Training process has finished. Saving trained model.')\n",
        "\n",
        "    # Print about testing\n",
        "    print('Starting testing')\n",
        "\n",
        "    # Saving the model\n",
        "    save_path = f'./model-fold-{fold}.pth'\n",
        "    torch.save(network.state_dict(), save_path)\n",
        "\n",
        "    # Evaluationfor this fold\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # Iterate over the test data and generate predictions\n",
        "      for i, data in enumerate(testloader, 0):\n",
        "\n",
        "        # Get inputs\n",
        "        inputs, targets = data\n",
        "\n",
        "        # Generate outputs\n",
        "        outputs = network(inputs)\n",
        "\n",
        "        # Set total and correct\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "      # Print accuracy\n",
        "      print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
        "      print('--------------------------------')\n",
        "      results[fold] = 100.0 * (correct / total)\n",
        "\n",
        "  # Print fold results\n",
        "  print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
        "  print('--------------------------------')\n",
        "  sum = 0.0\n",
        "  for key, value in results.items():\n",
        "    print(f'Fold {key}: {value} %')\n",
        "    sum += value\n",
        "  print(f'Average: {sum/len(results.items())} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX438zkEMESX",
        "outputId": "247ac986-f6eb-46d5-c35f-7b305e4844b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /content/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 102546188.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/train-images-idx3-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /content/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 29632019.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/train-labels-idx1-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /content/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 31930797.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/t10k-images-idx3-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /content/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 13597807.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/t10k-labels-idx1-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "--------------------------------\n",
            "FOLD 0\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "Reset trainable parameters of layer = Linear(in_features=6760, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=20, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 1.891\n",
            "Loss after mini-batch  1000: 0.856\n",
            "Loss after mini-batch  1500: 0.514\n",
            "Loss after mini-batch  2000: 0.450\n",
            "Loss after mini-batch  2500: 0.397\n",
            "Loss after mini-batch  3000: 0.377\n",
            "Loss after mini-batch  3500: 0.359\n",
            "Loss after mini-batch  4000: 0.345\n",
            "Loss after mini-batch  4500: 0.338\n",
            "Loss after mini-batch  5000: 0.319\n",
            "Loss after mini-batch  5500: 0.309\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 0: 91 %\n",
            "--------------------------------\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "Reset trainable parameters of layer = Linear(in_features=6760, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=20, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 1.831\n",
            "Loss after mini-batch  1000: 0.846\n",
            "Loss after mini-batch  1500: 0.533\n",
            "Loss after mini-batch  2000: 0.415\n",
            "Loss after mini-batch  2500: 0.373\n",
            "Loss after mini-batch  3000: 0.352\n",
            "Loss after mini-batch  3500: 0.336\n",
            "Loss after mini-batch  4000: 0.338\n",
            "Loss after mini-batch  4500: 0.299\n",
            "Loss after mini-batch  5000: 0.293\n",
            "Loss after mini-batch  5500: 0.287\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 1: 91 %\n",
            "--------------------------------\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "Reset trainable parameters of layer = Linear(in_features=6760, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=20, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 1.866\n",
            "Loss after mini-batch  1000: 0.826\n",
            "Loss after mini-batch  1500: 0.532\n",
            "Loss after mini-batch  2000: 0.440\n",
            "Loss after mini-batch  2500: 0.399\n",
            "Loss after mini-batch  3000: 0.369\n",
            "Loss after mini-batch  3500: 0.362\n",
            "Loss after mini-batch  4000: 0.319\n",
            "Loss after mini-batch  4500: 0.321\n",
            "Loss after mini-batch  5000: 0.332\n",
            "Loss after mini-batch  5500: 0.298\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 2: 91 %\n",
            "--------------------------------\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "Reset trainable parameters of layer = Linear(in_features=6760, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=20, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 1.861\n",
            "Loss after mini-batch  1000: 0.864\n",
            "Loss after mini-batch  1500: 0.550\n",
            "Loss after mini-batch  2000: 0.450\n",
            "Loss after mini-batch  2500: 0.414\n",
            "Loss after mini-batch  3000: 0.363\n",
            "Loss after mini-batch  3500: 0.329\n",
            "Loss after mini-batch  4000: 0.337\n",
            "Loss after mini-batch  4500: 0.302\n",
            "Loss after mini-batch  5000: 0.291\n",
            "Loss after mini-batch  5500: 0.303\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 3: 91 %\n",
            "--------------------------------\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "Reset trainable parameters of layer = Linear(in_features=6760, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=20, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 1.898\n",
            "Loss after mini-batch  1000: 0.943\n",
            "Loss after mini-batch  1500: 0.608\n",
            "Loss after mini-batch  2000: 0.503\n",
            "Loss after mini-batch  2500: 0.419\n",
            "Loss after mini-batch  3000: 0.372\n",
            "Loss after mini-batch  3500: 0.380\n",
            "Loss after mini-batch  4000: 0.360\n",
            "Loss after mini-batch  4500: 0.340\n",
            "Loss after mini-batch  5000: 0.341\n",
            "Loss after mini-batch  5500: 0.329\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 4: 90 %\n",
            "--------------------------------\n",
            "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
            "--------------------------------\n",
            "Fold 0: 91.60714285714285 %\n",
            "Fold 1: 91.05 %\n",
            "Fold 2: 91.10000000000001 %\n",
            "Fold 3: 91.16428571428571 %\n",
            "Fold 4: 90.66428571428573 %\n",
            "Average: 91.11714285714285 %\n"
          ]
        }
      ]
    }
  ]
}
