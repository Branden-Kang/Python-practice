{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Neural Networks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMUIJOUFLPjwqRrdYcuafys"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDi1qJlEH_V0"
      },
      "source": [
        "[Reference](https://towardsdatascience.com/deep-neural-networks-from-scratch-in-python-451f07999373)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7TZNcNhH-BC"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nn_architecture = [\n",
        "    {\"layer_size\": 4, \"activation\": \"none\"}, # input layer\n",
        "    {\"layer_size\": 5, \"activation\": \"relu\"},\n",
        "    {\"layer_size\": 4, \"activation\": \"relu\"},\n",
        "    {\"layer_size\": 3, \"activation\": \"relu\"},\n",
        "    {\"layer_size\": 1, \"activation\": \"sigmoid\"}\n",
        "]\n",
        "\n",
        "def initialize_parameters(nn_architecture, seed = 3):\n",
        "    np.random.seed(seed)\n",
        "    # python dictionary containing our parameters \"W1\", \"b1\", ..., \"WL\", \"bL\"\n",
        "    parameters = {}\n",
        "    number_of_layers = len(nn_architecture)\n",
        "\n",
        "    for l in range(1, number_of_layers):\n",
        "        parameters['W' + str(l)] = np.random.randn(\n",
        "            nn_architecture[l][\"layer_size\"],\n",
        "            nn_architecture[l-1][\"layer_size\"]\n",
        "            ) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((nn_architecture[l][\"layer_size\"], 1))\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yns676H2IE6K"
      },
      "source": [
        "def sigmoid(Z):\n",
        "    S = 1 / (1 + np.exp(-Z))\n",
        "    return S\n",
        "\n",
        "def relu(Z):\n",
        "    R = np.maximum(0, Z)\n",
        "    return R\n",
        "\n",
        "def sigmoid_backward(dA, Z):\n",
        "    S = sigmoid(Z)\n",
        "    dS = S * (1 - S)\n",
        "    return dA * dS\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    dZ = np.array(dA, copy = True)\n",
        "    dZ[Z <= 0] = 0\n",
        "    return dZ"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZW2kqfXIHUv"
      },
      "source": [
        "# Forward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu9NfJieIGaF"
      },
      "source": [
        "def L_model_forward(X, parameters, nn_architecture):\n",
        "    forward_cache = {}\n",
        "    A = X\n",
        "    number_of_layers = len(nn_architecture)\n",
        "    \n",
        "    for l in range(1, number_of_layers):\n",
        "        A_prev = A \n",
        "        W = parameters['W' + str(l)]\n",
        "        b = parameters['b' + str(l)]\n",
        "        activation = nn_architecture[l][\"activation\"]\n",
        "        Z, A = linear_activation_forward(A_prev, W, b, activation)\n",
        "        forward_cache['Z' + str(l)] = Z\n",
        "        forward_cache['A' + str(l-1)] = A\n",
        "\n",
        "    AL = A\n",
        "            \n",
        "    return AL, forward_cache\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    if activation == \"sigmoid\":\n",
        "        Z = linear_forward(A_prev, W, b)\n",
        "        A = sigmoid(Z)\n",
        "    elif activation == \"relu\":\n",
        "        Z = linear_forward(A_prev, W, b)\n",
        "        A = relu(Z)\n",
        "\n",
        "    return Z, A\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "    Z = np.dot(W, A) + b\n",
        "    return Z"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkjKIVY-IKkR"
      },
      "source": [
        "# Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06ICNZf5IJjK"
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from AL and y\n",
        "    logprobs = np.multiply(np.log(AL),Y) + np.multiply(1 - Y, np.log(1 - AL))\n",
        "    # cross-entropy cost\n",
        "    cost = - np.sum(logprobs) / m\n",
        "\n",
        "    cost = np.squeeze(cost)\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIVoFwscIPa1"
      },
      "source": [
        "# Backward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enWPt37QIOcq"
      },
      "source": [
        "def L_model_backward(AL, Y, parameters, forward_cache, nn_architecture):\n",
        "    grads = {}\n",
        "    number_of_layers = len(nn_architecture)\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    dA_prev = dAL\n",
        "\n",
        "    for l in reversed(range(1, number_of_layers)):\n",
        "        dA_curr = dA_prev\n",
        "\n",
        "        activation = nn_architecture[l][\"activation\"]\n",
        "        W_curr = parameters['W' + str(l)]\n",
        "        Z_curr = forward_cache['Z' + str(l)]\n",
        "        A_prev = forward_cache['A' + str(l-1)]\n",
        "\n",
        "        dA_prev, dW_curr, db_curr = linear_activation_backward(dA_curr, Z_curr, A_prev, W_curr, activation)\n",
        "\n",
        "        grads[\"dW\" + str(l)] = dW_curr\n",
        "        grads[\"db\" + str(l)] = db_curr\n",
        "\n",
        "    return grads\n",
        "\n",
        "def linear_activation_backward(dA, Z, A_prev, W, activation):\n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, Z)\n",
        "        dA_prev, dW, db = linear_backward(dZ, A_prev, W)\n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, Z)\n",
        "        dA_prev, dW, db = linear_backward(dZ, A_prev, W)\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def linear_backward(dZ, A_prev, W):\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = np.dot(dZ, A_prev.T) / m\n",
        "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oILEMMQjIU-K"
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
        "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8FzdXFbIX1I"
      },
      "source": [
        "def L_layer_model(X, Y, nn_architecture, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "    np.random.seed(1)\n",
        "    # keep track of cost\n",
        "    costs = []\n",
        "    \n",
        "    # Parameters initialization.\n",
        "    parameters = initialize_parameters(nn_architecture)\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        AL, forward_cache = L_model_forward(X, parameters, nn_architecture)\n",
        "        \n",
        "        # Compute cost.\n",
        "        cost = compute_cost(AL, Y)\n",
        "    \n",
        "        # Backward propagation.\n",
        "        grads = L_model_backward(AL, Y, parameters, forward_cache, nn_architecture)\n",
        " \n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "        costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}