{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/qhrPQWtRNDufJ3XQ2C0G"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://chaimrand.medium.com/optimizing-pytorch-model-inference-on-cpu-ccd3aa5884ad)"
      ],
      "metadata": {
        "id": "MLzt640LWZTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Experiment"
      ],
      "metadata": {
        "id": "ga7UAaKwWsos"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8ZGfevoWUxL",
        "outputId": "f7ed6f45-02f1-4c19-da4d-64f809258869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average samples per second: 5.10\n"
          ]
        }
      ],
      "source": [
        "import torch, torchvision\n",
        "import time\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    model = torchvision.models.resnet50()\n",
        "    model = model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_input(batch_size):\n",
        "    batch = torch.randn(batch_size, 3, 224, 224)\n",
        "    return batch\n",
        "\n",
        "\n",
        "def get_inference_fn(model):\n",
        "    def infer_fn(batch):\n",
        "        with torch.inference_mode():\n",
        "            output = model(batch)\n",
        "        return output\n",
        "    return infer_fn\n",
        "\n",
        "\n",
        "def benchmark(infer_fn, batch):\n",
        "    # warm-up\n",
        "    for _ in range(10):\n",
        "        _ = infer_fn(batch)\n",
        "\n",
        "    iters = 100\n",
        "\n",
        "    start = time.time()\n",
        "    for _ in range(iters):\n",
        "        _ = infer_fn(batch)\n",
        "    end = time.time()\n",
        "\n",
        "    return (end - start) / iters\n",
        "\n",
        "\n",
        "batch_size = 1\n",
        "model = get_model()\n",
        "batch = get_input(batch_size)\n",
        "infer_fn = get_inference_fn(model)\n",
        "avg_time = benchmark(infer_fn, batch)\n",
        "print(f\"\\nAverage samples per second: {(batch_size/avg_time):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Inference Optimization"
      ],
      "metadata": {
        "id": "JVfafuhMWusJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(channels_last=False):\n",
        "    model = torchvision.models.resnet50()\n",
        "    if channels_last:\n",
        "        model= model.to(memory_format=torch.channels_last)\n",
        "    model = model.eval()\n",
        "    return model\n",
        "\n",
        "def get_input(batch_size, channels_last=False):\n",
        "    batch = torch.randn(batch_size, 3, 224, 224)\n",
        "    if channels_last:\n",
        "        batch = batch.to(memory_format=torch.channels_last)\n",
        "    return batch\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "model = get_model(channels_last=True)\n",
        "batch = get_input(batch_size, channels_last=True)\n",
        "infer_fn = get_inference_fn(model)\n",
        "avg_time = benchmark(infer_fn, batch)\n",
        "print(f\"\\nAverage samples per second: {(batch_size/avg_time):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Jw4eFOrWp-D",
        "outputId": "f2b85601-b78b-408b-d904-d30cc523491c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average samples per second: 6.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inference_fn(model, enable_amp=False):\n",
        "    def infer_fn(batch):\n",
        "        with torch.inference_mode(), torch.amp.autocast(\n",
        "                'cpu',\n",
        "                dtype=torch.bfloat16,\n",
        "                enabled=enable_amp\n",
        "        ):\n",
        "            output = model(batch)\n",
        "        return output\n",
        "    return infer_fn\n",
        "\n",
        "batch_size = 8\n",
        "model = get_model(channels_last=True)\n",
        "batch = get_input(batch_size, channels_last=True)\n",
        "infer_fn = get_inference_fn(model, enable_amp=True)\n",
        "avg_time = benchmark(infer_fn, batch)\n",
        "print(f\"\\nAverage samples per second: {(batch_size/avg_time):.2f}\")"
      ],
      "metadata": {
        "id": "eocVgDOiWxIY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
