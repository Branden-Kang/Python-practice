{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOe24ECTsArFGLKDbSDaelL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@rohanmistry231/youre-not-bad-at-machine-learning-your-features-are-815b3f1db6b0)"
      ],
      "metadata": {
        "id": "_QFlxZaC6FZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation Analysis (The Simple & Fast)"
      ],
      "metadata": {
        "id": "4Wrts-E66KN4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q69bLL1V6DwU"
      },
      "outputs": [],
      "source": [
        "# Load your data\n",
        "df = pd.read_csv('your_data.csv')\n",
        "\n",
        "# Calculate correlation with target\n",
        "correlation_with_target = df.corr()['target_column'].sort_values(ascending=False)\n",
        "print(correlation_with_target)\n",
        "\n",
        "# Visualize\n",
        "import matplotlib.pyplot as plt\n",
        "correlation_with_target[1:].plot(kind='barh')\n",
        "plt.title('Feature Correlation with Target')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Permutation Feature Importance (The Trustworthy One)"
      ],
      "metadata": {
        "id": "5H1hWtSJ6Nlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train your model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Calculate permutation importance\n",
        "result = permutation_importance(\n",
        "    model, X_test, y_test,\n",
        "    n_repeats=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Get feature importances\n",
        "importances = pd.DataFrame({\n",
        "    'feature': X_test.columns,\n",
        "    'importance': result.importances_mean,\n",
        "    'std': result.importances_std\n",
        "}).sort_values('importance', ascending=False)\n",
        "print(importances)"
      ],
      "metadata": {
        "id": "r0mHNjj06L01"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tree-Based Feature Importance (The Fast Production Method)"
      ],
      "metadata": {
        "id": "G0_ejAIz6exh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Train Random Forest\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances (built-in)\n",
        "importances = model.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': importances\n",
        "}).sort_values('importance', ascending=False)\n",
        "print(feature_importance_df)\n",
        "\n",
        "# Visualize\n",
        "import matplotlib.pyplot as plt\n",
        "plt.barh(feature_importance_df['feature'], feature_importance_df['importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importance - Random Forest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iYkDRMNP6ZO3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHAP Values (The Game Theory Approach)"
      ],
      "metadata": {
        "id": "q-L-GRlw6oPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Create SHAP explainer\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Global importance\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
        "plt.title(\"SHAP Global Feature Importance\")\n",
        "plt.show()\n",
        "\n",
        "# Local explanation for one prediction\n",
        "shap.force_plot(explainer.expected_value[1],\n",
        "                shap_values[1][0], X_test.iloc[0])"
      ],
      "metadata": {
        "id": "SUGqKl3l6ksd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LIME (Local Interpretable Model-Agnostic Explanations)"
      ],
      "metadata": {
        "id": "LcIhIEgl6t8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lime\n",
        "import lime.lime_tabular\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Create LIME explainer\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    X_train.values,\n",
        "    feature_names=X_train.columns,\n",
        "    class_names=['Class_0', 'Class_1'],\n",
        "    mode='classification'\n",
        ")\n",
        "\n",
        "# Explain single prediction\n",
        "exp = explainer.explain_instance(\n",
        "    X_test.iloc[0].values,\n",
        "    model.predict_proba,\n",
        "    num_features=10\n",
        ")\n",
        "exp.show_in_notebook()"
      ],
      "metadata": {
        "id": "a0unYEFH6rc2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recursive Feature Elimination (RFE) â€” The Elimination Method"
      ],
      "metadata": {
        "id": "4seSkUAI629R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "# Recursive Feature Elimination with Cross-Validation\n",
        "rfecv = RFECV(estimator=model, step=1, cv=5)\n",
        "rfecv.fit(X_train, y_train)\n",
        "\n",
        "# Get selected features\n",
        "selected_features = X_train.columns[rfecv.support_].tolist()\n",
        "print(f\"Selected features: {selected_features}\")\n",
        "print(f\"Number of features: {rfecv.n_features_}\")\n",
        "\n",
        "# Plot feature ranking\n",
        "import matplotlib.pyplot as plt\n",
        "plt.barh(X_train.columns, rfecv.ranking_)\n",
        "plt.xlabel('Ranking (1 = selected)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Es6fp-lZ6xtv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Tests (Chi-Square, ANOVA, Correlation)"
      ],
      "metadata": {
        "id": "5iSJ42fh685W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency, f_oneway\n",
        "import pandas as pd\n",
        "\n",
        "# Chi-square for categorical features\n",
        "def chi_square_test(df, categorical_col, target_col):\n",
        "    contingency_table = pd.crosstab(df[categorical_col], df[target_col])\n",
        "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "    return p_value\n",
        "\n",
        "# ANOVA for continuous features\n",
        "def anova_test(df, continuous_col, target_col):\n",
        "    groups = [group[continuous_col].values for name, group in df.groupby(target_col)]\n",
        "    f_stat, p_value = f_oneway(*groups)\n",
        "    return p_value\n",
        "\n",
        "# Calculate p-values for all features\n",
        "for col in df.columns:\n",
        "    if col == 'target':\n",
        "        continue\n",
        "    if df[col].dtype == 'object':  # categorical\n",
        "        p_val = chi_square_test(df, col, 'target')\n",
        "    else:  # continuous\n",
        "        p_val = anova_test(df, col, 'target')\n",
        "\n",
        "    print(f\"{col}: p-value = {p_val:.4f}\")\n",
        "    # p-value < 0.05 means statistically significant"
      ],
      "metadata": {
        "id": "Difxlula669M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Complete Feature Engineering Workflow (Step-by-Step)"
      ],
      "metadata": {
        "id": "QI9KnNVf7CK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('your_data.csv')\n",
        "# Basic info\n",
        "print(df.info())  # Data types, missing values\n",
        "print(df.describe())  # Statistics\n",
        "\n",
        "# Missing values\n",
        "missing = df.isnull().sum()\n",
        "print(f\"Missing values:\\n{missing}\")\n",
        "\n",
        "# Outliers (for numeric columns)\n",
        "for col in df.select_dtypes(include=[np.number]).columns:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]\n",
        "    print(f\"{col}: {len(outliers)} outliers\")\n",
        "\n",
        "# Correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Features highly correlated with target\n",
        "target_corr = correlation_matrix['target'].sort_values(ascending=False)\n",
        "print(target_corr)\n",
        "\n",
        "# Remove highly correlated features (multicollinearity)\n",
        "# If two features have correlation > 0.9, remove one\n",
        "high_corr_pairs = np.where(\n",
        "    np.abs(correlation_matrix) > 0.9\n",
        ")\n",
        "high_corr_features = set()\n",
        "for i, j in zip(high_corr_pairs[0], high_corr_pairs[1]):\n",
        "    if i != j:\n",
        "        high_corr_features.add(correlation_matrix.columns[j])\n",
        "print(f\"High correlation features to remove: {high_corr_features}\")\n",
        "df = df.drop(columns=high_corr_features)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop('target', axis=1),\n",
        "    df['target'],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train baseline model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "print(importances)\n",
        "\n",
        "# Remove low-importance features (< 1% importance)\n",
        "important_features = importances[importances['importance'] > 0.01]['feature'].tolist()\n",
        "X_train = X_train[important_features]\n",
        "X_test = X_test[important_features]\n",
        "\n",
        "# Date-based features\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['day_of_week'] = df['date'].dt.dayofweek\n",
        "df['month'] = df['date'].dt.month\n",
        "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
        "\n",
        "# Interaction features\n",
        "df['age_x_income'] = df['age'] * df['income']\n",
        "df['price_per_unit'] = df['total_price'] / df['quantity']\n",
        "\n",
        "# Categorical encoding\n",
        "df['category_encoded'] = pd.factorize(df['category'])[0]\n",
        "\n",
        "# Binning continuous variables\n",
        "df['age_bin'] = pd.cut(df['age'], bins=[0, 25, 50, 75, 100])\n",
        "\n",
        "# Log transformation (for skewed distributions)\n",
        "df['log_price'] = np.log1p(df['price'])\n",
        "print(df.head())\n",
        "\n",
        "# Train model with new features\n",
        "model_final = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_final.fit(X_train, y_train)\n",
        "\n",
        "# Compare performance\n",
        "baseline_score = model.score(X_test, y_test)\n",
        "final_score = model_final.score(X_test, y_test)\n",
        "print(f\"Baseline accuracy: {baseline_score:.4f}\")\n",
        "print(f\"Final accuracy: {final_score:.4f}\")\n",
        "print(f\"Improvement: {(final_score - baseline_score):.4f}\")"
      ],
      "metadata": {
        "id": "DEQIdUXN7AOy"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}
