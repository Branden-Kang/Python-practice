{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0p9jXutRcoAlD1iPIKGwV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@yocissms/lets-replicate-a-paper-lightning-flash-clustering-and-analysis-2304e82feafd)"
      ],
      "metadata": {
        "id": "wyEa9waceEya"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C-sld46YeBSh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Dict, Tuple\n",
        "import gzip\n",
        "import datetime\n",
        "\n",
        "array_type = np.ndarray[tuple[float], np.dtype[np.float64]]\n",
        "\n",
        "def load_dat(gz_file_path: str, min_stations: int=7, max_chi_squared: float=1.0, max_altitude: float=20e3) -> Tuple[array_type, datetime.datetime]:\n",
        "\n",
        "    with gzip.open(gz_file_path, 'rt') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    start_time_str = \"\"\n",
        "    for l in lines:\n",
        "        if \"Data start time:\" in l:\n",
        "            start_time_str = l.replace(\"Data start time:\", \"\").strip()\n",
        "\n",
        "    start_time = datetime.datetime.strptime(start_time_str, \"%m/%d/%y %H:%M:%S\")\n",
        "    idx = 1 + lines.index(\"*** data ***\\n\")\n",
        "    lines = lines[idx:]\n",
        "    data = np.zeros((len(lines), 7))\n",
        "\n",
        "    for line_idx, l in enumerate(lines):\n",
        "        splt = l.strip().split()\n",
        "        for j in range(6):\n",
        "            data[line_idx,j] = float(splt[j])\n",
        "        data[line_idx,6] = float(int(splt[6], 0).bit_count())\n",
        "\n",
        "    # from https://github.com/deeplycloudy/lmatools/blob/8d55e11dfbbe040f58f9a393f83e33e2a4b84b4c/examples/flashsort/clustertests/lma.py#L144\n",
        "    return data[(data[:,6] >= min_stations) & (data[:,4] <= max_chi_squared) & (data[:,4] < max_altitude)], start_time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_ecef(spatial_data: array_type) -> array_type:\n",
        "    rad_lat = spatial_data[:,0] * np.pi / 180\n",
        "    rad_lon = spatial_data[:,1] * np.pi / 180\n",
        "    altitudes = spatial_data[:,2]\n",
        "    a=6378137.0\n",
        "    b=6356752.314245\n",
        "    e2=1 - b**2/a**2\n",
        "\n",
        "    cot = 1 / np.tan(rad_lat)\n",
        "    n_phi = a / np.sqrt(1 - (e2/ (1+ np.square(cot))))\n",
        "\n",
        "    transformed_data = np.zeros(spatial_data.shape)\n",
        "    transformed_data[:,0] = (n_phi + altitudes) * np.cos(rad_lat) * np.cos(rad_lon)\n",
        "    transformed_data[:,1] = (n_phi + altitudes) * np.cos(rad_lat) * np.sin(rad_lon)\n",
        "    transformed_data[:,2] = ((1-e2)*n_phi + altitudes) * np.sin(rad_lat)\n",
        "\n",
        "    return transformed_data"
      ],
      "metadata": {
        "id": "R55jesoKeLLI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "center_geo = np.array([40.4463980, -104.6368130, 1000.00]) # COLMA center\n",
        "lma_center = to_ecef(np.expand_dims(center_geo, axis=0))\n",
        "\n",
        "n_grid_points, grid_spacing = 18, 0.2559\n",
        "grid_start_lat = center_geo[0] - (n_grid_points / 2) * grid_spacing\n",
        "grid_start_lon = center_geo[1] - (n_grid_points / 2) * grid_spacing\n",
        "\n",
        "data, start_time = load_dat(os.path.join(data_dir, fname), min_stations=min_stations, max_chi_squared=max_chi_squared, max_altitude=max_altitude)\n",
        "\n",
        "data_grid_lat = np.floor( (data[:,1] - grid_start_lat) / grid_spacing )\n",
        "data_grid_lon = np.floor( (data[:,2] - grid_start_lon) / grid_spacing )\n",
        "spatial_data = to_ecef(data[:,1:4]) - center\n",
        "\n",
        "sources = np.zeros((data.shape[0], 10))\n",
        "sources[:,0] = data[:,0]\n",
        "sources[:,1:4] = spatial_data\n",
        "sources[:,4] = data[:,5]\n",
        "sources[:,5] = data_grid_lat\n",
        "sources[:,6] = data_grid_lon\n",
        "sources[:,7:] = data[:,1:4] # retain initial geodetic coordinates"
      ],
      "metadata": {
        "id": "z8D0ronDeQRL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# sources[time, x, y, z, power, grid_lat, grid_lon, init_lat, init_lon, init_alt]\n",
        "def cluster(sources: array_type, xyz_scale: float, t_scale: float, grid_max: int, min_samples: int=10, epsilon: float=1.0, max_duration: float=3.0) -> Tuple[List[array_type], int]:\n",
        "    min_t = np.min(sources[:,0])\n",
        "    max_t = np.max(sources[:,0])\n",
        "    time_start = min_t\n",
        "\n",
        "    all_flashes: List[array_type]=[]\n",
        "    total_removed = 0\n",
        "\n",
        "    algorithm = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
        "\n",
        "    while time_start <= max_t:\n",
        "        indexes = (sources[:,0] >= time_start) & (sources[:,0] < time_start + max_duration*2)\n",
        "        first_half_indexes = (sources[:,0] >= time_start) & (sources[:,0] < time_start + max_duration)\n",
        "\n",
        "        dbscan_data = np.zeros((np.sum(indexes), 4))\n",
        "        dbscan_data[:,:3] = sources[indexes, 1:4] / xyz_scale\n",
        "        dbscan_data[:,3] = sources[indexes, 0] / t_scale\n",
        "\n",
        "        if dbscan_data.shape[0] > 0:\n",
        "\n",
        "            clustering = algorithm.fit(dbscan_data)\n",
        "            first_half_unique_labels = np.unique(clustering.labels_[:np.sum(first_half_indexes)])\n",
        "            first_half_cluster_indexes = np.squeeze(np.argwhere(np.isin(clustering.labels_, first_half_unique_labels)))\n",
        "            cluster_labels = clustering.labels_[first_half_cluster_indexes]\n",
        "            cluster_info, n_removed = get_cluster_list(sources[first_half_cluster_indexes,:], cluster_labels, grid_max)\n",
        "\n",
        "            all_flashes += cluster_info\n",
        "            total_removed += n_removed\n",
        "\n",
        "            mask = np.ones(len(sources), bool)\n",
        "            mask[first_half_cluster_indexes] = 0\n",
        "            sources = sources[mask]\n",
        "\n",
        "        time_start += max_duration\n",
        "\n",
        "    return all_flashes, total_removed\n",
        "\n",
        "def get_cluster_list(sources: array_type, cluster_ids: np.ndarray[int, np.dtype[np.int64]], grid_max: int) -> Tuple[List[array_type], int]:\n",
        "    unique_cluster_ids = np.unique(cluster_ids)\n",
        "    all_cluster_sources: List[array_type] = []\n",
        "    n_removed = 0\n",
        "    min_sources = 5\n",
        "\n",
        "    for cluster_id in unique_cluster_ids:\n",
        "        if cluster_id == -1:\n",
        "            continue\n",
        "\n",
        "        cluster_sources = sources[cluster_ids == cluster_id]\n",
        "\n",
        "        # remove flashes with out-of-bounds sources\n",
        "        if not np.all((cluster_sources[:,5:7] > -1) & (cluster_sources[:,5:7] < grid_max)):\n",
        "            n_removed += 1\n",
        "            continue\n",
        "\n",
        "        if cluster_sources.shape[0] >= min_sources:\n",
        "            all_cluster_sources.append(cluster_sources)\n",
        "\n",
        "    return all_cluster_sources, n_removed"
      ],
      "metadata": {
        "id": "sfGpsh7jeWZ1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_flashes(flashes: List[array_type], t_threshold: float=0.15, xyz_threshold: float=3000.0) -> List[array_type]:\n",
        "    flashes_sorted = sorted(flashes, key=lambda x: x[0,0])\n",
        "\n",
        "    flash_merges = np.zeros((len(flashes), 2)) - 1.0\n",
        "\n",
        "    for j in range(1, len(flashes_sorted)):\n",
        "        branch_flash = flashes_sorted[j]\n",
        "\n",
        "        for i in range(j):\n",
        "            base_flash = flashes_sorted[i]\n",
        "\n",
        "            if branch_flash[0,0] - base_flash[-1,0] > t_threshold:\n",
        "                continue\n",
        "\n",
        "            dists: np.ndarray[float, np.dtype[np.float64]] = np.linalg.norm(base_flash[:,1:4] - branch_flash[0, 1:4], axis=1)\n",
        "\n",
        "            if np.any(dists <= xyz_threshold):\n",
        "                min_dist = np.min(dists)\n",
        "\n",
        "                if flash_merges[j,1] == -1.0 or flash_merges[j,1] > min_dist:\n",
        "                    merge_idx = int(flash_merges[i,0]) if flash_merges[i,0] > -1.0 else i\n",
        "                    flash_merges[merge_idx,:] = np.array([merge_idx, min_dist])\n",
        "\n",
        "    flashes_merged: List[array_type | None] = [elem for elem in flashes_sorted]\n",
        "\n",
        "    for idx, flash in enumerate(flashes_sorted):\n",
        "        merge_idx = int(flash_merges[idx, 0])\n",
        "\n",
        "        if merge_idx > -1:\n",
        "            merged_sources = np.concatenate((flash, flashes_sorted[merge_idx]))\n",
        "            flashes_merged[merge_idx] = np.sort(merged_sources, axis=0)\n",
        "            flashes_merged[idx] = None\n",
        "\n",
        "    flashes_merged_remove_none: List[array_type] = [elem for elem in flashes_merged if elem is not None]\n",
        "\n",
        "    return flashes_merged_remove_none"
      ],
      "metadata": {
        "id": "lfKlBlBweoBw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "def get_flash_params(flashes: array_type, start_time: datetime.datetime) -> List[Dict]:\n",
        "    flash_params: List[Dict] = []\n",
        "\n",
        "    north_pole = np.array([90.0,0.0,0.0])\n",
        "\n",
        "    for flash in flashes:\n",
        "        grid_points = np.unique(flash[:,5:7], axis=0).astype('int64')\n",
        "        n_sources = len(flash)\n",
        "        duration = flash[-1,0] - flash[0,0]\n",
        "        mean_power = np.mean(flash[:,4])\n",
        "\n",
        "        # 2d hull area: see https://github.com/deeplycloudy/lmatools/blob/8d55e11dfbbe040f58f9a393f83e33e2a4b84b4c/lmatools/flashsort/flash_stats.py#L112\n",
        "        init_coords = flash[:,7:]\n",
        "        mean_point = np.mean(init_coords, axis=0)\n",
        "        init_coords -= (mean_point + north_pole)\n",
        "        coords_ecef = to_ecef(init_coords)\n",
        "        hull = ConvexHull(coords_ecef)\n",
        "\n",
        "        dist_from_center = np.linalg.norm(flash[0,1:3]) / 1000.0 # km\n",
        "\n",
        "        init_time = flash[0,0]\n",
        "        seconds_from_start = init_time - (start_time.hour * 3600 + start_time.minute * 60 + start_time.second)\n",
        "        init_datetime = start_time + datetime.timedelta(seconds=seconds_from_start)\n",
        "\n",
        "        flash_params.append({'n_sources': n_sources, 'duration': duration, 'mean_power': mean_power, 'grid_points': grid_points.tolist(),\n",
        "            'init_alt': flash[0,9], 'init_time': init_datetime.strftime(\"%m/%d/%y %H:%M:%S\"), 'hull_area': hull.volume*1e-6,\n",
        "            'dist_from_center': dist_from_center})\n",
        "\n",
        "    return flash_params\n"
      ],
      "metadata": {
        "id": "gtN1z9O0etUC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from datetime import date, timedelta\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "all_files = os.listdir(data_dir)\n",
        "\n",
        "for f_idx, fname in enumerate(all_files):\n",
        "  file_stem = str(Path(fname)).rstrip(''.join(Path(fname).suffixes))#Path(fname).stem\n",
        "  print(f\"processing {file_stem} ({str(f_idx + 1)} / {str(len(all_files))})\")\n",
        "\n",
        "  # load data\n",
        "  # ...\n",
        "\n",
        "  flashes, n_removed = cluster(sources, xyz_scale, t_scale, n_grid_points, min_samples=min_samples, epsilon=epsilon, max_duration=max_duration)\n",
        "\n",
        "  if len(flashes) == 0:\n",
        "      continue\n",
        "\n",
        "  flashes_merged = merge_flashes(flashes, t_threshold=merge_t_threshold, xyz_threshold=merge_xyz_threshold)\n",
        "  merge_count = len(flashes) - len(flashes_merged)\n",
        "\n",
        "  if save_flash_sources:\n",
        "      flashes_merged_npy = np.zeros((sum([elem.shape[0] for elem in flashes_merged]), 1+flashes_merged[0].shape[1]))\n",
        "      counter = 0\n",
        "      for flash_idx, flash in enumerate(flashes_merged):\n",
        "          flashes_merged_npy[counter:counter+flash.shape[0],:-1] = flash\n",
        "          flashes_merged_npy[counter:counter+flash.shape[0],-1] = flash_idx\n",
        "          counter += flash.shape[0]\n",
        "\n",
        "      with open(os.path.join(out_dir, f'{file_stem}_sources.npy'), 'wb') as f:\n",
        "          np.save(f, flashes_merged_npy)\n",
        "\n",
        "  flash_params = get_flash_params(flashes_merged, start_time)\n",
        "\n",
        "  with open(os.path.join(out_dir, f'{file_stem}.json'), 'w') as f:\n",
        "      json.dump({'data_start_time': start_time.strftime(\"%m/%d/%y %H:%M:%S\"), 'n_removed': n_removed, 'merge_count': merge_count, 'flash_params': flash_params}, f)\n"
      ],
      "metadata": {
        "id": "0Xgk4DGSe2dv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_buckets = int(max_dist // bucket_size)\n",
        "param_names = ('duration', 'hull_area', 'mean_power', 'n_sources')\n",
        "param_units = ('(ms)', '(square km)', '(dBW)', '')\n",
        "stats_buckets = [{param: [] for param in param_names} for i in range(n_buckets)]\n",
        "\n",
        "for fname in os.listdir(json_dir):\n",
        "    with open(os.path.join(json_dir, fname), 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    flashes = data['flash_params']\n",
        "    total_flash_count += len(flashes)\n",
        "\n",
        "    for flash in flashes:\n",
        "        bucket_idx = int(flash['dist_from_center'] // bucket_size)\n",
        "        grid_x_vals, grid_y_vals = [pt[0] for pt in flash['grid_points']], [pt[1] for pt in flash['grid_points']]\n",
        "\n",
        "        if not ( (grid_min_x <= min(grid_x_vals) and grid_max_x > max(grid_x_vals)) and (grid_min_y <= min(grid_y_vals) and grid_max_y > max(grid_y_vals)) ):\n",
        "            continue\n",
        "\n",
        "        if bucket_idx > len(stats_buckets):\n",
        "            continue\n",
        "        for param in param_names:\n",
        "            stats_buckets[bucket_idx][param].append(flash[param])\n",
        "\n",
        "percentiles = (5, 50, 95)\n",
        "all_flash_durations = []\n",
        "for elem in stats_buckets:\n",
        "    all_flash_durations += elem['duration']\n",
        "all_flash_durations = np.array(all_flash_durations)\n",
        "\n",
        "for p in percentiles:\n",
        "    d = np.percentile(all_flash_durations, p) * 1000\n",
        "    print(f'percentile {p} of flash durations (milliseconds): {round(d)}')\n",
        "\n",
        "plt.rc('xtick', labelsize=8)\n",
        "fig, axs = plt.subplots(2, 2)\n",
        "\n",
        "for param_idx, param in enumerate(param_names):\n",
        "    scale_val = 1e3 if param == 'duration' else 1.0 # convert duration to ms\n",
        "    param_vals = [np.array(elem[param]) * scale_val for elem in stats_buckets]\n",
        "\n",
        "    if param != 'mean_power':\n",
        "        axs[param_idx//2, param_idx % 2].set_yscale('log')\n",
        "\n",
        "    axs[param_idx//2, param_idx % 2].boxplot(param_vals, tick_labels=[str(i) for i in range(bucket_size, bucket_size*(len(param_vals)+1), bucket_size)], showfliers=False)\n",
        "    axs[param_idx//2, param_idx % 2].set(xlabel='distance from LMA center (km)', ylabel=f'{param} {param_units[param_idx]}')\n",
        "    axs[param_idx//2, param_idx % 2].set_title(param)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(plot_dir, 'flash_params.png'))\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "zZRbfzbRe-Fs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as pe\n",
        "import datetime\n",
        "\n",
        "all_flash_areas = {}\n",
        "all_flash_hours = {}\n",
        "\n",
        "for fname in os.listdir(json_dir):\n",
        "    with open(os.path.join(json_dir, fname), 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    flashes = data['flash_params']\n",
        "\n",
        "    for flash in flashes:\n",
        "\n",
        "        for grid_point in flash['grid_points']:\n",
        "            if tuple(grid_point) not in all_flash_areas:\n",
        "                all_flash_areas[tuple(grid_point)] = []\n",
        "            all_flash_areas[tuple(grid_point)].append(flash['hull_area'])\n",
        "\n",
        "            if tuple(grid_point) not in all_flash_hours:\n",
        "                all_flash_hours[tuple(grid_point)] = []\n",
        "\n",
        "            dt = datetime.datetime.strptime(flash['init_time'], \"%m/%d/%y %H:%M:%S\")\n",
        "            all_flash_hours[tuple(grid_point)].append(dt.strftime(\"%m/%d/%y %H\"))\n",
        "\n",
        "area_grid = np.zeros((grid_size, grid_size))\n",
        "lh_grid = np.zeros((grid_size, grid_size))\n",
        "fph_grid = np.zeros((grid_size, grid_size))\n",
        "\n",
        "for k, v in all_flash_areas.items():\n",
        "    flashes_sorted = sorted(v)\n",
        "    median = flashes_sorted[len(flashes_sorted) // 2]\n",
        "    area_grid[k[0],k[1]] = median\n",
        "\n",
        "for k, v in all_flash_hours.items():\n",
        "    n_hours = len(list(set(v)))\n",
        "    lh_grid[k[0],k[1]] = n_hours\n",
        "    cntr = Counter(v)\n",
        "    fph_grid[k[0],k[1]] = np.mean(np.array(list(cntr.values())))\n",
        "\n",
        "grids = (area_grid, lh_grid, fph_grid)\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(9, 3), subplot_kw={'projection': ccrs.PlateCarree()})\n",
        "\n",
        "for param_idx, param in enumerate(('flash_area', 'lightning_hours', 'flashes_per_lightning_hour')):\n",
        "    grid = grids[param_idx]\n",
        "    vmax = 100.0 if param == 'flash_area' else np.max(grid)\n",
        "\n",
        "    axs[param_idx].add_feature(cfeature.STATES, linewidth=1.4, edgecolor='white')\n",
        "    plot_extent = [*grid_longitude_range, *grid_latitude_range]\n",
        "    axs[param_idx].set_extent(plot_extent,crs=ccrs.PlateCarree())\n",
        "    axs[param_idx].imshow(grid, origin='upper', cmap='jet', extent=plot_extent, vmax=vmax, transform=ccrs.PlateCarree())\n",
        "\n",
        "    for landmark_idx in range(len(landmark_names)):\n",
        "        landmark_coords = landmark_coordinates[landmark_idx]\n",
        "\n",
        "        axs[param_idx].plot(landmark_coords[1], landmark_coords[0], 'wo', markersize=7, transform=ccrs.PlateCarree())\n",
        "        axs[param_idx].text(landmark_coords[1]-0.001, landmark_coords[0]+0.2, landmark_names[landmark_idx], color='black', size=14, path_effects=[pe.withStroke(linewidth=2, foreground=\"white\")], transform=ccrs.PlateCarree())\n",
        "    axs[param_idx].set_title(param)\n",
        "\n",
        "plt.savefig(os.path.join(plot_dir, 'grid_stats.png'))"
      ],
      "metadata": {
        "id": "zP4HIH4NfE6P"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}
