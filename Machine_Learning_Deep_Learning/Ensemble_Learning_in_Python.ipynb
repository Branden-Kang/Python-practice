{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble Learning in Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP3r3FfbJwAEus/W5Sgw3YZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/)"
      ],
      "metadata": {
        "id": "YfMlIIaokG4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Ensemble Techniques\n",
        "## Max Voting"
      ],
      "metadata": {
        "id": "5g_patamkOdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, when you asked 5 of your colleagues to rate your movie (out of 5); weâ€™ll assume three of them rated it as 4 while two of them gave it a 5."
      ],
      "metadata": {
        "id": "vSqRNbLgketp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fT60DdHMkEvL"
      },
      "outputs": [],
      "source": [
        "# from sklearn.ensemble import VotingClassifier\n",
        "# model1 = LogisticRegression(random_state=1)\n",
        "# model2 = tree.DecisionTreeClassifier(random_state=1)\n",
        "# model = VotingClassifier(estimators=[('lr', model1), ('dt', model2)], voting='hard')\n",
        "# model.fit(x_train,y_train)\n",
        "# model.score(x_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Averaging\n",
        "For example, in the below case, the averaging method would take the average of all the values.\n",
        "\n",
        "i.e. (5+4+5+4+4)/5 = 4.4"
      ],
      "metadata": {
        "id": "rL3Qz9aqkgcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model1 = tree.DecisionTreeClassifier()\n",
        "# model2 = KNeighborsClassifier()\n",
        "# model3= LogisticRegression()\n",
        "\n",
        "# model1.fit(x_train,y_train)\n",
        "# model2.fit(x_train,y_train)\n",
        "# model3.fit(x_train,y_train)\n",
        "\n",
        "# pred1=model1.predict_proba(x_test)\n",
        "# pred2=model2.predict_proba(x_test)\n",
        "# pred3=model3.predict_proba(x_test)\n",
        "\n",
        "# finalpred=(pred1+pred2+pred3)/3"
      ],
      "metadata": {
        "id": "U4baD5DIkLaz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weighted Average\n",
        "The result is calculated as [(5*0.23) + (4*0.23) + (5*0.18) + (4*0.18) + (4*0.18)] = 4.41.\n",
        "\n"
      ],
      "metadata": {
        "id": "afcLXf6Akx16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model1 = tree.DecisionTreeClassifier()\n",
        "# model2 = KNeighborsClassifier()\n",
        "# model3= LogisticRegression()\n",
        "\n",
        "# model1.fit(x_train,y_train)\n",
        "# model2.fit(x_train,y_train)\n",
        "# model3.fit(x_train,y_train)\n",
        "\n",
        "# pred1=model1.predict_proba(x_test)\n",
        "# pred2=model2.predict_proba(x_test)\n",
        "# pred3=model3.predict_proba(x_test)\n",
        "\n",
        "# finalpred=(pred1*0.3+pred2*0.3+pred3*0.4)"
      ],
      "metadata": {
        "id": "M8_kXB1lknu-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Ensemble techniques\n",
        "## Stacking"
      ],
      "metadata": {
        "id": "bCl694Bck45W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def Stacking(model,train,y,test,n_fold):\n",
        "#    folds=StratifiedKFold(n_splits=n_fold,random_state=1)\n",
        "#    test_pred=np.empty((test.shape[0],1),float)\n",
        "#    train_pred=np.empty((0,1),float)\n",
        "#    for train_indices,val_indices in folds.split(train,y.values):\n",
        "#       x_train,x_val=train.iloc[train_indices],train.iloc[val_indices]\n",
        "#       y_train,y_val=y.iloc[train_indices],y.iloc[val_indices]\n",
        "\n",
        "#       model.fit(X=x_train,y=y_train)\n",
        "#       train_pred=np.append(train_pred,model.predict(x_val))\n",
        "#       test_pred=np.append(test_pred,model.predict(test))\n",
        "#       return test_pred.reshape(-1,1),train_pred"
      ],
      "metadata": {
        "id": "0G-bu5pgk3b0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model1 = tree.DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "# test_pred1 ,train_pred1=Stacking(model=model1,n_fold=10, train=x_train,test=x_test,y=y_train)\n",
        "\n",
        "# train_pred1=pd.DataFrame(train_pred1)\n",
        "# test_pred1=pd.DataFrame(test_pred1)"
      ],
      "metadata": {
        "id": "MONvyEd-lEcG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model2 = KNeighborsClassifier()\n",
        "\n",
        "# test_pred2 ,train_pred2=Stacking(model=model2,n_fold=10,train=x_train,test=x_test,y=y_train)\n",
        "\n",
        "# train_pred2=pd.DataFrame(train_pred2)\n",
        "# test_pred2=pd.DataFrame(test_pred2)"
      ],
      "metadata": {
        "id": "WAX5dk3ElII3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.concat([train_pred1, train_pred2], axis=1)\n",
        "# df_test = pd.concat([test_pred1, test_pred2], axis=1)\n",
        "\n",
        "# model = LogisticRegression(random_state=1)\n",
        "# model.fit(df,y_train)\n",
        "# model.score(df_test, y_test)"
      ],
      "metadata": {
        "id": "GMUAghAqlImN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blending"
      ],
      "metadata": {
        "id": "3oERsY1klOIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model1 = tree.DecisionTreeClassifier()\n",
        "# model1.fit(x_train, y_train)\n",
        "# val_pred1=model1.predict(x_val)\n",
        "# test_pred1=model1.predict(x_test)\n",
        "# val_pred1=pd.DataFrame(val_pred1)\n",
        "# test_pred1=pd.DataFrame(test_pred1)\n",
        "\n",
        "# model2 = KNeighborsClassifier()\n",
        "# model2.fit(x_train,y_train)\n",
        "# val_pred2=model2.predict(x_val)\n",
        "# test_pred2=model2.predict(x_test)\n",
        "# val_pred2=pd.DataFrame(val_pred2)\n",
        "# test_pred2=pd.DataFrame(test_pred2)\n",
        "\n",
        "# df_val=pd.concat([x_val, val_pred1,val_pred2],axis=1)\n",
        "# df_test=pd.concat([x_test, test_pred1,test_pred2],axis=1)\n",
        "\n",
        "# model = LogisticRegression()\n",
        "# model.fit(df_val,y_val)\n",
        "# model.score(df_test,y_test)"
      ],
      "metadata": {
        "id": "24cSs8hqlLXp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bagging\n",
        "1. Multiple subsets are created from the original dataset, selecting observations with replacement.\n",
        "2. A base model (weak model) is created on each of these subsets.\n",
        "3. The models run in parallel and are independent of each other.\n",
        "4. The final predictions are determined by combining the predictions from all the models"
      ],
      "metadata": {
        "id": "Ybunz2UIlW2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting\n",
        "1. A subset is created from the original dataset.\n",
        "2. Initially, all data points are given equal weights.\n",
        "3. A base model is created on this subset.\n",
        "4. This model is used to make predictions on the whole dataset.\n",
        "5. Errors are calculated using the actual values and predicted values.\n",
        "6. The observations which are incorrectly predicted, are given higher weights.\n",
        "(Here, the three misclassified blue-plus points will be given higher weights)\n",
        "7.Another model is created and predictions are made on the dataset.\n",
        "(This model tries to correct the errors from the previous model)\n",
        "8. Similarly, multiple models are created, each correcting the errors of the previous model.\n",
        "9. The final model (strong learner) is the weighted mean of all the models (weak learners)."
      ],
      "metadata": {
        "id": "7sELAsQElpSO"
      }
    }
  ]
}
