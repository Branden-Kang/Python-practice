{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzaf9ePg6IuVUD+7N5KRnF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@chenycy/accelerate-your-machine-learning-skills-with-hyperparameter-tuning-00dfd27e261c)"
      ],
      "metadata": {
        "id": "VTaHaraozSgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!pip install optuna-integration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFDP4_Dmzk6f",
        "outputId": "3b7f6997-c622-408b-c099-8b5f1940ff77"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Collecting optuna-integration\n",
            "  Downloading optuna_integration-3.6.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.4/93.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (from optuna-integration) (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (1.13.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (2.0.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna->optuna-integration) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna->optuna-integration) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna->optuna-integration) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna->optuna-integration) (2.1.5)\n",
            "Installing collected packages: optuna-integration\n",
            "Successfully installed optuna-integration-3.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "clf = SVC()\n",
        "param_distributions = {\n",
        "    \"C\": optuna.distributions.FloatDistribution(1e-10, 1e10, log=True)\n",
        "}\n",
        "optuna_search = optuna.integration.OptunaSearchCV(clf, param_distributions)\n",
        "X, y = load_iris(return_X_y=True)\n",
        "optuna_search.fit(X, y)\n",
        "y_pred = optuna_search.predict(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bi80G_yzjdZ",
        "outputId": "a30598ee-dac9-4d34-f00e-8921771d26da"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-97bd0419b4cb>:9: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
            "  optuna_search = optuna.integration.OptunaSearchCV(clf, param_distributions)\n",
            "[I 2024-06-26 23:08:44,922] A new study created in memory with name: no-name-6819e165-1abc-400d-bf56-166b037daba3\n",
            "[I 2024-06-26 23:08:44,947] Trial 0 finished with value: 0.9266666666666667 and parameters: {'C': 0.03028706732111826}. Best is trial 0 with value: 0.9266666666666667.\n",
            "[I 2024-06-26 23:08:44,968] Trial 1 finished with value: 0.9266666666666667 and parameters: {'C': 1.3317070172045463e-09}. Best is trial 0 with value: 0.9266666666666667.\n",
            "[I 2024-06-26 23:08:44,984] Trial 2 finished with value: 0.9800000000000001 and parameters: {'C': 3.3779618556336124}. Best is trial 2 with value: 0.9800000000000001.\n",
            "[I 2024-06-26 23:08:45,010] Trial 3 finished with value: 0.9400000000000001 and parameters: {'C': 1267373463.957981}. Best is trial 2 with value: 0.9800000000000001.\n",
            "[I 2024-06-26 23:08:45,035] Trial 4 finished with value: 0.9266666666666667 and parameters: {'C': 0.0005335274801604334}. Best is trial 2 with value: 0.9800000000000001.\n",
            "[I 2024-06-26 23:08:45,052] Trial 5 finished with value: 0.9733333333333334 and parameters: {'C': 81.59168511066764}. Best is trial 2 with value: 0.9800000000000001.\n",
            "[I 2024-06-26 23:08:45,077] Trial 6 finished with value: 0.9400000000000001 and parameters: {'C': 778277579.9783195}. Best is trial 2 with value: 0.9800000000000001.\n",
            "[I 2024-06-26 23:08:45,099] Trial 7 finished with value: 0.9266666666666667 and parameters: {'C': 8.770393477952852e-07}. Best is trial 2 with value: 0.9800000000000001.\n",
            "[I 2024-06-26 23:08:45,125] Trial 8 finished with value: 0.9400000000000001 and parameters: {'C': 1933347.7230871336}. Best is trial 2 with value: 0.9800000000000001.\n",
            "[I 2024-06-26 23:08:45,150] Trial 9 finished with value: 0.9266666666666667 and parameters: {'C': 0.031678807243744306}. Best is trial 2 with value: 0.9800000000000001.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln0_uvlJyvm7",
        "outputId": "f29ff89f-84bc-4cc0-8965-1942358e5c56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 0.1, 'coef0': 0.0, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "# Define the hyperparameter grid for DecisionTreeClassifier\n",
        "dt_param_grid = {\n",
        "    'max_depth': [None, 5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2],\n",
        "    'max_features': [None, 'sqrt', 'log2'],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'splitter': ['best', 'random']\n",
        "}\n",
        "\n",
        "# Create the DecisionTreeClassifier\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "\n",
        "\n",
        "# Define the hyperparameter grid for GradientBoostingClassifier\n",
        "gb_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'subsample': [0.8, 0.9, 1.0],  # Fraction of samples used for fitting the individual base learners\n",
        "    'max_features': ['sqrt', 'log2', None],  # Number of features to consider for the best split\n",
        "    'random_state': [42]  # Random seed for reproducibility\n",
        "}\n",
        "\n",
        "# Create the GradientBoostingClassifier\n",
        "gb_classifier = GradientBoostingClassifier()\n",
        "\n",
        "# Define the hyperparameter distributions\n",
        "rf_param_dist = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'bootstrap': [True, False],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "rf_classifier = RandomForestClassifier()\n",
        "\n",
        "# Define the hyperparameter grid for Multinomial Naive Bayes\n",
        "nb_param_grid = {\n",
        "    'alpha': [0.1, 0.5, 1.0],      # Additive smoothing parameter\n",
        "    'fit_prior': [True, False]     # Whether to learn class prior probabilities\n",
        "}\n",
        "\n",
        "# Create the Multinomial Naive Bayes model\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Define the hyperparameter grid for k\n",
        "knn_param_grid = {\n",
        "    'n_neighbors': [3, 5, 7],       # Number of neighbors to consider\n",
        "    'weights': ['uniform', 'distance'],  # Weighting method\n",
        "    'p': [1, 2],                      # Power parameter for Minkowski distance\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Algorithm for computing neighbors\n",
        "    'leaf_size': [10, 20, 30],        # Leaf size for tree-based algorithms\n",
        "    'metric': ['euclidean', 'manhattan', 'chebyshev']  # Distance metric\n",
        "}\n",
        "\n",
        "# Create the KNN model\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "\n",
        "# Define the hyperparameter grid for kernel type\n",
        "svm_param_grid = {\n",
        "    'C': [0.1, 1, 10],                # Regularization parameter\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'degree': [2, 3, 4],               # Degree of the polynomial kernel (only for 'poly')\n",
        "    'gamma': ['scale', 'auto', 0.1, 1],  # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
        "    'coef0': [0.0, 1.0, 2.0]            # Independent term in the kernel function\n",
        "}\n",
        "\n",
        "# Create the SVM model\n",
        "svm_classifier = SVC()\n",
        "\n",
        "# Use svc as the example for Grid Search\n",
        "grid_search = GridSearchCV(estimator=svm_classifier, param_grid=svm_param_grid, cv=5)\n",
        "\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# randomized_search = RandomizedSearchCV(estimator=svm_classifier, param_grid=svm_param_grid, cv=5)\n",
        "\n",
        "# randomized_search.fit(X, y)\n",
        "\n",
        "# print(\"Best Hyperparameters:\", randomized_search.best_params_)"
      ],
      "metadata": {
        "id": "RMAVLOmczcgG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from hyperopt import fmin, tpe, hp\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load a sample dataset (or use your own data)\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Define the objective function for neural network hyperparameter tuning\n",
        "def objective(params):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=params['units'], activation=params['activation'], input_dim=X.shape[1]))\n",
        "\n",
        "    for _ in range(params['num_layers']):\n",
        "        model.add(Dense(units=params['units'], activation=params['activation']))\n",
        "\n",
        "    model.add(Dense(units=params['output_units'], activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=params['optimizer'],\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    history = model.fit(X_train, y_train, epochs=params['epochs'], validation_data=(X_val, y_val), verbose=0)\n",
        "\n",
        "    val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "    # Hyperopt minimizes the objective function, so negate the validation loss\n",
        "    return val_loss\n",
        "\n",
        "# Define the search space for neural network hyperparameters\n",
        "space = {\n",
        "    'units': hp.quniform('units', 32, 512, 32),\n",
        "    'num_layers': hp.quniform('num_layers', 1, 3, 1),\n",
        "    'activation': hp.choice('activation', ['relu', 'tanh', 'sigmoid']),\n",
        "    'output_units': hp.quniform('output_units', 2, 10, 1),\n",
        "    'optimizer': hp.choice('optimizer', ['adam', 'sgd']),\n",
        "    'epochs': hp.choice('epochs', [10, 20, 30, 50]),\n",
        "    'batch_size': hp.choice('batch_size', [16, 32, 64]),\n",
        "    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5),\n",
        "    'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
        "}\n",
        "\n",
        "# Run Hyperopt to find the best hyperparameters\n",
        "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, rstate=np.random.RandomState(42))\n",
        "\n",
        "print(\"Best Hyperparameters:\", best)"
      ],
      "metadata": {
        "id": "oKKV3pr6zdjf"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}
