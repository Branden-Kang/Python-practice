{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqaaIvo6ScVA4/PBCQM59o"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@francescofranco_39234/batch-normalization-for-training-neural-networks-328112bda3ae)"
      ],
      "metadata": {
        "id": "bKvRux4SuYja"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jQz-82quWm5",
        "outputId": "4d206640-2dca-4886-b1eb-53bbc7ea529c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 45.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/cifar-10-python.tar.gz to /content\n",
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 2.144\n",
            "Loss after mini-batch  1000: 2.015\n",
            "Loss after mini-batch  1500: 1.950\n",
            "Loss after mini-batch  2000: 1.925\n",
            "Loss after mini-batch  2500: 1.879\n",
            "Loss after mini-batch  3000: 1.864\n",
            "Loss after mini-batch  3500: 1.833\n",
            "Loss after mini-batch  4000: 1.818\n",
            "Loss after mini-batch  4500: 1.797\n",
            "Loss after mini-batch  5000: 1.778\n",
            "Starting epoch 2\n",
            "Loss after mini-batch   500: 1.742\n",
            "Loss after mini-batch  1000: 1.747\n",
            "Loss after mini-batch  1500: 1.736\n",
            "Loss after mini-batch  2000: 1.749\n",
            "Loss after mini-batch  2500: 1.725\n",
            "Loss after mini-batch  3000: 1.729\n",
            "Loss after mini-batch  3500: 1.721\n",
            "Loss after mini-batch  4000: 1.695\n",
            "Loss after mini-batch  4500: 1.697\n",
            "Loss after mini-batch  5000: 1.686\n",
            "Starting epoch 3\n",
            "Loss after mini-batch   500: 1.660\n",
            "Loss after mini-batch  1000: 1.672\n",
            "Loss after mini-batch  1500: 1.667\n",
            "Loss after mini-batch  2000: 1.674\n",
            "Loss after mini-batch  2500: 1.643\n",
            "Loss after mini-batch  3000: 1.654\n",
            "Loss after mini-batch  3500: 1.659\n",
            "Loss after mini-batch  4000: 1.633\n",
            "Loss after mini-batch  4500: 1.641\n",
            "Loss after mini-batch  5000: 1.620\n",
            "Starting epoch 4\n",
            "Loss after mini-batch   500: 1.637\n",
            "Loss after mini-batch  1000: 1.614\n",
            "Loss after mini-batch  1500: 1.611\n",
            "Loss after mini-batch  2000: 1.608\n",
            "Loss after mini-batch  2500: 1.620\n",
            "Loss after mini-batch  3000: 1.604\n",
            "Loss after mini-batch  3500: 1.596\n",
            "Loss after mini-batch  4000: 1.582\n",
            "Loss after mini-batch  4500: 1.606\n",
            "Loss after mini-batch  5000: 1.600\n",
            "Starting epoch 5\n",
            "Loss after mini-batch   500: 1.577\n",
            "Loss after mini-batch  1000: 1.563\n",
            "Loss after mini-batch  1500: 1.544\n",
            "Loss after mini-batch  2000: 1.561\n",
            "Loss after mini-batch  2500: 1.558\n",
            "Loss after mini-batch  3000: 1.594\n",
            "Loss after mini-batch  3500: 1.588\n",
            "Loss after mini-batch  4000: 1.556\n",
            "Loss after mini-batch  4500: 1.580\n",
            "Loss after mini-batch  5000: 1.575\n",
            "Training process has finished.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(32 * 32 * 3, 64),\n",
        "      nn.BatchNorm1d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.BatchNorm1d(32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 10)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "\n",
        "  # Prepare CIFAR-10 dataset\n",
        "  dataset = CIFAR10(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
        "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "\n",
        "  # Initialize the MLP\n",
        "  mlp = MLP()\n",
        "\n",
        "  # Define the loss function and optimizer\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "  # Run the training loop\n",
        "  for epoch in range(0, 5): # 5 epochs at maximum\n",
        "\n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "    # Set current loss value\n",
        "    current_loss = 0.0\n",
        "\n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "      # Get inputs\n",
        "      inputs, targets = data\n",
        "\n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Perform forward pass\n",
        "      outputs = mlp(inputs)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = loss_function(outputs, targets)\n",
        "\n",
        "      # Perform backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # Perform optimization\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print statistics\n",
        "      current_loss += loss.item()\n",
        "      if i % 500 == 499:\n",
        "          print('Loss after mini-batch %5d: %.3f' %\n",
        "                (i + 1, current_loss / 500))\n",
        "          current_loss = 0.0\n",
        "\n",
        "  # Process is complete.\n",
        "  print('Training process has finished.')"
      ]
    }
  ]
}
