{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCfuaV1x0UQ4uOFNevMTN0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rus5ZAxFtn8H"
      },
      "outputs": [],
      "source": [
        "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
        "from imblearn.over_sampling import SMOTENC, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df_final.drop(['target'], axis=1)\n",
        "y = df_final['target']\n",
        "\n",
        "# Automatically extract categorical feature indices\n",
        "categorical_columns = X.select_dtypes(include=['object', 'category']).columns\n",
        "categorical_feature_indices = [X.columns.get_loc(col) for col in categorical_columns]\n",
        "\n",
        "# Split into training and test sets with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Define the target size for majority classes after undersampling and for minority classes after oversampling\n",
        "undersampling_target = 5000\n",
        "oversampling_target = 6000\n",
        "\n",
        "# Step 1: Undersample the majority classes\n",
        "undersampling_strategy = {cls: min(count, undersampling_target)\n",
        "                          for cls, count in Counter(y_train).items()}\n",
        "undersampler = RandomUnderSampler(sampling_strategy=undersampling_strategy, random_state=42)\n",
        "X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# Step 2: SMOTENC for oversampling\n",
        "smote = SMOTENC(categorical_features=categorical_feature_indices,\n",
        "                sampling_strategy={cls: max(count, oversampling_target)\n",
        "                                   for cls, count in Counter(y_train_under).items()},\n",
        "                random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_under, y_train_under)\n",
        "\n",
        "# Step 3: ADASYN oversampling\n",
        "adasyn = ADASYN(random_state=42)\n",
        "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_under, y_train_under)\n",
        "\n",
        "# Step 4: Define models for comparison\n",
        "models = {\n",
        "    \"BalancedRandomForest\": BalancedRandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"EasyEnsemble\": EasyEnsembleClassifier(n_estimators=100, random_state=42),\n",
        "    \"RandomForest_Weighted\": RandomForestClassifier(class_weight='balanced', random_state=42),\n",
        "    \"XGBoost_ScalePosWeight\": XGBClassifier(\n",
        "        scale_pos_weight=(len(y_train) - sum(y_train == 1)) / sum(y_train == 1),\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "# Train and evaluate each model with different balanced data\n",
        "results = {}\n",
        "for model_name, model in models.items():\n",
        "    # Select the appropriate training data for each model\n",
        "    if model_name in [\"BalancedRandomForest\", \"EasyEnsemble\"]:\n",
        "        # Use undersampled data for these ensemble models\n",
        "        X_train_balanced, y_train_balanced = X_train_under, y_train_under\n",
        "    else:\n",
        "        # Use SMOTE or ADASYN balanced data for other models\n",
        "        X_train_balanced, y_train_balanced = X_train_smote, y_train_smote\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')  # Weighted for multi-class\n",
        "    classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "    # Store the results\n",
        "    results[model_name] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1_score\": f1,\n",
        "        \"precision\": classification_rep['macro avg']['precision'],\n",
        "        \"recall\": classification_rep['macro avg']['recall']\n",
        "    }\n",
        "\n",
        "# Display results for comparison\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df.sort_values(by='f1_score', ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Use Ensemble Methods like BalancedRandomForest or EasyEnsemble:\n",
        "- BalancedRandomForest: An ensemble method that combines random under-sampling with bagging. It trains multiple decision trees on different balanced subsets of the data.\n",
        "- EasyEnsemble: Creates multiple balanced subsets of the majority class and trains a classifier on each, then aggregates their predictions."
      ],
      "metadata": {
        "id": "iKX5jJASt8PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Example with BalancedRandomForest\n",
        "brf = BalancedRandomForestClassifier(n_estimators=100, random_state=42)\n",
        "brf.fit(X_train, y_train)\n",
        "y_pred_brf = brf.predict(X_test)\n",
        "print(\"Balanced Random Forest Performance:\")\n",
        "print(classification_report(y_test, y_pred_brf))\n",
        "\n",
        "# Example with EasyEnsemble\n",
        "eec = EasyEnsembleClassifier(n_estimators=100, random_state=42)\n",
        "eec.fit(X_train, y_train)\n",
        "y_pred_eec = eec.predict(X_test)\n",
        "print(\"Easy Ensemble Classifier Performance:\")\n",
        "print(classification_report(y_test, y_pred_eec))"
      ],
      "metadata": {
        "id": "_0nEhst9t-EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Try Class Weights in Models:\n",
        "- Many classifiers like RandomForest, LogisticRegression, XGBoost, and others allow you to set class_weight='balanced', which automatically adjusts the weights inversely proportional to class frequencies.\n",
        "- For models like XGBoost or LightGBM, you can manually specify scale_pos_weight for handling imbalance."
      ],
      "metadata": {
        "id": "317IGuqnuDwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# RandomForest with class weights\n",
        "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"Random Forest with Balanced Class Weights Performance:\")\n",
        "print(classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "EC6fndLtuFjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Example with XGBoost\n",
        "xgb = XGBClassifier(scale_pos_weight=(len(y_train) - sum(y_train == 1)) / sum(y_train == 1))\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "print(\"XGBoost with Scale_Pos_Weight Performance:\")\n",
        "print(classification_report(y_test, y_pred_xgb))"
      ],
      "metadata": {
        "id": "25c7a6eBuJVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Advanced Sampling Techniques like ADASYN:\n",
        "- ADASYN (Adaptive Synthetic Sampling) is similar to SMOTE but focuses more on generating samples for harder-to-learn minority class samples.\n",
        "- It can be used when you need a more adaptive approach to handling class imbalance."
      ],
      "metadata": {
        "id": "LvYZnuLjuLVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "adasyn = ADASYN(random_state=42)\n",
        "X_train_balanced, y_train_balanced = adasyn.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "orSbZwUzuMwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADASYN might be more effective than SMOTE when some minority samples are harder to classify."
      ],
      "metadata": {
        "id": "u9yrzBknuPff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Feature Engineering and Data Augmentation:\n",
        "Focus on domain-specific feature engineering to create new features that might better differentiate between classes.\n",
        "Use data augmentation techniques (e.g., synthetic data generation, transformations) if applicable to your domain."
      ],
      "metadata": {
        "id": "jBFEbNzduTSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Deep Learning Models with Custom Loss Functions:\n",
        "If you have a large dataset, using deep learning models with custom loss functions like Focal Loss can be effective.\n",
        "Focal Loss focuses more on harder-to-classify samples, making it suitable for imbalanced data."
      ],
      "metadata": {
        "id": "p2Awr6SwuY24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def focal_loss(gamma=2., alpha=0.25):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        epsilon = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
        "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
        "        weight = alpha * tf.pow(1 - y_pred, gamma)\n",
        "        loss = weight * cross_entropy\n",
        "        return tf.reduce_mean(loss)\n",
        "    return focal_loss_fixed"
      ],
      "metadata": {
        "id": "G7JzK4w3uXY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Focal loss can be used in models like TensorFlow or PyTorch for better handling of imbalance."
      ],
      "metadata": {
        "id": "jrJLEuPhucnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Stacking or Blending Models:\n",
        "Combining multiple models using stacking or blending can help capture different aspects of the data distribution.\n",
        "This might be effective if different models handle different classes better."
      ],
      "metadata": {
        "id": "_rX12BdKueLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Adjust Evaluation Metrics:\n",
        "- If the issue is related to the perceived performance based on evaluation metrics, focus on metrics like AUC-ROC, F1-score, Precision-Recall curve, G-mean, or Cohen's Kappa rather than just accuracy.\n",
        "- These metrics provide better insights into model performance on imbalanced data.\n"
      ],
      "metadata": {
        "id": "HUSkplFiugUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "- Using ensemble methods like BalancedRandomForest and EasyEnsemble or gradient boosting techniques with class weights often yields significant improvements.\n",
        "Techniques like Focal Loss in deep learning, ADASYN, and advanced feature engineering can also offer gains.\n",
        "- Adjusting the evaluation metric might also reveal improvements that are not reflected in accuracy.\n",
        "- Consider testing these approaches on your dataset and evaluating the impact using metrics like precision, recall, and F1-score to determine which method provides the best balance between class performance."
      ],
      "metadata": {
        "id": "If8_xpuTul7Q"
      }
    }
  ]
}
