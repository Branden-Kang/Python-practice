{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVFuADkb7AJU3Zl/NX8d7/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Complete, runnable TabTransformer pipeline with automatic preprocessing for string categorical columns.\n",
        "# Single-class TabTransformer (Backbone + Head merged)\n",
        "\n",
        "# Third Party\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 0) Preprocessor: string categorical -> integer IDs (with OOV=0)\n",
        "#    + auto cat_idx/cont_idx + cardinalities\n",
        "# ---------------------------------------------------------\n",
        "class TabularPreprocessor:\n",
        "    def __init__(self, categorical_indices=None, use_oov=True, oov_token=0, add_na_token=True):\n",
        "        self.categorical_indices = None if categorical_indices is None else list(categorical_indices)\n",
        "        self.use_oov = use_oov\n",
        "        self.oov_token = int(oov_token)  # 0 recommended\n",
        "        self.add_na_token = add_na_token\n",
        "        self.cat_maps = {}          # col_idx -> {category_value: int_id}\n",
        "        self.cardinalities = []     # per-categorical-column unique count (K, excluding OOV)\n",
        "        self.cat_idx = []\n",
        "        self.cont_idx = []\n",
        "        self.fitted_ = False\n",
        "\n",
        "    def _ensure_ndarray(self, X):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values\n",
        "        return np.asarray(X, dtype=object)  # safe for mixed types\n",
        "\n",
        "    def fit(self, X, categorical_indices=None):\n",
        "        X = self._ensure_ndarray(X)\n",
        "        n_cols = X.shape[1]\n",
        "\n",
        "        if categorical_indices is not None:\n",
        "            self.categorical_indices = list(categorical_indices)\n",
        "\n",
        "        if self.categorical_indices is None:\n",
        "            # fallback: infer by dtype==object\n",
        "            self.categorical_indices = [j for j in range(n_cols) if X[:, j].dtype == object]\n",
        "\n",
        "        cat_set = set(self.categorical_indices)\n",
        "        self.cat_idx = sorted(list(cat_set))\n",
        "        self.cont_idx = [j for j in range(n_cols) if j not in cat_set]\n",
        "\n",
        "        # build per-column maps: real categories 1..K (0 reserved for OOV)\n",
        "        self.cat_maps = {}\n",
        "        self.cardinalities = []\n",
        "        for j in self.cat_idx:\n",
        "            col = X[:, j]\n",
        "            if self.add_na_token:\n",
        "                col = np.where(pd.isna(col), \"<NA>\", col)\n",
        "            uniques = pd.unique(col)\n",
        "            id_map = {val: i + 1 for i, val in enumerate(uniques)}  # 1..K\n",
        "            self.cat_maps[j] = id_map\n",
        "            self.cardinalities.append(len(uniques))  # exclude OOV\n",
        "\n",
        "        self.fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        assert self.fitted_, \"Call fit() before transform().\"\n",
        "        X = self._ensure_ndarray(X)\n",
        "\n",
        "        # categorical\n",
        "        if len(self.cat_idx) > 0:\n",
        "            x_cat = np.zeros((X.shape[0], len(self.cat_idx)), dtype=\"int64\")\n",
        "            for ti, j in enumerate(self.cat_idx):\n",
        "                col = X[:, j]\n",
        "                if self.add_na_token:\n",
        "                    col = np.where(pd.isna(col), \"<NA>\", col)\n",
        "                id_map = self.cat_maps[j]\n",
        "                x_cat[:, ti] = np.array([id_map.get(v, self.oov_token) for v in col], dtype=\"int64\")\n",
        "        else:\n",
        "            x_cat = np.zeros((X.shape[0], 0), dtype=\"int64\")\n",
        "\n",
        "        # continuous\n",
        "        if len(self.cont_idx) > 0:\n",
        "            x_cont = X[:, self.cont_idx].astype(\"float32\")\n",
        "        else:\n",
        "            x_cont = None\n",
        "\n",
        "        return x_cat, x_cont\n",
        "\n",
        "    def fit_transform(self, X, categorical_indices=None):\n",
        "        self.fit(X, categorical_indices)\n",
        "        return self.transform(X)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1) Single-class TabTransformer (Backbone + Continuous + Head)\n",
        "# ---------------------------------------------------------\n",
        "class TabTransformerNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Single-class TabTransformer:\n",
        "    - Categorical: per-column embedding (+ OOV/pad), column embedding, Transformer encoder, pooling\n",
        "    - Continuous: BatchNorm + (optional) Linear projection to d_token\n",
        "    - Head: MLP -> sigmoid\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        cat_cardinalities,          # List[int] (exclude OOV)\n",
        "        n_continuous=0,\n",
        "        d_token=32,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,\n",
        "        embedding_dropout=0.1,\n",
        "        add_cls=False,\n",
        "        pooling=\"concat\",           # \"concat\" or \"cls\"\n",
        "        cont_proj=\"linear\",         # \"none\" or \"linear\"\n",
        "        mlp_hidden_dims=(128, 64),\n",
        "        mlp_dropout=0.2,\n",
        "        padding_idx=0,              # reserve 0 for OOV/pad if not None\n",
        "        norm_first=True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert pooling in (\"concat\", \"cls\")\n",
        "        self.n_cat = len(cat_cardinalities)\n",
        "        self.n_cont = n_continuous\n",
        "        self.d_token = d_token\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "        self.cont_proj = cont_proj\n",
        "\n",
        "        # ---- Categorical path ----\n",
        "        if self.n_cat == 0:\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "            self.col_embedding = None\n",
        "        else:\n",
        "            # +1 slot if padding_idx is used (OOV/pad)\n",
        "            self.cat_embeddings = nn.ModuleList([\n",
        "                nn.Embedding(\n",
        "                    num_embeddings=c + (1 if (padding_idx is not None) else 0),\n",
        "                    embedding_dim=d_token,\n",
        "                    padding_idx=padding_idx\n",
        "                )\n",
        "                for c in cat_cardinalities\n",
        "            ])\n",
        "            self.col_embedding = nn.Embedding(self.n_cat, d_token)\n",
        "\n",
        "        if self.add_cls:\n",
        "            self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n",
        "            nn.init.normal_(self.cls_token, std=0.02)\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout(embedding_dropout)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_token,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=attn_dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=norm_first\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "\n",
        "        # init categorical embeddings\n",
        "        for emb in self.cat_embeddings:\n",
        "            nn.init.normal_(emb.weight, std=0.02)\n",
        "        if self.col_embedding is not None:\n",
        "            nn.init.normal_(self.col_embedding.weight, std=0.02)\n",
        "\n",
        "        # ---- Continuous path ----\n",
        "        if n_continuous > 0:\n",
        "            self.cont_bn = nn.BatchNorm1d(n_continuous)\n",
        "            if cont_proj == \"linear\":\n",
        "                self.cont_linear = nn.Linear(n_continuous, d_token)\n",
        "                nn.init.kaiming_uniform_(self.cont_linear.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "                cont_out_dim = d_token\n",
        "            else:\n",
        "                self.cont_linear = nn.Identity()\n",
        "                cont_out_dim = n_continuous\n",
        "        else:\n",
        "            self.cont_bn = None\n",
        "            self.cont_linear = None\n",
        "            cont_out_dim = 0\n",
        "\n",
        "        # ---- Head ----\n",
        "        backbone_out = (d_token if pooling == \"cls\" else self.n_cat * d_token)\n",
        "        in_dim = backbone_out + cont_out_dim\n",
        "\n",
        "        layers = []\n",
        "        prev = in_dim\n",
        "        for h in mlp_hidden_dims:\n",
        "            lin = nn.Linear(prev, h)\n",
        "            nn.init.kaiming_uniform_(lin.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "            layers.extend([lin, nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(mlp_dropout)])\n",
        "            prev = h\n",
        "        layers.append(nn.Linear(prev, 1))\n",
        "        layers.append(nn.Sigmoid())\n",
        "        self.head = nn.Sequential(*layers)\n",
        "\n",
        "    def _encode_categoricals(self, x_cat: torch.LongTensor):\n",
        "        \"\"\"\n",
        "        x_cat: (B, n_cat) -> contextualized representation\n",
        "        returns:\n",
        "          pooling='concat' -> (B, n_cat*d)\n",
        "          pooling='cls'    -> (B, d)\n",
        "        \"\"\"\n",
        "        B = x_cat.size(0)\n",
        "        if self.n_cat == 0:\n",
        "            return torch.zeros(B, self.d_token if self.pooling == \"cls\" else 0,\n",
        "                               device=x_cat.device, dtype=torch.float32)\n",
        "\n",
        "        tok_list = []\n",
        "        for j, emb in enumerate(self.cat_embeddings):\n",
        "            tok = emb(x_cat[:, j])                         # (B, d)\n",
        "            if self.col_embedding is not None:\n",
        "                tok = tok + self.col_embedding.weight[j]   # (d,)\n",
        "            tok_list.append(tok.unsqueeze(1))              # (B, 1, d)\n",
        "\n",
        "        x_tok = torch.cat(tok_list, dim=1)                 # (B, n_cat, d)\n",
        "\n",
        "        if self.add_cls:\n",
        "            cls = self.cls_token.expand(B, -1, -1)         # (B, 1, d)\n",
        "            x_tok = torch.cat([cls, x_tok], dim=1)         # (B, 1+n_cat, d)\n",
        "\n",
        "        x_tok = self.embedding_dropout(x_tok)\n",
        "        z = self.transformer(x_tok)                        # (B, T, d)\n",
        "\n",
        "        if self.pooling == \"cls\" and self.add_cls:\n",
        "            out = z[:, 0, :]                               # (B, d)\n",
        "        elif self.pooling == \"cls\":\n",
        "            out = z.mean(dim=1)                            # (B, d)\n",
        "        else:\n",
        "            if self.add_cls:\n",
        "                z = z[:, 1:, :]                            # drop CLS\n",
        "            out = z.reshape(B, -1)                         # (B, n_cat*d)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x_cat: torch.LongTensor, x_cont: torch.FloatTensor = None):\n",
        "        z_cat = self._encode_categoricals(x_cat)\n",
        "        if (x_cont is not None) and (self.n_cont > 0):\n",
        "            if x_cont.ndim == 1:\n",
        "                x_cont = x_cont.unsqueeze(1)\n",
        "            x_cont = self.cont_bn(x_cont)\n",
        "            x_cont = self.cont_linear(x_cont)\n",
        "            z = torch.cat([z_cat, x_cont], dim=1)\n",
        "        else:\n",
        "            z = z_cat\n",
        "        out = self.head(z)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2) Sklearn-Compatible Classifier with Auto Preprocessing\n",
        "# ---------------------------------------------------------\n",
        "class TabTransformerBinaryClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    Example:\n",
        "        clf = TabTransformerBinaryClassifier(\n",
        "            auto_preprocess=True,\n",
        "            categorical_indices=[0, 3, 5],\n",
        "            d_token=32, n_heads=4, n_layers=2, ...\n",
        "        )\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        cat_idx=None,\n",
        "        cat_cardinalities=None,\n",
        "        cont_idx=None,\n",
        "        d_token=32,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,\n",
        "        embedding_dropout=0.1,\n",
        "        add_cls=False,\n",
        "        pooling=\"concat\",\n",
        "        cont_proj=\"linear\",\n",
        "        hidden_dims=(128, 64),\n",
        "        mlp_dropout=0.2,\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-4,\n",
        "        loss_fn=\"logloss\",\n",
        "        device=None,\n",
        "        # Auto preprocessing\n",
        "        auto_preprocess=True,\n",
        "        categorical_indices=None,\n",
        "        use_oov=True\n",
        "    ):\n",
        "        # manual fields (kept for compatibility)\n",
        "        self.cat_idx = [] if cat_idx is None else list(cat_idx)\n",
        "        self.cat_cardinalities = None if cat_cardinalities is None else list(cat_cardinalities)\n",
        "        self.cont_idx = [] if cont_idx is None else list(cont_idx)\n",
        "\n",
        "        # model/training params\n",
        "        self.d_token = d_token\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.dim_feedforward = dim_feedforward\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.embedding_dropout = embedding_dropout\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "        self.cont_proj = cont_proj\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.mlp_dropout = mlp_dropout\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.loss_fn_name = loss_fn\n",
        "\n",
        "        self.model = None\n",
        "        self.best_model_weights = None\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # auto-preproc\n",
        "        self.auto_preprocess = auto_preprocess\n",
        "        self.categorical_indices = categorical_indices\n",
        "        self.use_oov = use_oov\n",
        "        self.preproc = None\n",
        "\n",
        "    def _build_model(self):\n",
        "        n_cont = len(self.cont_idx)\n",
        "        padding_idx = 0 if self.use_oov else None\n",
        "        model = TabTransformerNet(\n",
        "            cat_cardinalities=self.cat_cardinalities,\n",
        "            n_continuous=n_cont,\n",
        "            d_token=self.d_token,\n",
        "            n_heads=self.n_heads,\n",
        "            n_layers=self.n_layers,\n",
        "            dim_feedforward=self.dim_feedforward,\n",
        "            attn_dropout=self.attn_dropout,\n",
        "            embedding_dropout=self.embedding_dropout,\n",
        "            add_cls=self.add_cls,\n",
        "            pooling=self.pooling,\n",
        "            cont_proj=self.cont_proj,\n",
        "            mlp_hidden_dims=self.hidden_dims,\n",
        "            mlp_dropout=self.mlp_dropout,\n",
        "            padding_idx=padding_idx,\n",
        "            norm_first=True\n",
        "        )\n",
        "        return model.to(self.device)\n",
        "\n",
        "    def _define_loss_fn(self):\n",
        "        if self.loss_fn_name == \"logloss\":\n",
        "            return nn.BCELoss(reduction=\"none\")\n",
        "        else:\n",
        "            raise Exception(f\"{self.loss_fn_name} is not defined\")\n",
        "\n",
        "    def _split_X(self, X):\n",
        "        \"\"\"\n",
        "        If auto_preprocess=True -> use preproc.transform(X).\n",
        "        Else -> use provided indices.\n",
        "        \"\"\"\n",
        "        if self.auto_preprocess:\n",
        "            assert self.preproc is not None and self.preproc.fitted_, \"Call fit() first.\"\n",
        "            x_cat_np, x_cont_np = self.preproc.transform(X)\n",
        "        else:\n",
        "            X_np = X.detach().cpu().numpy() if isinstance(X, torch.Tensor) else X\n",
        "            if len(self.cat_idx) > 0:\n",
        "                x_cat_np = X_np[:, self.cat_idx].astype(\"int64\")\n",
        "            else:\n",
        "                x_cat_np = np.zeros((X_np.shape[0], 0), dtype=\"int64\")\n",
        "            x_cont_np = (X_np[:, self.cont_idx].astype(\"float32\") if len(self.cont_idx) > 0 else None)\n",
        "\n",
        "        x_cat = torch.tensor(x_cat_np, dtype=torch.long, device=self.device)\n",
        "        x_cont = torch.tensor(x_cont_np, dtype=torch.float32, device=self.device) if x_cont_np is not None else None\n",
        "        return x_cat, x_cont\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        X,\n",
        "        y,\n",
        "        sample_weight=None,\n",
        "        eval_set=None,            # list of tuples: [(X_val, y_val)]\n",
        "        eval_metric=None,         # supports [\"logloss\"]\n",
        "        max_epochs=10,\n",
        "        patience=None,\n",
        "        batch_size=32,\n",
        "        num_workers=0,\n",
        "        verbose=True,\n",
        "        pin_memory=None\n",
        "    ):\n",
        "        if pin_memory is None:\n",
        "            pin_memory = (self.device == \"cuda\")\n",
        "\n",
        "        # Auto-preprocessing: infer indices + cardinalities from data\n",
        "        if self.auto_preprocess:\n",
        "            self.preproc = TabularPreprocessor(\n",
        "                categorical_indices=self.categorical_indices,\n",
        "                use_oov=self.use_oov,\n",
        "                oov_token=0,\n",
        "                add_na_token=True\n",
        "            )\n",
        "            self.preproc.fit(X)  # derive cat_idx/cont_idx/cardinalities\n",
        "            self.cat_idx = self.preproc.cat_idx\n",
        "            self.cont_idx = self.preproc.cont_idx\n",
        "            self.cat_cardinalities = self.preproc.cardinalities\n",
        "\n",
        "        # Tensors\n",
        "        x_cat, x_cont = self._split_X(X)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "        w_tensor = (torch.tensor(sample_weight, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "                    if sample_weight is not None else torch.ones_like(y_tensor))\n",
        "\n",
        "        # Validation\n",
        "        if eval_set is not None:\n",
        "            X_val, y_val = eval_set[0]\n",
        "            x_cat_val, x_cont_val = self._split_X(X_val)\n",
        "            y_val_tensor = torch.tensor(y_val, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "        else:\n",
        "            x_cat_val = x_cont_val = y_val_tensor = None\n",
        "\n",
        "        # Build model\n",
        "        if self.model is None:\n",
        "            self.model = self._build_model()\n",
        "\n",
        "        loss_fn = self._define_loss_fn()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        # DataLoaders\n",
        "        if x_cont is None:\n",
        "            train_dataset = TensorDataset(x_cat, y_tensor, w_tensor)\n",
        "            def _iter(loader):\n",
        "                for Xc_b, y_b, w_b in loader:\n",
        "                    yield Xc_b, None, y_b, w_b\n",
        "        else:\n",
        "            train_dataset = TensorDataset(x_cat, x_cont, y_tensor, w_tensor)\n",
        "            def _iter(loader):\n",
        "                for Xc_b, Xn_b, y_b, w_b in loader:\n",
        "                    yield Xc_b, Xn_b, y_b, w_b\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True,\n",
        "            num_workers=num_workers, pin_memory=pin_memory\n",
        "        )\n",
        "\n",
        "        best_loss, patience_counter = float(\"inf\"), 0\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            self.model.train()\n",
        "            epoch_loss, n_steps = 0.0, 0\n",
        "\n",
        "            for Xc_b, Xn_b, y_b, w_b in _iter(train_loader):\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = self.model(Xc_b, Xn_b)\n",
        "                loss = loss_fn(y_pred, y_b)\n",
        "                (loss * w_b).sum().div(w_b.sum()).backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += (loss * w_b).sum().div(w_b.sum()).item()\n",
        "                n_steps += 1\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch + 1}/{max_epochs} - train_loss: {epoch_loss / max(1, n_steps):.6f}\")\n",
        "\n",
        "            # ---- Validation ----\n",
        "            if eval_set is not None:\n",
        "                self.model.eval()\n",
        "                with torch.no_grad():\n",
        "                    if x_cont_val is None:\n",
        "                        val_ds = TensorDataset(x_cat_val, y_val_tensor)\n",
        "                        val_loader = DataLoader(val_ds, batch_size=2048, shuffle=False,\n",
        "                                                num_workers=num_workers, pin_memory=pin_memory)\n",
        "                        eval_loss, n_eval = 0.0, 0\n",
        "                        for Xc_v, y_v in val_loader:\n",
        "                            y_p = self.model(Xc_v, None)\n",
        "                            lv = loss_fn(y_p, y_v)\n",
        "                            eval_loss += (lv.sum() / len(lv)).item()\n",
        "                            n_eval += 1\n",
        "                    else:\n",
        "                        val_ds = TensorDataset(x_cat_val, x_cont_val, y_val_tensor)\n",
        "                        val_loader = DataLoader(val_ds, batch_size=2048, shuffle=False,\n",
        "                                                num_workers=num_workers, pin_memory=pin_memory)\n",
        "                        eval_loss, n_eval = 0.0, 0\n",
        "                        for Xc_v, Xn_v, y_v in val_loader:\n",
        "                            y_p = self.model(Xc_v, Xn_v)\n",
        "                            lv = loss_fn(y_p, y_v)\n",
        "                            eval_loss += (lv.sum() / len(lv)).item()\n",
        "                            n_eval += 1\n",
        "                    eval_loss /= max(1, n_eval)\n",
        "                    if verbose:\n",
        "                        print(f\"          val_loss: {eval_loss:.6f}\")\n",
        "\n",
        "                    if patience is not None:\n",
        "                        if eval_loss < best_loss:\n",
        "                            best_loss = eval_loss\n",
        "                            patience_counter = 0\n",
        "                            self.best_model_weights = {k: v.detach().cpu().clone()\n",
        "                                                       for k, v in self.model.state_dict().items()}\n",
        "                        else:\n",
        "                            patience_counter += 1\n",
        "                            if patience_counter >= patience:\n",
        "                                if verbose:\n",
        "                                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                                break\n",
        "\n",
        "        if self.best_model_weights is not None:\n",
        "            self.model.load_state_dict(self.best_model_weights)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        x_cat, x_cont = self._split_X(X)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            probs1 = self.model(x_cat, x_cont).detach().cpu().numpy()  # (B, 1)\n",
        "        probs1 = probs1.astype(\"float\")\n",
        "        probs0 = 1.0 - probs1\n",
        "        return np.hstack([probs0, probs1])\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return probs.argmax(axis=1)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3) Sample mixed-type dataset generator (with strings)\n",
        "# ---------------------------------------------------------\n",
        "def make_mixed_sample(\n",
        "    n_samples=40000,\n",
        "    seed=7\n",
        "):\n",
        "    rng = np.random.RandomState(seed)\n",
        "\n",
        "    genders = np.array([\"남성\", \"여성\", \"기타\"], dtype=object)\n",
        "    cities = np.array([\"서울\", \"부산\", \"대구\", \"인천\", \"수원\", \"고양\"], dtype=object)\n",
        "    devices = np.array([\"ios\", \"android\", \"web\"], dtype=object)\n",
        "\n",
        "    # categorical columns\n",
        "    gender_col = rng.choice(genders, size=n_samples, p=[0.48, 0.48, 0.04])\n",
        "    city_col   = rng.choice(cities, size=n_samples)\n",
        "    device_col = rng.choice(devices, size=n_samples, p=[0.35, 0.55, 0.10])\n",
        "\n",
        "    # continuous columns\n",
        "    n_cont = 10\n",
        "    X_cont = rng.randn(n_samples, n_cont).astype(\"float32\")\n",
        "\n",
        "    # latent score\n",
        "    w_gender = {g: w for g, w in zip(genders, rng.uniform(-0.8, 0.8, size=len(genders)))}\n",
        "    w_city   = {c: w for c, w in zip(cities, rng.uniform(-0.6, 1.0, size=len(cities)))}\n",
        "    w_device = {d: w for d, w in zip(devices, rng.uniform(-0.5, 0.9, size=len(devices)))}\n",
        "    w_cont   = rng.randn(n_cont).astype(\"float32\")\n",
        "\n",
        "    score_cat = (np.vectorize(lambda v: w_gender[v])(gender_col) +\n",
        "                 np.vectorize(lambda v: w_city[v])(city_col) +\n",
        "                 np.vectorize(lambda v: w_device[v])(device_col)).astype(\"float32\")\n",
        "    score_cont = (X_cont * w_cont).sum(axis=1).astype(\"float32\")\n",
        "\n",
        "    bias = 0.1\n",
        "    noise = rng.normal(scale=0.5, size=n_samples).astype(\"float32\")\n",
        "    logit = 0.7 * score_cat + 0.8 * score_cont + bias + noise\n",
        "    prob = 1.0 / (1.0 + np.exp(-logit))\n",
        "    y = (prob > 0.5).astype(\"int64\")\n",
        "\n",
        "    # combine into feature array (object dtype)\n",
        "    X = np.empty((n_samples, 3 + n_cont), dtype=object)\n",
        "    X[:, 0] = gender_col\n",
        "    X[:, 1] = city_col\n",
        "    X[:, 2] = device_col\n",
        "    X[:, 3:] = X_cont\n",
        "\n",
        "    categorical_feature_indices = [0, 1, 2]\n",
        "    return X, y, categorical_feature_indices\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4) Train & Evaluate Demo\n",
        "# ---------------------------------------------------------\n",
        "def train_and_evaluate_demo():\n",
        "    np.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # ----- Data -----\n",
        "    X, y, categorical_feature_indices = make_mixed_sample(n_samples=30000, seed=123)\n",
        "\n",
        "    # train/val/test split\n",
        "    N = X.shape[0]\n",
        "    idx = np.arange(N)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    tr_end = int(N * 0.7)\n",
        "    va_end = int(N * 0.85)\n",
        "    tr_idx, va_idx, te_idx = idx[:tr_end], idx[tr_end:va_end], idx[va_end:]\n",
        "\n",
        "    X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
        "    X_va, y_va = X[va_idx], y[va_idx]\n",
        "    X_te, y_te = X[te_idx], y[te_idx]\n",
        "\n",
        "    # ----- Model -----\n",
        "    clf = TabTransformerBinaryClassifier(\n",
        "        auto_preprocess=True,\n",
        "        categorical_indices=categorical_feature_indices,\n",
        "        use_oov=True,                # reserve 0 for OOV/pad\n",
        "        # model hyperparams\n",
        "        d_token=32,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,\n",
        "        embedding_dropout=0.05,\n",
        "        pooling=\"concat\",\n",
        "        add_cls=False,\n",
        "        cont_proj=\"linear\",\n",
        "        hidden_dims=(128, 64),\n",
        "        mlp_dropout=0.2,\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-4,\n",
        "        loss_fn=\"logloss\",\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric=[\"logloss\"],\n",
        "        max_epochs=10,\n",
        "        patience=2,\n",
        "        batch_size=1024,     # adjust per GPU/CPU memory\n",
        "        num_workers=0,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # ----- Eval -----\n",
        "    proba_te = clf.predict_proba(X_te)[:, 1]\n",
        "    pred_te = (proba_te >= 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_te, pred_te)\n",
        "    auc = roc_auc_score(y_te, proba_te)\n",
        "    ll  = log_loss(y_te, np.vstack([1 - proba_te, proba_te]).T)\n",
        "\n",
        "    print(\"\\n===== Test Metrics =====\")\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"ROC-AUC  : {auc:.4f}\")\n",
        "    print(f\"Logloss  : {ll:.4f}\")\n",
        "\n",
        "    print(\"\\nSample predictions (first 10):\")\n",
        "    print(np.round(proba_te[:10], 4))\n",
        "\n",
        "    if clf.auto_preprocess:\n",
        "        print(\"\\n[Info] Derived categorical indices:\", clf.cat_idx)\n",
        "        print(\"[Info] Derived continuous indices :\", clf.cont_idx[:10], \"... (total:\", len(clf.cont_idx), \")\")\n",
        "        print(\"[Info] Cardinalities (per cat col):\", clf.cat_cardinalities)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate_demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SledIQyXyDrM",
        "outputId": "c77494a5-8f8f-4b9e-c7c7-2a3f6244bc8b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - train_loss: 0.552070\n",
            "          val_loss: 0.384910\n",
            "Epoch 2/10 - train_loss: 0.333158\n",
            "          val_loss: 0.254635\n",
            "Epoch 3/10 - train_loss: 0.231352\n",
            "          val_loss: 0.198390\n",
            "Epoch 4/10 - train_loss: 0.184456\n",
            "          val_loss: 0.162576\n",
            "Epoch 5/10 - train_loss: 0.156747\n",
            "          val_loss: 0.154019\n",
            "Epoch 6/10 - train_loss: 0.143633\n",
            "          val_loss: 0.139364\n",
            "Epoch 7/10 - train_loss: 0.135302\n",
            "          val_loss: 0.143477\n",
            "Epoch 8/10 - train_loss: 0.130429\n",
            "          val_loss: 0.137863\n",
            "Epoch 9/10 - train_loss: 0.128534\n",
            "          val_loss: 0.129339\n",
            "Epoch 10/10 - train_loss: 0.124611\n",
            "          val_loss: 0.130620\n",
            "\n",
            "===== Test Metrics =====\n",
            "Accuracy : 0.9436\n",
            "ROC-AUC  : 0.9892\n",
            "Logloss  : 0.1340\n",
            "\n",
            "Sample predictions (first 10):\n",
            "[6.650e-02 4.000e-03 9.848e-01 9.843e-01 2.000e-04 4.890e-02 8.000e-04\n",
            " 9.824e-01 9.808e-01 9.997e-01]\n",
            "\n",
            "[Info] Derived categorical indices: [0, 1, 2]\n",
            "[Info] Derived continuous indices : [3, 4, 5, 6, 7, 8, 9, 10, 11, 12] ... (total: 10 )\n",
            "[Info] Cardinalities (per cat col): [3, 6, 3]\n"
          ]
        }
      ]
    }
  ]
}
