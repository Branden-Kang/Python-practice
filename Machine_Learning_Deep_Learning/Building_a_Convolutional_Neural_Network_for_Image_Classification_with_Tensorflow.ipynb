{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building a Convolutional Neural Network for Image Classification with Tensorflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPB7o3Dj4HJJZSecsgaaoeJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEDiIBKW1KdH"
      },
      "source": [
        "[Reference](https://medium.com/@sidathasiri/building-a-convolutional-neural-network-for-image-classification-with-tensorflow-f1f2f56bd83b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7roBEu8suwDM",
        "outputId": "7946ac17-5b2f-4da3-9390-ff02d7c56dcf"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-14 11:26:01--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.2.128, 142.250.141.128, 142.250.101.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.2.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M   157MB/s    in 0.4s    \n",
            "\n",
            "2021-07-14 11:26:02 (157 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoCI-NVaxAft",
        "outputId": "d01df084-1300-4b71-ea76-c88811aea765"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdgVSqodxFJU"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBFakQCdxRna"
      },
      "source": [
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "validation_cats_dir = os.path.join(validation_dir, 'dogs')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w96Ry7ISxy9M",
        "outputId": "a3919fb6-5088-4710-a4a7-51d9b48adf15"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "test_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(validation_dir,\n",
        "                                                        batch_size=20,\n",
        "                                                        class_mode='binary',\n",
        "                                                        target_size=(150, 150))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq5S-r1fygkZ"
      },
      "source": [
        "# Constructing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqVVc2W-zBkt"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "-SKhy3pIzFnc",
        "outputId": "a3edb09f-0d99-4da9-a1db-71426f200944"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny_7nBAkyVyA"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1024, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5izewIbVzYKT"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-p9fyOwy_8M"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(optimizer=RMSprop(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics = ['acc'])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKjA-rYRzh0i",
        "outputId": "ca626705-5558-466c-9220-8967455129e9"
      },
      "source": [
        "history = model.fit_generator(train_generator,\n",
        "                              validation_data=validation_generator,\n",
        "                              steps_per_epoch=100,\n",
        "                              epochs=15,\n",
        "                              validation_steps=50,\n",
        "                              verbose=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "100/100 [==============================] - 71s 695ms/step - loss: 0.9084 - acc: 0.5785 - val_loss: 0.9427 - val_acc: 0.5010\n",
            "Epoch 2/15\n",
            "100/100 [==============================] - 69s 691ms/step - loss: 0.6479 - acc: 0.6500 - val_loss: 0.6358 - val_acc: 0.6710\n",
            "Epoch 3/15\n",
            "100/100 [==============================] - 69s 689ms/step - loss: 0.5613 - acc: 0.7210 - val_loss: 0.6459 - val_acc: 0.6520\n",
            "Epoch 4/15\n",
            "100/100 [==============================] - 69s 690ms/step - loss: 0.4849 - acc: 0.7575 - val_loss: 0.6201 - val_acc: 0.6840\n",
            "Epoch 5/15\n",
            "100/100 [==============================] - 69s 689ms/step - loss: 0.3835 - acc: 0.8235 - val_loss: 0.6626 - val_acc: 0.6870\n",
            "Epoch 6/15\n",
            "100/100 [==============================] - 69s 690ms/step - loss: 0.2930 - acc: 0.8750 - val_loss: 0.6389 - val_acc: 0.7190\n",
            "Epoch 7/15\n",
            "100/100 [==============================] - 69s 690ms/step - loss: 0.1967 - acc: 0.9200 - val_loss: 1.2167 - val_acc: 0.6540\n",
            "Epoch 8/15\n",
            "100/100 [==============================] - 69s 689ms/step - loss: 0.1424 - acc: 0.9455 - val_loss: 1.2761 - val_acc: 0.6500\n",
            "Epoch 9/15\n",
            "100/100 [==============================] - 69s 686ms/step - loss: 0.0987 - acc: 0.9645 - val_loss: 1.2223 - val_acc: 0.7130\n",
            "Epoch 10/15\n",
            "100/100 [==============================] - 69s 687ms/step - loss: 0.0887 - acc: 0.9765 - val_loss: 1.0681 - val_acc: 0.7170\n",
            "Epoch 11/15\n",
            "100/100 [==============================] - 69s 686ms/step - loss: 0.0461 - acc: 0.9885 - val_loss: 1.5149 - val_acc: 0.6900\n",
            "Epoch 12/15\n",
            "100/100 [==============================] - 69s 688ms/step - loss: 0.0726 - acc: 0.9810 - val_loss: 1.3434 - val_acc: 0.6990\n",
            "Epoch 13/15\n",
            "100/100 [==============================] - 69s 687ms/step - loss: 0.0602 - acc: 0.9870 - val_loss: 1.7240 - val_acc: 0.7050\n",
            "Epoch 14/15\n",
            "100/100 [==============================] - 68s 685ms/step - loss: 0.0311 - acc: 0.9935 - val_loss: 1.8956 - val_acc: 0.7330\n",
            "Epoch 15/15\n",
            "100/100 [==============================] - 69s 686ms/step - loss: 0.1237 - acc: 0.9770 - val_loss: 1.7241 - val_acc: 0.7200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ23B-p-0dDu"
      },
      "source": [
        "# Image Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbG6dASjzuvP"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsHk1G-u5pmQ"
      },
      "source": [
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2we66qEe5n58",
        "outputId": "7d83d63a-dd62-4cac-d959-61479f75454c"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(validation_dir,\n",
        "                                                        batch_size=20,\n",
        "                                                        class_mode='binary',\n",
        "                                                        target_size=(150, 150))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDykB17Y0unL",
        "outputId": "288d2625-b44a-4a8d-cb7a-d968ea7c83d0"
      },
      "source": [
        "history = model.fit_generator(train_generator,\n",
        "                              validation_data=validation_generator,\n",
        "                              steps_per_epoch=100,\n",
        "                              epochs=15,\n",
        "                              validation_steps=50,\n",
        "                              verbose=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "100/100 [==============================] - 78s 782ms/step - loss: 0.7011 - acc: 0.6560 - val_loss: 0.6287 - val_acc: 0.6410\n",
            "Epoch 2/15\n",
            "100/100 [==============================] - 78s 778ms/step - loss: 0.6351 - acc: 0.6580 - val_loss: 0.6414 - val_acc: 0.6650\n",
            "Epoch 3/15\n",
            "100/100 [==============================] - 78s 778ms/step - loss: 0.6183 - acc: 0.6865 - val_loss: 0.6024 - val_acc: 0.6580\n",
            "Epoch 4/15\n",
            "100/100 [==============================] - 78s 779ms/step - loss: 0.6154 - acc: 0.6660 - val_loss: 0.5837 - val_acc: 0.6800\n",
            "Epoch 5/15\n",
            "100/100 [==============================] - 78s 775ms/step - loss: 0.5985 - acc: 0.6850 - val_loss: 0.6067 - val_acc: 0.6910\n",
            "Epoch 6/15\n",
            "100/100 [==============================] - 78s 778ms/step - loss: 0.6056 - acc: 0.6815 - val_loss: 0.6762 - val_acc: 0.6410\n",
            "Epoch 7/15\n",
            "100/100 [==============================] - 77s 774ms/step - loss: 0.6087 - acc: 0.6935 - val_loss: 0.5711 - val_acc: 0.7050\n",
            "Epoch 8/15\n",
            "100/100 [==============================] - 77s 774ms/step - loss: 0.5866 - acc: 0.6980 - val_loss: 0.5695 - val_acc: 0.7160\n",
            "Epoch 9/15\n",
            "100/100 [==============================] - 77s 773ms/step - loss: 0.5809 - acc: 0.7115 - val_loss: 0.5833 - val_acc: 0.6960\n",
            "Epoch 10/15\n",
            "100/100 [==============================] - 77s 774ms/step - loss: 0.5807 - acc: 0.7105 - val_loss: 0.5639 - val_acc: 0.7270\n",
            "Epoch 11/15\n",
            "100/100 [==============================] - 78s 777ms/step - loss: 0.5835 - acc: 0.6960 - val_loss: 0.5550 - val_acc: 0.7150\n",
            "Epoch 12/15\n",
            "100/100 [==============================] - 78s 775ms/step - loss: 0.5782 - acc: 0.7030 - val_loss: 0.5565 - val_acc: 0.7080\n",
            "Epoch 13/15\n",
            "100/100 [==============================] - 78s 777ms/step - loss: 0.5524 - acc: 0.7295 - val_loss: 0.5578 - val_acc: 0.7040\n",
            "Epoch 14/15\n",
            "100/100 [==============================] - 78s 778ms/step - loss: 0.5637 - acc: 0.7175 - val_loss: 0.7121 - val_acc: 0.6130\n",
            "Epoch 15/15\n",
            "100/100 [==============================] - 78s 777ms/step - loss: 0.5642 - acc: 0.7170 - val_loss: 0.5690 - val_acc: 0.7230\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}