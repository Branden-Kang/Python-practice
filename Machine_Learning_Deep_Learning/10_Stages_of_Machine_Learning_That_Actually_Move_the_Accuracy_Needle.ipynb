{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK4cNFr6KhdN7Ii/EWT/XR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@Rohan_Dutt/10-stages-of-machine-learning-that-actually-move-the-accuracy-needle-not-the-toy-stuff-aac2c0e078c8)"
      ],
      "metadata": {
        "id": "ElcGTHWfrX2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 1: Precision Data Sanitization (Treat Your Dataset Like Production Code)"
      ],
      "metadata": {
        "id": "1USdArsIrbOP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a52fZjUnqtMZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# 1. Outlier Removal (IQR)\n",
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "# 2. Predictive Imputation\n",
        "target = \"target_column\"\n",
        "features = df.drop(columns=[target])\n",
        "missing_cols = features.columns[features.isnull().any()]\n",
        "for col in missing_cols:\n",
        "    not_null = features[features[col].notnull()]\n",
        "    is_null = features[features[col].isnull()]\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(not_null.drop(columns=[col]), not_null[col])\n",
        "    features.loc[is_null.index, col] = model.predict(is_null.drop(columns=[col]))\n",
        "\n",
        "# 3. Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 2: Intelligence-Grade Feature Engineering"
      ],
      "metadata": {
        "id": "C0urmZrsrmZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from category_encoders import TargetEncoder\n",
        "\n",
        "df = pd.read_csv(\"housing.csv\")\n",
        "\n",
        "# 1. Domain Transform\n",
        "df[\"log_price\"] = np.log1p(df[\"price\"])\n",
        "\n",
        "# 2. Interaction Feature\n",
        "df[\"price_per_sqft\"] = df[\"price\"] / df[\"sqft_living\"]\n",
        "\n",
        "# 3. Target Encoding (High Cardinality)\n",
        "encoder = TargetEncoder(cols=[\"zipcode\"], smoothing=10)\n",
        "df[\"zipcode_encoded\"] = encoder.fit_transform(df[\"zipcode\"], df[\"price\"])\n",
        "\n",
        "# 4. Polynomial Interactions (Controlled)\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "interaction_features = poly.fit_transform(df[[\"sqft_living\", \"bedrooms\"]])"
      ],
      "metadata": {
        "id": "hyZxgheHrhrf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 3: Strategic Data Splitting"
      ],
      "metadata": {
        "id": "dK06eGTjr2PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "df = pd.read_csv(\"healthcare.csv\")\n",
        "X = df.drop(columns=[\"target\", \"patient_id\"])\n",
        "y = df[\"target\"]\n",
        "\n",
        "# 1. Stratified Split (Classification)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 2. Time-Based Split (Forecasting)\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# 3. Leakage-Safe Pipeline\n",
        "pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", LogisticRegression(max_iter=1000))\n",
        "])\n",
        "pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "oWNwSLBErx1X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 4: Capital-Allocation Model Selection (Invest in Algorithms That Pay Off)"
      ],
      "metadata": {
        "id": "DMgF1_Xqr-og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "X = df.drop(columns=[\"target\"])\n",
        "y = df[\"target\"]\n",
        "models = {\n",
        "    \"Linear\": LogisticRegression(max_iter=2000),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=300),\n",
        "    \"XGBoost\": XGBClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        eval_metric=\"logloss\"\n",
        "    )\n",
        "}\n",
        "for name, model in models.items():\n",
        "    score = cross_val_score(model, X, y, cv=5, scoring=\"roc_auc\").mean()\n",
        "    print(f\"{name}: AUC={score:.3f}\")"
      ],
      "metadata": {
        "id": "oNRPYHNqr4j1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoML Shortcut: Bruteforce the Search Space"
      ],
      "metadata": {
        "id": "Fbqr_A87sIfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tpot import TPOTClassifier\n",
        "\n",
        "tpot = TPOTClassifier(\n",
        "    generations=5,\n",
        "    population_size=40,\n",
        "    scoring=\"roc_auc\",\n",
        "    verbosity=2,\n",
        "    random_state=42\n",
        ")\n",
        "tpot.fit(X, y)"
      ],
      "metadata": {
        "id": "NneB7gswsGR6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 5: High-Velocity Hyperparameter Optimization"
      ],
      "metadata": {
        "id": "--8MeVAmsSFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "        \"eval_metric\": \"logloss\"\n",
        "    }\n",
        "    model = XGBClassifier(**params, random_state=42)\n",
        "    return cross_val_score(model, X, y, cv=3, scoring=\"roc_auc\").mean()\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=50)\n",
        "print(\"Best AUC:\", study.best_value)\n",
        "print(\"Best Params:\", study.best_params)"
      ],
      "metadata": {
        "id": "483W0hJhsMAw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 6: Discipline-Driven Regularization"
      ],
      "metadata": {
        "id": "5It-08hNsXDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# 1. L1 vs L2 (Linear Models)\n",
        "l1_model = LogisticRegression(penalty=\"l1\", solver=\"saga\", C=0.5)\n",
        "l2_model = LogisticRegression(penalty=\"l2\", C=1.0)\n",
        "\n",
        "# 2. Tree Regularization\n",
        "xgb_model = XGBClassifier(\n",
        "    max_depth=4,\n",
        "    reg_alpha=0.1,   # L1\n",
        "    reg_lambda=1.5  # L2\n",
        ")\n",
        "\n",
        "# 3. Dropout + Early Stopping\n",
        "nn = Sequential([\n",
        "    Dense(128, activation=\"relu\"),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "nn.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "jvO9HYYdsUOt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 7: Portfolio-Grade Ensembling (Diversify Models, Control Risk)"
      ],
      "metadata": {
        "id": "6ZtYAUAvsc6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "base_models = [\n",
        "    (\"linear\", LogisticRegression(max_iter=2000)),\n",
        "    (\"rf\", RandomForestClassifier(n_estimators=300, max_depth=8)),\n",
        "    (\"xgb\", XGBClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=5,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        eval_metric=\"logloss\"\n",
        "    ))\n",
        "]\n",
        "stack = StackingClassifier(\n",
        "    estimators=base_models,\n",
        "    final_estimator=LogisticRegression(),\n",
        "    cv=5\n",
        ")\n",
        "stack.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "4s2XR0Kssa3h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 8: Forensic Model Interpretability (Trust, but Verify)"
      ],
      "metadata": {
        "id": "m6IqiUy1sgcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "\n",
        "\n",
        "# 1. SHAP (Tree-Based Models)\n",
        "explainer = shap.TreeExplainer(xgb_model)\n",
        "shap_values = explainer.shap_values(X_val)\n",
        "shap.summary_plot(shap_values, X_val)\n",
        "\n",
        "# 2. LIME (Local Explanation)\n",
        "lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train.values,\n",
        "    feature_names=X_train.columns,\n",
        "    class_names=[\"No\", \"Yes\"],\n",
        "    mode=\"classification\"\n",
        ")\n",
        "exp = lime_explainer.explain_instance(\n",
        "    X_val.iloc[0].values,\n",
        "    stack.predict_proba\n",
        ")\n",
        "\n",
        "# 3. Partial Dependence Plot\n",
        "PartialDependenceDisplay.from_estimator(\n",
        "    xgb_model,\n",
        "    X_val,\n",
        "    features=[\"age\", \"income\"]\n",
        ")"
      ],
      "metadata": {
        "id": "sRV6e6xsseyF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 9: Model Monitoring (Assume Failure, Detect Early)"
      ],
      "metadata": {
        "id": "ZvHr3Kr-skfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import ks_2samp\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 1. Data Drift (KS Test)\n",
        "def ks_drift(train_col, prod_col, alpha=0.05):\n",
        "    stat, p_value = ks_2samp(train_col, prod_col)\n",
        "    return p_value < alpha  # True = drift detected\n",
        "drift_detected = ks_drift(\n",
        "    X_train[\"age\"],\n",
        "    X_prod[\"age\"]\n",
        ")\n",
        "\n",
        "# 2. Concept Drift (Metric Tracking)\n",
        "prod_auc = roc_auc_score(y_prod, model.predict_proba(X_prod)[:, 1])\n",
        "# Compare vs. historical baseline\n",
        "if prod_auc < baseline_auc - 0.05:\n",
        "    alert = \"Concept drift suspected\"\n",
        "\n",
        "# 3. Shadow Deployment Check\n",
        "old_preds = old_model.predict_proba(X_prod)[:, 1]\n",
        "new_preds = new_model.predict_proba(X_prod)[:, 1]\n",
        "delta = np.mean(np.abs(new_preds - old_preds))"
      ],
      "metadata": {
        "id": "6mGp3k8LsihB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 10: Deployment (Ship Models That Survive Contact)"
      ],
      "metadata": {
        "id": "qmkUj5L2suh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Convert Model to ONNX\n",
        "import skl2onnx\n",
        "from skl2onnx import convert_sklearn\n",
        "from skl2onnx.common.data_types import FloatTensorType\n",
        "\n",
        "initial_type = [(\"input\", FloatTensorType([None, X.shape[1]]))]\n",
        "onnx_model = convert_sklearn(model, initial_types=initial_type)\n",
        "with open(\"model.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())\n",
        "\n",
        "# 2. FastAPI Inference Server\n",
        "from fastapi import FastAPI\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "app = FastAPI()\n",
        "session = ort.InferenceSession(\"model.onnx\")\n",
        "@app.post(\"/predict\")\n",
        "def predict(features: list):\n",
        "    inputs = np.array([features], dtype=np.float32)\n",
        "    preds = session.run(None, {\"input\": inputs})\n",
        "    return {\"prediction\": float(preds[0][0])}\n",
        "\n",
        "# 3. Dockerfile\n",
        "FROM python:3.10-slim\n",
        "COPY . /app\n",
        "WORKDIR /app\n",
        "RUN pip install fastapi uvicorn onnxruntime\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]"
      ],
      "metadata": {
        "id": "srlYAqpSsnHZ"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}
