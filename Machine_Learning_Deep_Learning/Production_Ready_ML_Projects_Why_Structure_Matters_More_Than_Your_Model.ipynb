{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPI3O6iHgtnvyS0hSQA+dqb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://pub.towardsai.net/production-ready-ml-projects-why-structure-matters-more-than-your-model-ace54f2351ff)"
      ],
      "metadata": {
        "id": "3TGlLdOKGRj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Why Structure Is Architecture, Not Organization\n",
        "\n",
        "```\n",
        "laptop/\n",
        "  ├── model_v1.pkl\n",
        "  ├── model_v2.pkl\n",
        "  ├── model_final.pkl\n",
        "  ├── model_final_final.pkl\n",
        "  ├── model_FINAL_USE_THIS.pkl\n",
        "  ├── notebook_experiment.ipynb\n",
        "  ├── notebook_experiment_v2.ipynb\n",
        "  ├── data.csv\n",
        "  ├── data_cleaned.csv\n",
        "  ├── data_cleaned_v2.csv\n",
        "  ├── utils.py (utility functions mixed everywhere)\n",
        "  └── README.txt (outdated, nobody reads it)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "UT7zelndbYvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. The Production-Ready Project Structure (Explained)\n",
        "```\n",
        "ml-project-example/\n",
        "├── config/\n",
        "│   ├── local.yaml          # Development settings\n",
        "│   └── prod.yaml           # Production settings\n",
        "├── data/\n",
        "│   ├── 01-raw/             # Original, never touch\n",
        "│   ├── 02-preprocessed/    # Cleaned data\n",
        "│   ├── 03-features/        # Engineered features\n",
        "│   └── 04-predictions/     # Model outputs\n",
        "├── entrypoint/\n",
        "│   ├── train.py            # Training script\n",
        "│   └── inference.py        # Prediction script\n",
        "├── notebooks/\n",
        "│   ├── 01-eda.ipynb        # Exploration only\n",
        "│   └── 02-baseline.ipynb   # Baseline experiments\n",
        "├── src/\n",
        "│   ├── pipelines/          # Data + training pipelines\n",
        "│   │   ├── __init__.py\n",
        "│   │   ├── feature_eng_pipeline.py\n",
        "│   │   ├── training_pipeline.py\n",
        "│   │   └── inference_pipeline.py\n",
        "│   └── utils.py            # Shared utilities\n",
        "├── tests/\n",
        "│   ├── __init__.py\n",
        "│   ├── test_training.py    # Training tests\n",
        "│   └── test_pipelines.py   # Pipeline tests\n",
        "├── .gitlab-ci.yml          # CI/CD pipeline\n",
        "├── Dockerfile              # Containerization\n",
        "├── docker-compose.yml      # Local docker setup\n",
        "├── env.yaml                # Environment variables\n",
        "├── env-dev.yaml            # Dev environment\n",
        "├── Makefile                # Common commands\n",
        "├── README.md               # Documentation\n",
        "├── requirements-dev.txt    # Dev dependencies\n",
        "└── requirements-prod.txt   # Production dependencies\n",
        "```"
      ],
      "metadata": {
        "id": "kPKBFQe6b_8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Each Section Explained (The Architecture)\n",
        "## 1. Config/ — Environment Separation"
      ],
      "metadata": {
        "id": "gJmBhiG_cITX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# config/local.yaml (development)\n",
        "data_path: ./data/\n",
        "model_version: v1\n",
        "batch_size: 32\n",
        "learning_rate: 0.001\n",
        "log_level: DEBUG\n",
        "\n",
        "# config/prod.yaml (production)\n",
        "data_path: s3://ml-bucket/data/\n",
        "model_version: v1.2.3\n",
        "batch_size: 128\n",
        "learning_rate: 0.0001\n",
        "log_level: WARNING\n",
        "```"
      ],
      "metadata": {
        "id": "DS7umipbcQ6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data/ — Data Versioning & Lineage\n",
        "```\n",
        "data/\n",
        "├── 01-raw/\n",
        "│   ├── raw_data_2025_01_15.csv    # Original from source\n",
        "│   └── metadata.json              # Data lineage\n",
        "├── 02-preprocessed/\n",
        "│   ├── cleaned_data_v1.csv        # After missing value handling\n",
        "│   └── pipeline_config.yaml       # What was done\n",
        "├── 03-features/\n",
        "│   ├── features_v1.csv            # Engineered features\n",
        "│   └── feature_list.txt           # Which features created\n",
        "└── 04-predictions/\n",
        "    └── predictions_2025_01_15.csv # Model outputs\n",
        "```"
      ],
      "metadata": {
        "id": "VOi9HfDDcmoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Entrypoint/ — Clear Entry Points"
      ],
      "metadata": {
        "id": "a94E3nk7cqxm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T2a9UNlAGMOG"
      },
      "outputs": [],
      "source": [
        "# entrypoint/train.py\n",
        "import yaml\n",
        "from src.pipelines.training_pipeline import TrainingPipeline\n",
        "\n",
        "config = yaml.safe_load(open('config/prod.yaml'))\n",
        "pipeline = TrainingPipeline(config)\n",
        "pipeline.run()\n",
        "\n",
        "# entrypoint/inference.py\n",
        "import yaml\n",
        "from src.pipelines.inference_pipeline import InferencePipeline\n",
        "config = yaml.safe_load(open('config/prod.yaml'))\n",
        "pipeline = InferencePipeline(config)\n",
        "predictions = pipeline.predict(new_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Notebooks/ — Exploration Stays Exploration\n",
        "```\n",
        "notebooks/\n",
        "├── 01-eda.ipynb        # Data exploration, visualizations, insights\n",
        "└── 02-baseline.ipynb   # Quick baseline experiments\n",
        "```"
      ],
      "metadata": {
        "id": "M4e7WXWAcxA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Src/Pipelines/ — Production Logic"
      ],
      "metadata": {
        "id": "zBCRkLWZc1CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src/pipelines/training_pipeline.py\n",
        "class TrainingPipeline:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def load_data(self):\n",
        "        # Load from data/01-raw/\n",
        "        pass\n",
        "\n",
        "    def preprocess(self):\n",
        "        # Save to data/02-preprocessed/\n",
        "        pass\n",
        "\n",
        "    def engineer_features(self):\n",
        "        # Save to data/03-features/\n",
        "        pass\n",
        "\n",
        "    def train_model(self):\n",
        "        # Train and save model\n",
        "        pass\n",
        "\n",
        "    def validate_model(self):\n",
        "        # Test on holdout set\n",
        "        pass\n",
        "\n",
        "    def run(self):\n",
        "        self.load_data()\n",
        "        self.preprocess()\n",
        "        self.engineer_features()\n",
        "        self.train_model()\n",
        "        self.validate_model()"
      ],
      "metadata": {
        "id": "iPyZXouGcs4D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Tests/ — Protection Against Silent Failures"
      ],
      "metadata": {
        "id": "noU37Viec5xI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tests/test_training.py\n",
        "def test_data_loading():\n",
        "    pipeline = TrainingPipeline(config)\n",
        "    data = pipeline.load_data()\n",
        "    assert len(data) > 0\n",
        "    assert data.isnull().sum() == 0  # No missing values after loading\n",
        "def test_preprocessing():\n",
        "    pipeline = TrainingPipeline(config)\n",
        "    data = pipeline.load_data()\n",
        "    processed = pipeline.preprocess(data)\n",
        "    assert processed.shape[1] > 0  # Has features\n",
        "def test_model_training():\n",
        "    pipeline = TrainingPipeline(config)\n",
        "    model = pipeline.train_model()\n",
        "    predictions = model.predict(X_test)\n",
        "    assert len(predictions) == len(X_test)\n",
        "    assert model.accuracy > 0.7  # Reasonable baseline"
      ],
      "metadata": {
        "id": "0c8OA4ogc3_R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$# 7. Docker/ — Reproducibility & Deployment\n",
        "```\n",
        "# Dockerfile\n",
        "FROM python:3.9-slim\n",
        "WORKDIR /app\n",
        "COPY requirements-prod.txt .\n",
        "RUN pip install --no-cache-dir -r requirements-prod.txt\n",
        "COPY . .\n",
        "ENTRYPOINT [\"python\", \"entrypoint/train.py\"]\n",
        "```"
      ],
      "metadata": {
        "id": "5eQBPANyc9Pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Requirements Files — Dependency Management\n",
        "```\n",
        "# requirements-dev.txt\n",
        "jupyter==1.0.0\n",
        "matplotlib==3.5.0\n",
        "scikit-learn==1.0.0\n",
        "pytest==7.0.0\n",
        "\n",
        "# requirements-dev.txt\n",
        "jupyter==1.0.0\n",
        "matplotlib==3.5.0\n",
        "scikit-learn==1.0.0\n",
        "pytest==7.0.0\n",
        "```"
      ],
      "metadata": {
        "id": "q9Hhc6wDdA4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. README.md — Documentation\n",
        "```\n",
        "# ML Project: Customer Churn Prediction\n",
        "\n",
        "## Project Overview\n",
        "Predicts which customers will churn using historical purchase behavior.\n",
        "\n",
        "## Project Structure\n",
        "- `data/`: Data at different pipeline stages\n",
        "- `src/`: Production code and pipelines\n",
        "- `notebooks/`: EDA and baseline experiments\n",
        "- `entrypoint/`: Main training and inference scripts\n",
        "\n",
        "## Setup\n",
        "1. `pip install -r requirements-dev.txt`\n",
        "2. `docker-compose up` (for local development)\n",
        "3. `python entrypoint/train.py` (to train)\n",
        "\n",
        "## Data\n",
        "- Source: CRM database\n",
        "- Size: 1M records\n",
        "- Training/test split: 80/20\n",
        "- Features: 45 (see feature_list.txt)\n",
        "\n",
        "## Model\n",
        "- Algorithm: Random Forest\n",
        "- Accuracy: 0.85 (on test set)\n",
        "- Deployed: 2025-01-15\n",
        "\n",
        "## Monitoring\n",
        "- Run tests: `pytest tests/`\n",
        "- Check metrics: `python entrypoint/inference.py --metrics`\n",
        "```"
      ],
      "metadata": {
        "id": "Q59Ir6cndFN-"
      }
    }
  ]
}
