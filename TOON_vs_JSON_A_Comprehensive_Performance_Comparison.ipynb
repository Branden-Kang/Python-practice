{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMYlnTw/9B5r16tGmrLdDs"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://pub.towardsai.net/toon-vs-json-a-comprehensive-performance-comparison-446a2fb82f20)"
      ],
      "metadata": {
        "id": "f99UnQUhvlUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Setup\n",
        "\n",
        "```\n",
        "openai>=1.0.0\n",
        "pandas>=2.0.0\n",
        "matplotlib>=3.7.0\n",
        "seaborn>=0.12.0\n",
        "python-dotenv>=1.0.0\n",
        "git+https://github.com/toon-format/toon-python.git\n",
        "faker\n",
        "```\n",
        "\n",
        "```\n",
        "# .env file\n",
        "OPENAI_API_KEY = \"YOUR_KEY_HERE\"\n",
        "```"
      ],
      "metadata": {
        "id": "QCsGDTF4vvde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "Zd9ZdVvTv3TJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS49NKl3wDJb",
        "outputId": "22e9fe7a-dd79-4046-91ae-bd900f6a5345"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-38.2.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from faker) (2025.2)\n",
            "Downloading faker-38.2.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-38.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqvmCZ2tvjIK",
        "outputId": "74bb2685-37ef-4972-b198-1c1a56b33585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 1, 'name': 'Allison Hill', 'email': 'allison_hill@example.com', 'department': 'Engineering', 'salary': 59592, 'yearsExperience': 21, 'active': True}, {'id': 2, 'name': 'Noah Rhodes', 'email': 'noah_rhodes@example.com', 'department': 'Sales', 'salary': 77098, 'yearsExperience': 9, 'active': True}, {'id': 3, 'name': 'Angie Henderson', 'email': 'angie_henderson@example.com', 'department': 'Marketing', 'salary': 58434, 'yearsExperience': 24, 'active': True}, {'id': 4, 'name': 'Daniel Wagner', 'email': 'daniel_wagner@example.com', 'department': 'HR', 'salary': 56395, 'yearsExperience': 18, 'active': True}, {'id': 5, 'name': 'Cristian Santos', 'email': 'cristian_santos@example.com', 'department': 'Operations', 'salary': 48905, 'yearsExperience': 2, 'active': True}]\n"
          ]
        }
      ],
      "source": [
        "from faker import Faker\n",
        "import random\n",
        "import re\n",
        "\n",
        "# List of departments to assign employees to\n",
        "DEPARTMENTS = ['Engineering', 'Sales', 'Marketing', 'HR', 'Operations', 'Finance']\n",
        "\n",
        "def slugify(name: str) -> str:\n",
        "    \"\"\"Convert a name to lowercase, alphanumeric slug suitable for email.\"\"\"\n",
        "    name = name.lower()\n",
        "    name = re.sub(r\"[^a-z0-9]+\", \"\", name)  # remove accents, punctuation, etc.\n",
        "    return name\n",
        "\n",
        "def generate_employees(count: int, seed: int = None):\n",
        "    \"\"\"\n",
        "    Generate synthetic employee data with consistent randomization.\n",
        "\n",
        "    Args:\n",
        "        count: Number of employees to generate\n",
        "        seed: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with 'employees' key containing list of employee records\n",
        "    \"\"\"\n",
        "    # Apply seed for reproducibility\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "        faker = Faker()\n",
        "        faker.seed_instance(seed)\n",
        "    else:\n",
        "        faker = Faker()\n",
        "\n",
        "    employees = []\n",
        "\n",
        "    for i in range(count):\n",
        "        # Generate random name using Faker\n",
        "        full_name = faker.name()\n",
        "        parts = full_name.split()\n",
        "\n",
        "        # Create email from name parts\n",
        "        first = slugify(parts[0])\n",
        "        last  = slugify(parts[-1])  # handles middle names gracefully\n",
        "\n",
        "        email = f\"{first}_{last}@example.com\"\n",
        "\n",
        "        # Random years of experience\n",
        "        years_exp = random.randint(1, 25)\n",
        "\n",
        "        # Construct employee record\n",
        "        employee = {\n",
        "            \"id\": i + 1,\n",
        "            \"name\": full_name,\n",
        "            \"email\": email,\n",
        "            \"department\": DEPARTMENTS[i % len(DEPARTMENTS)],  # Round-robin assignment\n",
        "            \"salary\": random.randint(45000, 150000),\n",
        "            \"yearsExperience\": years_exp,\n",
        "            \"active\": random.random() < 0.8,  # 80% active\n",
        "        }\n",
        "\n",
        "        employees.append(employee)\n",
        "\n",
        "    return {\"employees\": employees}\n",
        "\n",
        "\n",
        "# Generate 100 employees with seed for reproducibility\n",
        "\n",
        "dataset = generate_employees(100, seed=42)\n",
        "print(dataset[\"employees\"][:5])  # preview first 5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Generation"
      ],
      "metadata": {
        "id": "SjpQI4kXv8mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees = dataset[\"employees\"]\n",
        "\n",
        "questions = []\n",
        "\n",
        "def q(id, prompt, gt, qtype, ans):\n",
        "    \"\"\"Helper function to create a question dictionary.\"\"\"\n",
        "    return {\n",
        "        \"id\": id,\n",
        "        \"prompt\": prompt,\n",
        "        \"groundTruth\": gt,\n",
        "        \"type\": qtype,\n",
        "        \"dataset\": \"tabular\",\n",
        "        \"answerType\": ans\n",
        "    }\n",
        "\n",
        "# ---- FIELD RETRIEVAL (30) ----\n",
        "# Ask about specific employee attributes\n",
        "for i, emp in enumerate(employees[:10]):  # 10 employees * 3 fields = 30\n",
        "    questions.append(q(f\"fr{i*3+1}\", f\"What is the salary of {emp['name']}?\", str(emp[\"salary\"]), \"field-retrieval\", \"integer\"))\n",
        "    questions.append(q(f\"fr{i*3+2}\", f\"What department does {emp['name']} work in?\", emp[\"department\"], \"field-retrieval\", \"string\"))\n",
        "    questions.append(q(f\"fr{i*3+3}\", f\"What is the email address of {emp['name']}?\", emp[\"email\"], \"field-retrieval\", \"string\"))\n",
        "\n",
        "# ---- AGGREGATION (30) ----\n",
        "# 1) Count by department (6 questions)\n",
        "dept_counts = {d: sum(1 for e in employees if e[\"department\"] == d) for d in DEPARTMENTS}\n",
        "for idx, (d,c) in enumerate(dept_counts.items()):\n",
        "    questions.append(q(f\"ag{idx+1}\", f\"How many employees work in {d}?\", str(c), \"aggregation\", \"integer\"))\n",
        "\n",
        "# 2) Salary thresholds (10 questions)\n",
        "thresholds = [50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000]\n",
        "for i, t in enumerate(thresholds):\n",
        "    c = sum(1 for e in employees if e[\"salary\"] > t)\n",
        "    questions.append(q(f\"ag{7+i}\", f\"How many employees have a salary greater than {t}?\", str(c), \"aggregation\", \"integer\"))\n",
        "\n",
        "# 3) General statistics (4 questions)\n",
        "total = len(employees)\n",
        "avg_sal = round(sum(e[\"salary\"] for e in employees)/total)\n",
        "active = sum(e[\"active\"] for e in employees)\n",
        "inactive = total - active\n",
        "\n",
        "questions.append(q(\"ag17\", \"How many employees are in the dataset?\", str(total), \"aggregation\", \"integer\"))\n",
        "questions.append(q(\"ag18\", \"What is the average salary across all employees?\", str(avg_sal), \"aggregation\", \"integer\"))\n",
        "questions.append(q(\"ag19\", \"How many employees are active?\", str(active), \"aggregation\", \"integer\"))\n",
        "questions.append(q(\"ag20\", \"How many employees are inactive?\", str(inactive), \"aggregation\", \"integer\"))\n",
        "\n",
        "# 4) Experience thresholds (10 questions)\n",
        "exp_levels = [2,5,8,10,12,15,18,20,22,24]\n",
        "for i, t in enumerate(exp_levels):\n",
        "    c = sum(1 for e in employees if e[\"yearsExperience\"] > t)\n",
        "    questions.append(q(f\"ag21{i}\", f\"How many employees have more than {t} years of experience?\", str(c), \"aggregation\", \"integer\"))\n",
        "\n",
        "# Trim aggregation to exactly 30\n",
        "agg_questions = [q for q in questions if q[\"type\"]==\"aggregation\"][:30]\n",
        "\n",
        "# ---- FILTERING (30) ----\n",
        "filter_questions = []\n",
        "\n",
        "# 1) Department + salary filter (6 questions)\n",
        "dept = DEPARTMENTS\n",
        "for i,d in enumerate(dept):\n",
        "    c = sum(1 for e in employees if e[\"department\"]==d and e[\"salary\"]>90000)\n",
        "    filter_questions.append(q(f\"ft{i+1}\", f\"How many employees in {d} have a salary greater than 90000?\", str(c), \"filtering\", \"integer\"))\n",
        "\n",
        "# 2) Active + experience filter (10 questions)\n",
        "for idx,t in enumerate(exp_levels):\n",
        "    c = sum(1 for e in employees if e[\"active\"] and e[\"yearsExperience\"] > t)\n",
        "    filter_questions.append(q(f\"ft10{idx}\", f\"How many active employees have more than {t} years of experience?\", str(c), \"filtering\", \"integer\"))\n",
        "\n",
        "# 3) Department + active filter (6 questions)\n",
        "for i,d in enumerate(dept):\n",
        "    c = sum(1 for e in employees if e[\"department\"]==d and e[\"active\"])\n",
        "    filter_questions.append(q(f\"ft20{i}\", f\"How many active employees work in {d}?\", str(c), \"filtering\", \"integer\"))\n",
        "\n",
        "# 4) Department + experience filter (8 questions)\n",
        "for i,t in enumerate(exp_levels[:8]):\n",
        "    c = sum(1 for e in employees if e[\"yearsExperience\"] > t and e[\"department\"]==DEPARTMENTS[i % 6])\n",
        "    filter_questions.append(q(f\"ft30{i}\", f\"How many employees in {DEPARTMENTS[i % 6]} have more than {t} years of experience?\", str(c), \"filtering\", \"integer\"))\n",
        "\n",
        "# Trim filter to 30\n",
        "filter_questions = filter_questions[:30]\n",
        "\n",
        "# Combine all question types: 30 field retrieval + 30 aggregation + 30 filtering = 90 total\n",
        "final_questions = questions[:30] + agg_questions + filter_questions\n",
        "\n",
        "# printing first 5 questions\n",
        "len(final_questions), final_questions[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DEZUr6ev6Q-",
        "outputId": "12317dd3-f85c-4534-d251-2d02fd6c972c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90,\n",
              " [{'id': 'fr1',\n",
              "   'prompt': 'What is the salary of Allison Hill?',\n",
              "   'groundTruth': '59592',\n",
              "   'type': 'field-retrieval',\n",
              "   'dataset': 'tabular',\n",
              "   'answerType': 'integer'},\n",
              "  {'id': 'fr2',\n",
              "   'prompt': 'What department does Allison Hill work in?',\n",
              "   'groundTruth': 'Engineering',\n",
              "   'type': 'field-retrieval',\n",
              "   'dataset': 'tabular',\n",
              "   'answerType': 'string'},\n",
              "  {'id': 'fr3',\n",
              "   'prompt': 'What is the email address of Allison Hill?',\n",
              "   'groundTruth': 'allison_hill@example.com',\n",
              "   'type': 'field-retrieval',\n",
              "   'dataset': 'tabular',\n",
              "   'answerType': 'string'},\n",
              "  {'id': 'fr4',\n",
              "   'prompt': 'What is the salary of Noah Rhodes?',\n",
              "   'groundTruth': '77098',\n",
              "   'type': 'field-retrieval',\n",
              "   'dataset': 'tabular',\n",
              "   'answerType': 'integer'},\n",
              "  {'id': 'fr5',\n",
              "   'prompt': 'What department does Noah Rhodes work in?',\n",
              "   'groundTruth': 'Sales',\n",
              "   'type': 'field-retrieval',\n",
              "   'dataset': 'tabular',\n",
              "   'answerType': 'string'}])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "scHi8-JIwMXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Required Libraries\n"
      ],
      "metadata": {
        "id": "m2xLq3ldwT7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from toon_format import encode, decode\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load OpenAI API key from environment variables\n",
        "load_dotenv()\n",
        "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "PhrHLXI_wKPd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert Employee Data to Toon Format"
      ],
      "metadata": {
        "id": "VrDNDu9gwWjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert dataset from JSON to TOON format using the encode function\n",
        "employee_jsoan = dataset  # Already have this as dict\n",
        "# employee_toon = encode(employee_json)\n",
        "\n",
        "# # Display sample of both formats for comparison\n",
        "# print(\"JSON Format (first 200 chars):\")\n",
        "# print(json.dumps(employee_json, indent=2)[:200])\n",
        "# print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "# print(\"TOON Format (first 200 chars):\")\n",
        "# print(employee_toon[:200])\n",
        "# print(f\"\\n\\nTOON format type: {type(employee_toon)}\")"
      ],
      "metadata": {
        "id": "oCwAUOD5wN30"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining LLM Call and Evaluation Functions"
      ],
      "metadata": {
        "id": "XUD7JiAZwYPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def call_llm(data_str, question, data_format=\"JSON\", model_name=\"gpt-5-nano\"):\n",
        "    \"\"\"\n",
        "    Query GPT model with employee data and a question, returning response + metadata.\n",
        "\n",
        "    Args:\n",
        "        data_str: Employee data as string (JSON or TOON format)\n",
        "        question: Question to ask about the data\n",
        "        data_format: \"JSON\" or \"TOON\" for context\n",
        "        model_name: LLM model name to use\n",
        "\n",
        "    Returns:\n",
        "        Dict with:\n",
        "        {\n",
        "            \"response\": model output text,\n",
        "            \"input_tokens\": prompt tokens used,\n",
        "            \"output_tokens\": completion tokens used,\n",
        "            \"total_tokens\": total tokens used,\n",
        "            \"time_taken_seconds\": API call duration\n",
        "        }\n",
        "    \"\"\"\n",
        "    # System prompt guides the model on how to interpret the data\n",
        "    system_prompt = f\"\"\"You are a helpful assistant that answers questions about employee data.\n",
        "The data is provided in {data_format} format.\n",
        "Analyze the data carefully and provide concise, accurate answers.\n",
        "For numeric answers, provide ONLY the number without any additional text or explanation.\n",
        "For text answers, provide ONLY the requested information.\"\"\"\n",
        "\n",
        "    # User prompt contains the data and question\n",
        "    user_prompt = f\"\"\"Employee Data ({data_format} format):\n",
        "{data_str}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        # Call OpenAI API\n",
        "        response_obj = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "\n",
        "        )\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # Extract response content\n",
        "        response_text = response_obj.choices[0].message.content.strip()\n",
        "\n",
        "        # Extract token usage metadata\n",
        "        usage = response_obj.usage\n",
        "        input_tokens = usage.prompt_tokens if hasattr(usage, \"prompt_tokens\") else None\n",
        "        output_tokens = usage.completion_tokens if hasattr(usage, \"completion_tokens\") else None\n",
        "        total_tokens = usage.total_tokens if hasattr(usage, \"total_tokens\") else None\n",
        "\n",
        "        return {\n",
        "            \"response\": response_text,\n",
        "            \"input_tokens\": input_tokens,\n",
        "            \"output_tokens\": output_tokens,\n",
        "            \"total_tokens\": total_tokens,\n",
        "            \"time_taken_seconds\": elapsed\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        elapsed = time.time() - start_time\n",
        "        return {\n",
        "            \"response\": f\"ERROR: {str(e)}\",\n",
        "            \"input_tokens\": None,\n",
        "            \"output_tokens\": None,\n",
        "            \"total_tokens\": None,\n",
        "            \"time_taken_seconds\": elapsed\n",
        "        }"
      ],
      "metadata": {
        "id": "QqsTHhjNwQEX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_all_queries(employee_json, questions, data_format=\"JSON\", model_name=\"gpt-5-nano\"):\n",
        "    \"\"\"\n",
        "    Runs the LLM on all questions using call_llm() and stores model outputs + metadata.\n",
        "\n",
        "    Args:\n",
        "        data_str: Employee data as string (JSON or TOON format)\n",
        "        questions: List of question dictionaries\n",
        "        data_format: \"JSON\" or \"TOON\"\n",
        "        model_name: Model to use for queries\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries with full metadata for each question.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüöÄ Running queries on {len(questions)} questions...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    for i, q in enumerate(questions, 1):\n",
        "        # Call the LLM for this question\n",
        "        result = call_llm(data_str, q[\"prompt\"], data_format, model_name=model_name)\n",
        "\n",
        "        # Store results with metadata\n",
        "        outputs.append({\n",
        "            \"id\": q[\"id\"],\n",
        "            \"question\": q[\"prompt\"],\n",
        "            \"type\": q[\"type\"],\n",
        "            \"ground_truth\": q[\"groundTruth\"],\n",
        "            \"model_response\": result[\"response\"],\n",
        "            \"input_tokens\": result[\"input_tokens\"],\n",
        "            \"output_tokens\": result[\"output_tokens\"],\n",
        "            \"total_tokens\": result[\"total_tokens\"],\n",
        "            \"time_taken_seconds\": result[\"time_taken_seconds\"]\n",
        "        })\n",
        "\n",
        "        # Progress indicator every 10 questions\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Progress: {i}/{len(questions)} completed...\")\n",
        "\n",
        "    print(\"\\n‚úÖ Querying complete!\")\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "CdLVgSvxwcK3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_results(results, model_name=\"gpt-5-nano\"):\n",
        "    \"\"\"\n",
        "    Evaluate correctness of previously generated model outputs and aggregate token usage.\n",
        "\n",
        "    Args:\n",
        "        results: List of question results from run_all_queries()\n",
        "        model_name: Name of the model used for querying\n",
        "\n",
        "    Returns:\n",
        "        Summary statistics + accuracy breakdown + token usage\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = len(results)\n",
        "    total_input_tokens = 0\n",
        "    total_output_tokens = 0\n",
        "\n",
        "    # Evaluate each result by comparing response to ground truth\n",
        "    for r in results:\n",
        "        r[\"correct\"] = (\n",
        "            r[\"model_response\"].lower().strip() ==\n",
        "            r[\"ground_truth\"].lower().strip()\n",
        "        )\n",
        "        if r[\"correct\"]:\n",
        "            correct += 1\n",
        "\n",
        "        # Aggregate tokens (handle None safely)\n",
        "        total_input_tokens += r.get(\"input_tokens\", 0) or 0\n",
        "        total_output_tokens += r.get(\"output_tokens\", 0) or 0\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    accuracy = (correct / total) * 100\n",
        "\n",
        "    # Compute stats by question type\n",
        "    type_stats = {}\n",
        "    for qtype in [\"field-retrieval\", \"aggregation\", \"filtering\"]:\n",
        "        type_subset = [r for r in results if r[\"type\"] == qtype]\n",
        "        type_correct = sum(1 for r in type_subset if r[\"correct\"])\n",
        "        type_total = len(type_subset)\n",
        "        type_stats[qtype] = {\n",
        "            \"correct\": type_correct,\n",
        "            \"total\": type_total,\n",
        "            \"accuracy\": (type_correct / type_total * 100) if type_total else 0\n",
        "        }\n",
        "\n",
        "    # Display summary\n",
        "    print(\"\\nüìä Evaluation Results\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Overall Accuracy: {correct}/{total} ({accuracy:.2f}%)\")\n",
        "    print(f\"Total Input Tokens: {total_input_tokens}\")\n",
        "    print(f\"Total Output Tokens: {total_output_tokens}\")\n",
        "\n",
        "    return {\n",
        "        \"model_name\": model_name,\n",
        "        \"overall_accuracy\": accuracy,\n",
        "        \"correct\": correct,\n",
        "        \"total\": total,\n",
        "        \"type_stats\": type_stats,\n",
        "        \"total_input_tokens\": total_input_tokens,\n",
        "        \"total_output_tokens\": total_output_tokens,\n",
        "        \"results\": results\n",
        "    }"
      ],
      "metadata": {
        "id": "CZN5mABGweDB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Evaluation for JSON format"
      ],
      "metadata": {
        "id": "tRVH_OrpwivF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run all 90 questions against the JSON format\n",
        "# This will make 90 API calls and may take a few minutes\n",
        "outputs_json = run_all_queries(employee_json, final_questions, data_format=\"JSON\")\n",
        "outputs_json"
      ],
      "metadata": {
        "id": "MS4uvNYfwgUZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert outputs to DataFrame and save as CSV\n",
        "import pandas as pd\n",
        "outputs_df = pd.DataFrame(outputs_json)\n",
        "outputs_df.to_csv('json_format_results.csv', index=False)\n",
        "print(f\"‚úÖ Saved JSON format results to 'json_format_results.csv'\")\n",
        "print(f\"   Shape: {outputs_df.shape}\")\n",
        "print(f\"\\nPreview:\")\n",
        "print(outputs_df.head())"
      ],
      "metadata": {
        "id": "vDGXtH98wkOo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Evaluation for TOON format\n"
      ],
      "metadata": {
        "id": "LVYWYI32wnz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run all 90 questions against the TOON format\n",
        "# This will make another 90 API calls and may take a few minutes\n",
        "outputs_toon = run_all_queries(employee_toon, final_questions, data_format=\"TOON\")\n",
        "outputs_toon"
      ],
      "metadata": {
        "id": "YYrWSnkiwmJh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Data"
      ],
      "metadata": {
        "id": "_tUY8lV-wqvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert outputs to DataFrame and save as CSV\n",
        "import pandas as pd\n",
        "outputs_df_toon = pd.DataFrame(outputs_toon)\n",
        "outputs_df_toon.to_csv('toon_format_results.csv', index=False)\n",
        "print(f\"‚úÖ Saved JSON format results to 'toon_format_results.csv'\")\n",
        "print(f\"   Shape: {outputs_df_toon.shape}\")\n",
        "print(f\"\\nPreview:\")\n",
        "outputs_df_toon.head()"
      ],
      "metadata": {
        "id": "9BoJNa8iwpUx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the TOON format results\n",
        "toon_results = evaluate_results(outputs_toon)"
      ],
      "metadata": {
        "id": "YxO2zW0HwtEf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare Results"
      ],
      "metadata": {
        "id": "D5v3gFU5wvov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create comprehensive comparison of JSON vs TOON formats\n",
        "print(\"=\"*70)\n",
        "print(\" \"*20 + \"üìä COMPREHENSIVE COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ===== CALCULATE TOKEN COUNTS FROM DATAFRAMES =====\n",
        "# Use average input tokens from the raw data (each row has input_tokens)\n",
        "json_tokens = int(outputs_df_json['input_tokens'].mean())\n",
        "toon_tokens = int(outputs_df_toon['input_tokens'].mean())\n",
        "token_reduction = json_tokens - toon_tokens\n",
        "reduction_percentage = (token_reduction / json_tokens) * 100\n",
        "\n",
        "# ===== TOKEN COUNT COMPARISON =====\n",
        "print(\"\\nüî¢ TOKEN COUNT COMPARISON:\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Format':<15} {'Tokens':<15} {'Reduction':<20} {'Cost/Call'}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'JSON':<15} {json_tokens:>10,}      {'-':<20} ${(json_tokens/1_000_000)*0.150:.6f}\")\n",
        "print(f\"{'TOON':<15} {toon_tokens:>10,}      {token_reduction:>6,} ({reduction_percentage:>5.2f}%)     ${(toon_tokens/1_000_000)*0.150:.6f}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# ===== CALCULATE ACCURACY FROM DATAFRAMES =====\n",
        "# Overall accuracy\n",
        "json_overall_acc = (outputs_df_json['correct'].sum() / len(outputs_df_json)) * 100\n",
        "toon_overall_acc = (outputs_df_toon['correct'].sum() / len(outputs_df_toon)) * 100\n",
        "\n",
        "# Accuracy by question type\n",
        "json_fr_acc = (outputs_df_json[outputs_df_json['type'] == 'field-retrieval']['correct'].sum() /\n",
        "               len(outputs_df_json[outputs_df_json['type'] == 'field-retrieval'])) * 100\n",
        "json_ag_acc = (outputs_df_json[outputs_df_json['type'] == 'aggregation']['correct'].sum() /\n",
        "               len(outputs_df_json[outputs_df_json['type'] == 'aggregation'])) * 100\n",
        "json_ft_acc = (outputs_df_json[outputs_df_json['type'] == 'filtering']['correct'].sum() /\n",
        "               len(outputs_df_json[outputs_df_json['type'] == 'filtering'])) * 100\n",
        "\n",
        "toon_fr_acc = (outputs_df_toon[outputs_df_toon['type'] == 'field-retrieval']['correct'].sum() /\n",
        "               len(outputs_df_toon[outputs_df_toon['type'] == 'field-retrieval'])) * 100\n",
        "toon_ag_acc = (outputs_df_toon[outputs_df_toon['type'] == 'aggregation']['correct'].sum() /\n",
        "               len(outputs_df_toon[outputs_df_toon['type'] == 'aggregation'])) * 100\n",
        "toon_ft_acc = (outputs_df_toon[outputs_df_toon['type'] == 'filtering']['correct'].sum() /\n",
        "               len(outputs_df_toon[outputs_df_toon['type'] == 'filtering'])) * 100\n",
        "\n",
        "# ===== ACCURACY COMPARISON =====\n",
        "print(\"\\nüéØ ACCURACY COMPARISON:\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Format':<15} {'Overall':<15} {'Field Retrieval':<20} {'Aggregation':<15} {'Filtering'}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# JSON accuracy metrics\n",
        "print(f\"{'JSON':<15} {json_overall_acc:>6.2f}%        {json_fr_acc:>6.2f}%               {json_ag_acc:>6.2f}%          {json_ft_acc:>6.2f}%\")\n",
        "\n",
        "# TOON accuracy metrics\n",
        "print(f\"{'TOON':<15} {toon_overall_acc:>6.2f}%        {toon_fr_acc:>6.2f}%               {toon_ag_acc:>6.2f}%          {toon_ft_acc:>6.2f}%\")\n",
        "\n",
        "print(\"-\"*70)\n",
        "\n",
        "# ===== DETAILED STATS BY CATEGORY =====\n",
        "print(\"\\nüìà DETAILED ACCURACY BY QUESTION TYPE:\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "comparison_data = []\n",
        "for qtype, qtype_label in [(\"field-retrieval\", \"Field Retrieval\"),\n",
        "                            (\"aggregation\", \"Aggregation\"),\n",
        "                            (\"filtering\", \"Filtering\")]:\n",
        "    json_subset = outputs_df_json[outputs_df_json['type'] == qtype]\n",
        "    toon_subset = outputs_df_toon[outputs_df_toon['type'] == qtype]\n",
        "\n",
        "    json_correct = json_subset['correct'].sum()\n",
        "    json_total = len(json_subset)\n",
        "    json_acc = (json_correct / json_total) * 100 if json_total > 0 else 0\n",
        "\n",
        "    toon_correct = toon_subset['correct'].sum()\n",
        "    toon_total = len(toon_subset)\n",
        "    toon_acc = (toon_correct / toon_total) * 100 if toon_total > 0 else 0\n",
        "\n",
        "    comparison_data.append({\n",
        "        \"Question Type\": qtype_label,\n",
        "        \"JSON Accuracy\": f\"{json_correct}/{json_total} ({json_acc:.2f}%)\",\n",
        "        \"TOON Accuracy\": f\"{toon_correct}/{toon_total} ({toon_acc:.2f}%)\",\n",
        "        \"Difference\": f\"{toon_acc - json_acc:+.2f}%\"\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# ===== KEY FINDINGS SUMMARY =====\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\\nüí° KEY FINDINGS:\")\n",
        "print(f\"   ‚Ä¢ Token reduction: {reduction_percentage:.2f}% ({token_reduction:,} tokens saved)\")\n",
        "print(f\"   ‚Ä¢ JSON accuracy: {json_overall_acc:.2f}%\")\n",
        "print(f\"   ‚Ä¢ TOON accuracy: {toon_overall_acc:.2f}%\")\n",
        "print(f\"   ‚Ä¢ Accuracy difference: {toon_overall_acc - json_overall_acc:+.2f}%\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "ijUoAQzNwu1i"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Results:"
      ],
      "metadata": {
        "id": "by2KIIeAwzaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Calculate token counts from DataFrames\n",
        "json_tokens = int(outputs_df_json['input_tokens'].mean())\n",
        "toon_tokens = int(outputs_df_toon['input_tokens'].mean())\n",
        "token_reduction = json_tokens - toon_tokens\n",
        "reduction_percentage = (token_reduction / json_tokens) * 100\n",
        "\n",
        "# Create figure with two side-by-side subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# ===== SUBPLOT 1: TOKEN COUNT COMPARISON =====\n",
        "ax1 = axes[0]\n",
        "formats = ['JSON', 'TOON']\n",
        "tokens = [json_tokens, toon_tokens]\n",
        "colors = ['#3498db', '#2ecc71']\n",
        "\n",
        "# Create bar chart\n",
        "bars = ax1.bar(formats, tokens, color=colors, alpha=0.7, edgecolor='black')\n",
        "ax1.set_ylabel('Token Count', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Token Count Comparison\\n(Lower is Better)', fontsize=14, fontweight='bold')\n",
        "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{int(height):,}',\n",
        "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "# Add reduction annotation with arrow\n",
        "reduction_text = f'{reduction_percentage:.1f}% reduction'\n",
        "ax1.annotate(reduction_text, xy=(1, toon_tokens), xytext=(0.5, json_tokens - 200),\n",
        "             arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
        "             fontsize=11, color='red', fontweight='bold',\n",
        "             bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
        "\n",
        "# ===== SUBPLOT 2: ACCURACY COMPARISON BY TYPE =====\n",
        "ax2 = axes[1]\n",
        "question_types = ['Field\\nRetrieval', 'Aggregation', 'Filtering', 'Overall']\n",
        "\n",
        "# Calculate accuracy data from DataFrames for both formats\n",
        "json_fr_acc = (outputs_df_json[outputs_df_json['type'] == 'field-retrieval']['correct'].sum() /\n",
        "               len(outputs_df_json[outputs_df_json['type'] == 'field-retrieval'])) * 100\n",
        "json_ag_acc = (outputs_df_json[outputs_df_json['type'] == 'aggregation']['correct'].sum() /\n",
        "               len(outputs_df_json[outputs_df_json['type'] == 'aggregation'])) * 100\n",
        "json_ft_acc = (outputs_df_json[outputs_df_json['type'] == 'filtering']['correct'].sum() /\n",
        "               len(outputs_df_json[outputs_df_json['type'] == 'filtering'])) * 100\n",
        "json_overall_acc = (outputs_df_json['correct'].sum() / len(outputs_df_json)) * 100\n",
        "\n",
        "toon_fr_acc = (outputs_df_toon[outputs_df_toon['type'] == 'field-retrieval']['correct'].sum() /\n",
        "               len(outputs_df_toon[outputs_df_toon['type'] == 'field-retrieval'])) * 100\n",
        "toon_ag_acc = (outputs_df_toon[outputs_df_toon['type'] == 'aggregation']['correct'].sum() /\n",
        "               len(outputs_df_toon[outputs_df_toon['type'] == 'aggregation'])) * 100\n",
        "toon_ft_acc = (outputs_df_toon[outputs_df_toon['type'] == 'filtering']['correct'].sum() /\n",
        "               len(outputs_df_toon[outputs_df_toon['type'] == 'filtering'])) * 100\n",
        "toon_overall_acc = (outputs_df_toon['correct'].sum() / len(outputs_df_toon)) * 100\n",
        "\n",
        "json_accuracies = [json_fr_acc, json_ag_acc, json_ft_acc, json_overall_acc]\n",
        "toon_accuracies = [toon_fr_acc, toon_ag_acc, toon_ft_acc, toon_overall_acc]\n",
        "\n",
        "# Create grouped bar chart\n",
        "x = np.arange(len(question_types))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax2.bar(x - width/2, json_accuracies, width, label='JSON',\n",
        "                color='#3498db', alpha=0.7, edgecolor='black')\n",
        "bars2 = ax2.bar(x + width/2, toon_accuracies, width, label='TOON',\n",
        "                color='#2ecc71', alpha=0.7, edgecolor='black')\n",
        "\n",
        "ax2.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Accuracy Comparison by Question Type\\n(Higher is Better)',\n",
        "              fontsize=14, fontweight='bold')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(question_types, fontsize=10)\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "ax2.set_ylim(0, 105)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                 f'{height:.1f}%',\n",
        "                 ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "# Save and display\n",
        "plt.tight_layout()\n",
        "plt.savefig('json_vs_toon_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Visualization saved as 'json_vs_toon_comparison.png'\")"
      ],
      "metadata": {
        "id": "AR06rYZ5wx4U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Error Analysis"
      ],
      "metadata": {
        "id": "MeAc0h4ow4Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify questions where the model gave incorrect answers\n",
        "json_errors = outputs_df_json[~outputs_df_json['correct']].to_dict('records')\n",
        "toon_errors = outputs_df_toon[~outputs_df_toon['correct']].to_dict('records')\n",
        "\n",
        "print(f\"üîç ERROR ANALYSIS:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nJSON Format: {len(json_errors)} errors out of {len(outputs_df_json)} questions\")\n",
        "print(f\"TOON Format: {len(toon_errors)} errors out of {len(outputs_df_toon)} questions\")\n",
        "\n",
        "# ===== SHOW SAMPLE ERRORS FROM JSON FORMAT =====\n",
        "print(f\"\\nüìù Sample Errors from JSON Format (first 5):\")\n",
        "print(\"-\"*70)\n",
        "for i, err in enumerate(json_errors[:5], 1):\n",
        "    print(f\"\\n{i}. Question: {err['question']}\")\n",
        "    print(f\"   Type: {err['type']}\")\n",
        "    print(f\"   Expected: {err['ground_truth']}\")\n",
        "    print(f\"   Got: {err['model_response']}\")\n",
        "\n",
        "# ===== SHOW SAMPLE ERRORS FROM TOON FORMAT =====\n",
        "print(f\"\\n\\nüìù Sample Errors from TOON Format (first 5):\")\n",
        "print(\"-\"*70)\n",
        "for i, err in enumerate(toon_errors[:5], 1):\n",
        "    print(f\"\\n{i}. Question: {err['question']}\")\n",
        "    print(f\"   Type: {err['type']}\")\n",
        "    print(f\"   Expected: {err['ground_truth']}\")\n",
        "    print(f\"   Got: {err['model_response']}\")\n",
        "\n",
        "# ===== ANALYZE DIFFERENTIAL ERRORS =====\n",
        "# Find questions that failed in one format but not the other\n",
        "json_error_ids = set(outputs_df_json[~outputs_df_json['correct']]['id'])\n",
        "toon_error_ids = set(outputs_df_toon[~outputs_df_toon['correct']]['id'])\n",
        "\n",
        "only_json_errors = json_error_ids - toon_error_ids\n",
        "only_toon_errors = toon_error_ids - json_error_ids\n",
        "\n",
        "print(f\"\\n\\nüîÑ DIFFERENTIAL ERRORS:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Questions that failed ONLY with JSON: {len(only_json_errors)}\")\n",
        "print(f\"Questions that failed ONLY with TOON: {len(only_toon_errors)}\")\n",
        "print(f\"Questions that failed with BOTH: {len(json_error_ids & toon_error_ids)}\")"
      ],
      "metadata": {
        "id": "3M-WfwYgw1ng"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}