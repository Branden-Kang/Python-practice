{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvTuRclL0iZPc/+a/6uJ+m"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://indraneeldb1993ds.medium.com/dimensionality-reduction-techniques-pca-lca-and-svd-f2a56b097f7c)"
      ],
      "metadata": {
        "id": "Dlm2HnTVNbgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis (PCA)"
      ],
      "metadata": {
        "id": "IEkQnq2ENxn_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHe-wHNIMT-U",
        "outputId": "2d6c3c8f-5d63-40f2-d2db-c588aab8360a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Training Data Shape: (120, 4)\n",
            "Reduced Training Data Shape (PCA): (120, 2)\n",
            "Number of Components Selected: 2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load iris dataset as an example\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "X_test_std = scaler.transform(X_test)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "X_train_pca = pca.fit_transform(X_train_std)\n",
        "\n",
        "# Calculate the cumulative explained variance\n",
        "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Determine the number of components to keep for 85% variance explained\n",
        "n_components = np.argmax(cumulative_variance_ratio >= 0.85) + 1\n",
        "\n",
        "# Apply PCA with the selected number of components\n",
        "pca = PCA(n_components=n_components)\n",
        "X_train_pca = pca.fit_transform(X_train_std)\n",
        "X_test_pca = pca.transform(X_test_std)\n",
        "\n",
        "# Display the results\n",
        "print(\"Original Training Data Shape:\", X_train.shape)\n",
        "print(\"Reduced Training Data Shape (PCA):\", X_train_pca.shape)\n",
        "print(\"Number of Components Selected:\", n_components)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Discriminant Analysis (LDA)"
      ],
      "metadata": {
        "id": "kFhjdxtjQf91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (important for LDA)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize LDA and fit on the training data\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "X_train_lda = lda.fit_transform(X_train, y_train)\n",
        "\n",
        "# Calculate explained variance ratio for each component\n",
        "explained_variance_ratio = lda.explained_variance_ratio_\n",
        "\n",
        "# Calculate the cumulative explained variance\n",
        "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Find the number of components that explain at least 75% of the variance\n",
        "n_components = np.argmax(cumulative_explained_variance >= 0.75) + 1\n",
        "\n",
        "# Transform both the training and test data to the selected number of components\n",
        "X_train_lda_selected = lda.transform(X_train)[:, :n_components]\n",
        "X_test_lda_selected = lda.transform(X_test)[:, :n_components]\n",
        "\n",
        "# Print the number of components selected\n",
        "print(f\"Number of components selected: {n_components}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxtK52hIQS8K",
        "outputId": "10a6678e-de55-4612-faaa-83f90c37359d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of components selected: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Singular Value Decomposition (SVD)"
      ],
      "metadata": {
        "id": "Q5E8-ccgSzko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (important for SVD)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize SVD and fit on the training data\n",
        "svd = TruncatedSVD(n_components=X_train.shape[1] - 1)  # Use one less component than the feature count\n",
        "X_train_svd = svd.fit_transform(X_train)\n",
        "\n",
        "# Calculate explained variance ratio for each component\n",
        "explained_variance_ratio = svd.explained_variance_ratio_\n",
        "\n",
        "# Calculate the cumulative explained variance\n",
        "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Find the number of components that explain at least 75% of the variance\n",
        "n_components = np.argmax(cumulative_explained_variance >= 0.75) + 1\n",
        "\n",
        "# Transform both the training and test data to the selected number of components\n",
        "X_train_svd_selected = svd.transform(X_train)[:, :n_components]\n",
        "X_test_svd_selected = svd.transform(X_test)[:, :n_components]\n",
        "\n",
        "# Print the number of components selected\n",
        "print(f\"Number of components selected: {n_components}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U63Du1xySzzQ",
        "outputId": "b7003c78-3e40-4121-90a7-a8186e3b9e1d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of components selected: 12\n"
          ]
        }
      ]
    }
  ]
}