{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8xWivNu+088yjTy87xjGc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@Rohan_Dutt/10-bayesian-optimization-tricks-for-hyperparameters-in-ml-you-can-use-today-7a149a32c830)"
      ],
      "metadata": {
        "id": "Gf9iBslQThJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Priors Like a Bayesian Pro"
      ],
      "metadata": {
        "id": "3bcu7rC6Tn2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LeKmOI2TtUb",
        "outputId": "441c076a-b827-478b-d994-95a82138da86"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (1.5.2)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-25.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (25.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.7.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-25.7.0 scikit-optimize-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SSgNh2XfTfYY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern\n",
        "from skopt import Optimizer\n",
        "\n",
        "\n",
        "# Step 1: Quick cheap search to build priors\n",
        "def objective(params):\n",
        "    lr, depth = params\n",
        "    return train_model(lr, depth)  # your training loop returning validation loss\n",
        "search_space = [\n",
        "    (1e-4, 1e-1),   # learning rate\n",
        "    (2, 10)         # depth\n",
        "]\n",
        "# quick 8-run grid/random search\n",
        "initial_points = [\n",
        "    (1e-4, 4), (1e-3, 4), (1e-2, 4),\n",
        "    (1e-4, 8), (1e-3, 8), (1e-2, 8),\n",
        "    (5e-3, 6), (8e-3, 10)\n",
        "]\n",
        "initial_results = [objective(p) for p in initial_points]\n",
        "# Step 2: Build priors for Bayesian Optimization\n",
        "kernel = Matern(nu=2.5)\n",
        "gp = GaussianProcessRegressor(kernel=kernel, normalize_y=True)\n",
        "# Step 3: Initialize optimizer with priors\n",
        "opt = Optimizer(\n",
        "    dimensions=search_space,\n",
        "    base_estimator=gp,\n",
        "    initial_point_generator=\"sobol\",\n",
        ")\n",
        "# Feed prior observations\n",
        "for p, r in zip(initial_points, initial_results):\n",
        "    opt.tell(p, r)\n",
        "# Step 4: Bayesian Optimization with informed priors\n",
        "for _ in range(30):\n",
        "    next_params = opt.ask()\n",
        "    score = objective(next_params)\n",
        "    opt.tell(next_params, score)\n",
        "best_params = opt.get_result().x\n",
        "print(\"Best Params:\", best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamically Adjust Your Acquisition Function"
      ],
      "metadata": {
        "id": "Z1tofCR8TrYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skopt import Optimizer\n",
        "from skopt.acquisition import gaussian_ei, gaussian_pi, gaussian_ucb\n",
        "\n",
        "# Dummy expensive objective\n",
        "def objective(params):\n",
        "    lr, depth = params\n",
        "    return train_model(lr, depth)  # Replace with your actual training loop\n",
        "space = [(1e-4, 1e-1), (2, 10)]\n",
        "opt = Optimizer(\n",
        "    dimensions=space,\n",
        "    base_estimator=\"GP\",\n",
        "    acq_func=\"EI\"   # initial acquisition function\n",
        ")\n",
        "def should_switch(iteration, recent_scores):\n",
        "    # Simple heuristic: if scores haven't improved in last 5 steps, switch mode\n",
        "    if iteration > 10 and np.std(recent_scores[-5:]) < 1e-4:\n",
        "        return True\n",
        "    return False\n",
        "scores = []\n",
        "for i in range(40):\n",
        "    # Dynamically pick acquisition function\n",
        "    if should_switch(i, scores):\n",
        "        # Choose UCB when nearing convergence, PI for risky exploration\n",
        "        opt.acq_func = \"UCB\" if scores[-1] < np.median(scores) else \"PI\"\n",
        "    x = opt.ask()\n",
        "    y = objective(x)\n",
        "    scores.append(y)\n",
        "    opt.tell(x, y)\n",
        "best_params = opt.get_result().x\n",
        "print(\"Best Params:\", best_params)"
      ],
      "metadata": {
        "id": "TIUddu16TqK2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Early Stopping for Hyper-hypers"
      ],
      "metadata": {
        "id": "YLQhC39zUEyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skopt import Optimizer\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
        "\n",
        "# Meta-learned priors from previous similar tasks\n",
        "meta_length_scale = 0.3\n",
        "meta_noise_level = 1e-3\n",
        "kernel = (\n",
        "    Matern(length_scale=meta_length_scale, nu=2.5) +\n",
        "    WhiteKernel(noise_level=meta_noise_level)\n",
        ")\n",
        "# Early-stop BO's own hyperparameter tuning\n",
        "gp = GaussianProcessRegressor(\n",
        "    kernel=kernel,\n",
        "    optimizer=\"fmin_l_bfgs_b\",\n",
        "    n_restarts_optimizer=0,    # Crucial: prevent expensive hyper-hyper loops\n",
        "    normalize_y=True\n",
        ")\n",
        "# BO with a stable, meta-initialized GP\n",
        "opt = Optimizer(\n",
        "    dimensions=[(1e-4, 1e-1), (2, 12)],\n",
        "    base_estimator=gp,\n",
        "    acq_func=\"EI\"\n",
        ")\n",
        "def objective(params):\n",
        "    lr, depth = params\n",
        "    return train_model(lr, depth)   # your model's validation loss\n",
        "scores = []\n",
        "for _ in range(40):\n",
        "    x = opt.ask()\n",
        "    y = objective(x)\n",
        "    opt.tell(x, y)\n",
        "    scores.append(y)\n",
        "best_params = opt.get_result().x\n",
        "print(\"Best Params:\", best_params)"
      ],
      "metadata": {
        "id": "7PzUlXiPUBz_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Penalize Promising but Costly Regions"
      ],
      "metadata": {
        "id": "5ikUSv0FUOdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skopt import Optimizer\n",
        "from skopt.acquisition import gaussian_ei\n",
        "\n",
        "# Objective returns BOTH validation loss and estimated training cost\n",
        "def objective(params):\n",
        "    lr, depth = params\n",
        "    val_loss = train_model(lr, depth)\n",
        "    cost = estimate_cost(lr, depth)   # e.g., GPU hours or FLOPs proxy\n",
        "    return val_loss, cost\n",
        "# Custom cost-aware EI: maximize EI / Cost\n",
        "def cost_aware_ei(model, X, y_min, costs):\n",
        "    raw_ei = gaussian_ei(X, model, y_min=y_min)\n",
        "    normalized_costs = costs / np.max(costs)\n",
        "    penalty = 1.0 / (1e-6 + normalized_costs)\n",
        "    return raw_ei * penalty\n",
        "# Search space\n",
        "opt = Optimizer(\n",
        "    dimensions=[(1e-4, 1e-1), (2, 20)],\n",
        "    base_estimator=\"GP\"\n",
        ")\n",
        "observed_losses = []\n",
        "observed_costs = []\n",
        "for _ in range(40):\n",
        "    # Ask a batch of candidate points\n",
        "    candidates = opt.ask(n_points=20)\n",
        "\n",
        "    # Evaluate cost-aware EI for each candidate\n",
        "    y_min = np.min(observed_losses) if observed_losses else np.inf\n",
        "    cost_scores = cost_aware_ei(\n",
        "        opt.base_estimator_,\n",
        "        np.array(candidates),\n",
        "        y_min=y_min,\n",
        "        costs=np.array(observed_costs[-len(candidates):] + [1]*len(candidates))  # fallback cost=1\n",
        "    )\n",
        "    # Pick best candidate under cost-awareness\n",
        "    next_x = candidates[np.argmax(cost_scores)]\n",
        "\n",
        "    (loss, cost) = objective(next_x)\n",
        "\n",
        "    observed_losses.append(loss)\n",
        "    observed_costs.append(cost)\n",
        "\n",
        "    opt.tell(next_x, loss)\n",
        "best_params = opt.get_result().x\n",
        "print(\"Best Params (Cost-Aware):\", best_params)"
      ],
      "metadata": {
        "id": "AQf9A839UHhn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybridize BO with Random Search"
      ],
      "metadata": {
        "id": "4wmlkylZUSKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skopt import Optimizer\n",
        "from skopt.space import Real, Integer\n",
        "\n",
        "# Define search space\n",
        "space = [\n",
        "    Real(1e-4, 1e-1, name=\"lr\"),\n",
        "    Integer(2, 12, name=\"depth\")\n",
        "]\n",
        "# Expensive training loop\n",
        "def objective(params):\n",
        "    lr, depth = params\n",
        "    return train_model(lr, depth)   # your model's validation loss\n",
        "# BO Optimizer\n",
        "opt = Optimizer(\n",
        "    dimensions=space,\n",
        "    base_estimator=\"GP\",\n",
        "    acq_func=\"EI\"\n",
        ")\n",
        "n_total = 50\n",
        "n_random = int(0.20 * n_total)      # first 20% = random exploration\n",
        "results = []\n",
        "for i in range(n_total):\n",
        "    if i < n_random:\n",
        "        # ----- Phase 1: Pure Random Search -----\n",
        "        x = [\n",
        "            np.random.uniform(1e-4, 1e-1),\n",
        "            np.random.randint(2, 13)\n",
        "        ]\n",
        "    else:\n",
        "        # ----- Phase 2: Bayesian Optimization -----\n",
        "        x = opt.ask()\n",
        "    y = objective(x)\n",
        "    results.append((x, y))\n",
        "    # Only tell BO after evaluations (keeps history consistent)\n",
        "    opt.tell(x, y)\n",
        "best_params = opt.get_result().x\n",
        "# print(\"Best Params (Hybrid):\", best_params)a"
      ],
      "metadata": {
        "id": "rCRskzQKUQvZ"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}