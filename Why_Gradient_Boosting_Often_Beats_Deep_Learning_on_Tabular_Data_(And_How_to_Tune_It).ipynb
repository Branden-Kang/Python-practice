{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNttNuufDDKEsoXcfWRb6N+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@Rohan_Dutt/why-gradient-boosting-often-beats-deep-learning-on-tabular-data-and-how-to-tune-it-17c4c59b1782)"
      ],
      "metadata": {
        "id": "FsGuSUFWH704"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Control Tree Depth Before Anything Else"
      ],
      "metadata": {
        "id": "zfHgyUfCIX35"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "93b1QlGIH5gP"
      },
      "outputs": [],
      "source": [
        "max_depth = 4    # try 3 to 6\n",
        "num_leaves = 2**max_depth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Shrink With Learning Rate"
      ],
      "metadata": {
        "id": "9wr5iSVtIlMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.03\n",
        "n_estimators = 1500"
      ],
      "metadata": {
        "id": "FKmetJjnIZ7d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Use Feature Subsampling"
      ],
      "metadata": {
        "id": "2w3lhcq7IpeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colsample_bytree = 0.7\n",
        "subsample = 0.8"
      ],
      "metadata": {
        "id": "85dlWF2kInNQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Regularize Hard on Small Datasets"
      ],
      "metadata": {
        "id": "vkkFOiOhItzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_l1 = 2.0\n",
        "lambda_l2 = 5.0\n",
        "min_child_weight = 10"
      ],
      "metadata": {
        "id": "TJgiUD1ZIrmm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimal Python Template to Tune Gradient Boosting"
      ],
      "metadata": {
        "id": "Icgy9pN9IyWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LGBMClassifier(\n",
        "    max_depth=5,\n",
        "    num_leaves=32,\n",
        "    learning_rate=0.03,\n",
        "    n_estimators=1200,\n",
        "    colsample_bytree=0.7,\n",
        "    subsample=0.8,\n",
        "    lambda_l1=2.0,\n",
        "    lambda_l2=5.0,\n",
        "    min_child_weight=10\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict_proba(X_val)[:, 1]\n",
        "print(\"AUC:\", roc_auc_score(y_val, preds))"
      ],
      "metadata": {
        "id": "CPa0SSa1Iwjo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Boosting Baseline"
      ],
      "metadata": {
        "id": "N8xzJsSeJbnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "model = XGBClassifier(\n",
        "    max_depth=5,\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=800,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.7,\n",
        "    reg_alpha=2.0,\n",
        "    reg_lambda=5.0,\n",
        "    min_child_weight=10,\n",
        "    eval_metric=\"auc\"\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict_proba(X_val)[:, 1]\n",
        "print(\"AUC:\", roc_auc_score(y_val, preds))"
      ],
      "metadata": {
        "id": "5NMVyrEaI0Rb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Hyperparameter Search That Actually Works"
      ],
      "metadata": {
        "id": "IEbP-tZoJeVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 5),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.05, 0.2),\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 1200),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.7, 0.9),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 0.9),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 5),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1, 10),\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 5, 20),\n",
        "        \"eval_metric\": \"auc\"\n",
        "    }\n",
        "    model = XGBClassifier(**params)\n",
        "    score = cross_val_score(model, X, y, cv=3, scoring=\"roc_auc\").mean()\n",
        "    return score\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=40)\n",
        "print(\"Best params:\", study.best_params)"
      ],
      "metadata": {
        "id": "cZjkcU_-JYQn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But deep learning is only worth the pain if you are dealing with:\n",
        "- ✔ Hierarchical data, like nested patient histories\n",
        "- ✔ More than 500 unique categories where tree splits fall apart\n",
        "- ✔ Hidden temporal or sensor patterns that trees cannot model cleanly"
      ],
      "metadata": {
        "id": "7A0orWncJsHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Hybrid Tree + Neural Head"
      ],
      "metadata": {
        "id": "dtiAzeYAcN9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from lightgbm import LGBMRegressor\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Step 1. Train a boosting model and extract leaf indexes as features\n",
        "gboost = LGBMRegressor(\n",
        "    max_depth=5,\n",
        "    n_estimators=800,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8\n",
        ")\n",
        "gboost.fit(X_train, y_train)\n",
        "leaf_train = gboost.predict(X_train, pred_leaf=True)\n",
        "leaf_val = gboost.predict(X_val, pred_leaf=True)\n",
        "# Step 2. One hot encode leaf indices\n",
        "leaf_train = np.array([np.eye(gboost.n_estimators, dtype=\"float32\")[row] for row in leaf_train])\n",
        "leaf_val = np.array([np.eye(gboost.n_estimators, dtype=\"float32\")[row] for row in leaf_val])\n",
        "# Step 3. Tiny neural head\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation=\"relu\", input_shape=(leaf_train.shape[1],)),\n",
        "    layers.Dense(1, activation=\"linear\")\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.fit(leaf_train, y_train, epochs=15, batch_size=64, verbose=0)\n",
        "preds = model.predict(leaf_val).flatten()"
      ],
      "metadata": {
        "id": "BlmBLqC7Jj-M"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}