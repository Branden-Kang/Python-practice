{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvN/DpfwRKB8TO0Lf1T11c"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@harishk3493/optuna-the-hyperparameter-optimization-framework-that-saved-my-machine-learning-sanity-315febd8f722)"
      ],
      "metadata": {
        "id": "9A18cmlfwGK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQFYQ_bkwV0X",
        "outputId": "e62e3a09-0b98-4058-f434-305d8585fa1d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.16.5)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.9.0 optuna-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASc0BQ5VvWim",
        "outputId": "86295900-2bda-47c6-f9e0-077485032cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-09-16 13:40:55,409] A new study created in memory with name: no-name-646082d2-7a32-404f-ad6f-edbdcc61fb7b\n",
            "[I 2025-09-16 13:40:59,502] Trial 0 finished with value: 0.9630957925787922 and parameters: {'max_depth': 5, 'learning_rate': 0.03485215151170995, 'n_estimators': 155, 'subsample': 0.6244171001452504, 'colsample_bytree': 0.959407615349203}. Best is trial 0 with value: 0.9630957925787922.\n",
            "[I 2025-09-16 13:41:04,389] Trial 1 finished with value: 0.9736221083682658 and parameters: {'max_depth': 8, 'learning_rate': 0.11346228710974687, 'n_estimators': 202, 'subsample': 0.8580416800987205, 'colsample_bytree': 0.6427383915112795}. Best is trial 1 with value: 0.9736221083682658.\n",
            "[I 2025-09-16 13:41:08,011] Trial 2 finished with value: 0.9666200900481293 and parameters: {'max_depth': 8, 'learning_rate': 0.1158036361703085, 'n_estimators': 128, 'subsample': 0.6684521911982491, 'colsample_bytree': 0.8747135611590535}. Best is trial 1 with value: 0.9736221083682658.\n",
            "[I 2025-09-16 13:41:08,751] Trial 3 finished with value: 0.9753764943331781 and parameters: {'max_depth': 5, 'learning_rate': 0.10908415553419774, 'n_estimators': 211, 'subsample': 0.8427740797397266, 'colsample_bytree': 0.8671531083952231}. Best is trial 3 with value: 0.9753764943331781.\n",
            "[I 2025-09-16 13:41:09,741] Trial 4 finished with value: 0.9683589504735289 and parameters: {'max_depth': 4, 'learning_rate': 0.07205305729612309, 'n_estimators': 297, 'subsample': 0.8426564653185649, 'colsample_bytree': 0.9550826492548928}. Best is trial 3 with value: 0.9753764943331781.\n",
            "[I 2025-09-16 13:41:10,003] Trial 5 finished with value: 0.9683744760130415 and parameters: {'max_depth': 3, 'learning_rate': 0.2796024118869551, 'n_estimators': 86, 'subsample': 0.6449956918716503, 'colsample_bytree': 0.7714121274894281}. Best is trial 3 with value: 0.9753764943331781.\n",
            "[I 2025-09-16 13:41:10,683] Trial 6 finished with value: 0.9701288619779538 and parameters: {'max_depth': 6, 'learning_rate': 0.13810339169353694, 'n_estimators': 232, 'subsample': 0.9900171125297695, 'colsample_bytree': 0.8169072921421389}. Best is trial 3 with value: 0.9753764943331781.\n",
            "[I 2025-09-16 13:41:11,294] Trial 7 finished with value: 0.9683589504735289 and parameters: {'max_depth': 10, 'learning_rate': 0.15105975951909562, 'n_estimators': 278, 'subsample': 0.9046054694195246, 'colsample_bytree': 0.6311451383705978}. Best is trial 3 with value: 0.9753764943331781.\n",
            "[I 2025-09-16 13:41:12,055] Trial 8 finished with value: 0.9683589504735288 and parameters: {'max_depth': 3, 'learning_rate': 0.04706305366749746, 'n_estimators': 172, 'subsample': 0.6288239782053202, 'colsample_bytree': 0.7697466655688392}. Best is trial 3 with value: 0.9753764943331781.\n",
            "[I 2025-09-16 13:41:12,816] Trial 9 finished with value: 0.9789007918025151 and parameters: {'max_depth': 9, 'learning_rate': 0.10407746586719333, 'n_estimators': 257, 'subsample': 0.806611901490071, 'colsample_bytree': 0.7321404344537279}. Best is trial 9 with value: 0.9789007918025151.\n",
            "[I 2025-09-16 13:41:13,301] Trial 10 finished with value: 0.9683434249340165 and parameters: {'max_depth': 10, 'learning_rate': 0.24601914909843497, 'n_estimators': 254, 'subsample': 0.756281746255163, 'colsample_bytree': 0.6926025588788088}. Best is trial 9 with value: 0.9789007918025151.\n",
            "[I 2025-09-16 13:41:13,806] Trial 11 finished with value: 0.9771464058376027 and parameters: {'max_depth': 8, 'learning_rate': 0.20421141652565278, 'n_estimators': 218, 'subsample': 0.7560018470869821, 'colsample_bytree': 0.8736990283475131}. Best is trial 9 with value: 0.9789007918025151.\n",
            "[I 2025-09-16 13:41:14,332] Trial 12 finished with value: 0.9736221083682658 and parameters: {'max_depth': 8, 'learning_rate': 0.20775668234009703, 'n_estimators': 245, 'subsample': 0.7424306174640599, 'colsample_bytree': 0.7201502002233497}. Best is trial 9 with value: 0.9789007918025151.\n",
            "[I 2025-09-16 13:41:14,834] Trial 13 finished with value: 0.9666045645086168 and parameters: {'max_depth': 9, 'learning_rate': 0.19723395898490842, 'n_estimators': 202, 'subsample': 0.7664945499494134, 'colsample_bytree': 0.8708238831509966}. Best is trial 9 with value: 0.9789007918025151.\n",
            "[I 2025-09-16 13:41:15,440] Trial 14 finished with value: 0.9718832479428661 and parameters: {'max_depth': 7, 'learning_rate': 0.19344019811356505, 'n_estimators': 263, 'subsample': 0.9179478868516696, 'colsample_bytree': 0.8216057258923918}. Best is trial 9 with value: 0.9789007918025151.\n",
            "[I 2025-09-16 13:41:15,707] Trial 15 finished with value: 0.9718677224033534 and parameters: {'max_depth': 9, 'learning_rate': 0.23887432844572098, 'n_estimators': 54, 'subsample': 0.7073184717306246, 'colsample_bytree': 0.7012223375068661}. Best is trial 9 with value: 0.9789007918025151.\n",
            "[I 2025-09-16 13:41:16,686] Trial 16 finished with value: 0.9701288619779538 and parameters: {'max_depth': 7, 'learning_rate': 0.07677866506483255, 'n_estimators': 227, 'subsample': 0.7876932526309175, 'colsample_bytree': 0.9057729416752993}. Best is trial 9 with value: 0.9789007918025151.\n",
            "[I 2025-09-16 13:41:19,330] Trial 17 finished with value: 0.9753920198726906 and parameters: {'max_depth': 9, 'learning_rate': 0.16955825666128643, 'n_estimators': 298, 'subsample': 0.7249085623648214, 'colsample_bytree': 0.7483007388658918}. Best is trial 9 with value: 0.9789007918025151.\n",
            "[I 2025-09-16 13:41:21,211] Trial 18 finished with value: 0.9701133364384411 and parameters: {'max_depth': 6, 'learning_rate': 0.24238367295806962, 'n_estimators': 140, 'subsample': 0.8145926954584067, 'colsample_bytree': 0.912168454271292}. Best is trial 9 with value: 0.9789007918025151.\n",
            "[I 2025-09-16 13:41:22,661] Trial 19 finished with value: 0.9631113181183046 and parameters: {'max_depth': 10, 'learning_rate': 0.010739957184790913, 'n_estimators': 196, 'subsample': 0.6977910664607624, 'colsample_bytree': 0.6666621534755852}. Best is trial 9 with value: 0.9789007918025151.\n",
            "[I 2025-09-16 13:41:23,243] Trial 20 finished with value: 0.9718832479428661 and parameters: {'max_depth': 8, 'learning_rate': 0.29477436400891893, 'n_estimators': 275, 'subsample': 0.8968556763720922, 'colsample_bytree': 0.9983857181262388}. Best is trial 9 with value: 0.9789007918025151.\n",
            "[I 2025-09-16 13:41:23,838] Trial 21 finished with value: 0.9771464058376027 and parameters: {'max_depth': 9, 'learning_rate': 0.17634817016580326, 'n_estimators': 296, 'subsample': 0.7216124167432705, 'colsample_bytree': 0.756759614892802}. Best is trial 9 with value: 0.9789007918025151.\n",
            "[I 2025-09-16 13:41:24,405] Trial 22 finished with value: 0.9683744760130415 and parameters: {'max_depth': 9, 'learning_rate': 0.17786119359498992, 'n_estimators': 235, 'subsample': 0.7785941895683492, 'colsample_bytree': 0.7939356806277036}. Best is trial 9 with value: 0.9789007918025151.\n",
            "[I 2025-09-16 13:41:24,946] Trial 23 finished with value: 0.9736221083682658 and parameters: {'max_depth': 7, 'learning_rate': 0.21418892808963355, 'n_estimators': 277, 'subsample': 0.6771247386148799, 'colsample_bytree': 0.7284440781644396}. Best is trial 9 with value: 0.9789007918025151.\n",
            "[I 2025-09-16 13:41:25,527] Trial 24 finished with value: 0.9806551777674274 and parameters: {'max_depth': 9, 'learning_rate': 0.14122818833599388, 'n_estimators': 264, 'subsample': 0.8100562692908606, 'colsample_bytree': 0.6014627220349051}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:26,083] Trial 25 finished with value: 0.9771464058376027 and parameters: {'max_depth': 8, 'learning_rate': 0.13034314330376692, 'n_estimators': 174, 'subsample': 0.8079624826143488, 'colsample_bytree': 0.6019632716208927}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:26,802] Trial 26 finished with value: 0.9771464058376027 and parameters: {'max_depth': 10, 'learning_rate': 0.08056217574248364, 'n_estimators': 222, 'subsample': 0.816295540190664, 'colsample_bytree': 0.6137755246557405}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:27,664] Trial 27 finished with value: 0.9683744760130415 and parameters: {'max_depth': 9, 'learning_rate': 0.09930630421108605, 'n_estimators': 253, 'subsample': 0.869119204488422, 'colsample_bytree': 0.8418263200281968}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:28,231] Trial 28 finished with value: 0.9753764943331781 and parameters: {'max_depth': 7, 'learning_rate': 0.15116693504684864, 'n_estimators': 187, 'subsample': 0.9420403438963472, 'colsample_bytree': 0.6596254700173071}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:28,789] Trial 29 finished with value: 0.9771464058376029 and parameters: {'max_depth': 8, 'learning_rate': 0.2210995896704721, 'n_estimators': 266, 'subsample': 0.746740287093236, 'colsample_bytree': 0.9253715604228079}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:29,348] Trial 30 finished with value: 0.9718677224033534 and parameters: {'max_depth': 6, 'learning_rate': 0.22527699197530435, 'n_estimators': 257, 'subsample': 0.7982040425098769, 'colsample_bytree': 0.9449666849434047}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:32,486] Trial 31 finished with value: 0.9718832479428661 and parameters: {'max_depth': 8, 'learning_rate': 0.2639416482610395, 'n_estimators': 271, 'subsample': 0.7521089600766794, 'colsample_bytree': 0.9229668083952434}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:33,156] Trial 32 finished with value: 0.9701133364384411 and parameters: {'max_depth': 9, 'learning_rate': 0.1701284651399136, 'n_estimators': 242, 'subsample': 0.7807742327406779, 'colsample_bytree': 0.9857949665106349}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:33,866] Trial 33 finished with value: 0.9736531594472908 and parameters: {'max_depth': 8, 'learning_rate': 0.1234848465965686, 'n_estimators': 217, 'subsample': 0.8327027464132895, 'colsample_bytree': 0.8495988445280716}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:34,745] Trial 34 finished with value: 0.9718832479428661 and parameters: {'max_depth': 8, 'learning_rate': 0.09433645015154157, 'n_estimators': 278, 'subsample': 0.8764699585295421, 'colsample_bytree': 0.8874516947525563}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:35,878] Trial 35 finished with value: 0.9648501785437043 and parameters: {'max_depth': 10, 'learning_rate': 0.050624637651718145, 'n_estimators': 206, 'subsample': 0.7379194481287804, 'colsample_bytree': 0.9310201368416117}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:36,777] Trial 36 finished with value: 0.9666045645086168 and parameters: {'max_depth': 7, 'learning_rate': 0.18625681163915114, 'n_estimators': 287, 'subsample': 0.8335152051505847, 'colsample_bytree': 0.8860237032019238}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:38,473] Trial 37 finished with value: 0.9718677224033534 and parameters: {'max_depth': 5, 'learning_rate': 0.14567433740536173, 'n_estimators': 240, 'subsample': 0.6728192986877987, 'colsample_bytree': 0.7901465013808364}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:39,304] Trial 38 finished with value: 0.9718677224033534 and parameters: {'max_depth': 9, 'learning_rate': 0.11418890339182618, 'n_estimators': 265, 'subsample': 0.8474697472043544, 'colsample_bytree': 0.9745745780019957}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:39,726] Trial 39 finished with value: 0.9630957925787922 and parameters: {'max_depth': 8, 'learning_rate': 0.2244068427975278, 'n_estimators': 114, 'subsample': 0.7681303629797306, 'colsample_bytree': 0.8490668631869239}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:40,192] Trial 40 finished with value: 0.9665890389691041 and parameters: {'max_depth': 10, 'learning_rate': 0.254169835709926, 'n_estimators': 250, 'subsample': 0.7005647938286736, 'colsample_bytree': 0.6457394290920362}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:40,808] Trial 41 finished with value: 0.9736221083682658 and parameters: {'max_depth': 9, 'learning_rate': 0.16172214866653933, 'n_estimators': 298, 'subsample': 0.7211785561620275, 'colsample_bytree': 0.7603889750198879}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:41,401] Trial 42 finished with value: 0.9736221083682658 and parameters: {'max_depth': 9, 'learning_rate': 0.13472633685069133, 'n_estimators': 296, 'subsample': 0.6442204185056696, 'colsample_bytree': 0.6888454882742194}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:43,831] Trial 43 finished with value: 0.9718987734823784 and parameters: {'max_depth': 9, 'learning_rate': 0.2086948327769176, 'n_estimators': 289, 'subsample': 0.7299948228537751, 'colsample_bytree': 0.7391254788103642}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:44,398] Trial 44 finished with value: 0.9736221083682658 and parameters: {'max_depth': 10, 'learning_rate': 0.18544549802873472, 'n_estimators': 283, 'subsample': 0.7918513305531562, 'colsample_bytree': 0.7111993251806717}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:45,340] Trial 45 finished with value: 0.9736376339077782 and parameters: {'max_depth': 8, 'learning_rate': 0.16027546899501552, 'n_estimators': 228, 'subsample': 0.7555652633988685, 'colsample_bytree': 0.8198353919142327}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:45,876] Trial 46 finished with value: 0.9718677224033534 and parameters: {'max_depth': 9, 'learning_rate': 0.19995529393088446, 'n_estimators': 264, 'subsample': 0.6900124677704699, 'colsample_bytree': 0.6788173845694115}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:47,163] Trial 47 finished with value: 0.9771619313771154 and parameters: {'max_depth': 8, 'learning_rate': 0.10654104138869695, 'n_estimators': 157, 'subsample': 0.8234043367893149, 'colsample_bytree': 0.7859802467249072}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:47,932] Trial 48 finished with value: 0.9666045645086166 and parameters: {'max_depth': 7, 'learning_rate': 0.06309789781220285, 'n_estimators': 156, 'subsample': 0.6026394162942983, 'colsample_bytree': 0.7819438043309623}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:49,504] Trial 49 finished with value: 0.9736376339077782 and parameters: {'max_depth': 8, 'learning_rate': 0.09896039874569669, 'n_estimators': 165, 'subsample': 0.8247571163309432, 'colsample_bytree': 0.9013762984156668}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:50,593] Trial 50 finished with value: 0.971883247942866 and parameters: {'max_depth': 6, 'learning_rate': 0.12334043215316029, 'n_estimators': 137, 'subsample': 0.852951717408752, 'colsample_bytree': 0.9614032803870538}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:51,511] Trial 51 finished with value: 0.9718677224033534 and parameters: {'max_depth': 8, 'learning_rate': 0.14197844338635704, 'n_estimators': 120, 'subsample': 0.770300763832922, 'colsample_bytree': 0.8071174312616765}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:52,049] Trial 52 finished with value: 0.9718677224033534 and parameters: {'max_depth': 9, 'learning_rate': 0.2342419543588879, 'n_estimators': 265, 'subsample': 0.7118614612856482, 'colsample_bytree': 0.7517717165898594}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:52,815] Trial 53 finished with value: 0.9683589504735288 and parameters: {'max_depth': 8, 'learning_rate': 0.08875040361510766, 'n_estimators': 196, 'subsample': 0.7955400075930924, 'colsample_bytree': 0.7724500853114009}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:54,877] Trial 54 finished with value: 0.9701288619779538 and parameters: {'max_depth': 4, 'learning_rate': 0.10951386174134983, 'n_estimators': 183, 'subsample': 0.7484913525382308, 'colsample_bytree': 0.7346294983223774}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:56,450] Trial 55 finished with value: 0.9736376339077782 and parameters: {'max_depth': 7, 'learning_rate': 0.2168629481273412, 'n_estimators': 212, 'subsample': 0.8174362529065875, 'colsample_bytree': 0.8643662186740001}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:56,890] Trial 56 finished with value: 0.9701443875174661 and parameters: {'max_depth': 9, 'learning_rate': 0.16900239807309841, 'n_estimators': 88, 'subsample': 0.8023470412234556, 'colsample_bytree': 0.8282151430208017}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:57,445] Trial 57 finished with value: 0.9736376339077782 and parameters: {'max_depth': 10, 'learning_rate': 0.1918328162173168, 'n_estimators': 256, 'subsample': 0.7637564416614459, 'colsample_bytree': 0.8006759435182519}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:57,923] Trial 58 finished with value: 0.9771308802980905 and parameters: {'max_depth': 9, 'learning_rate': 0.12449290074185086, 'n_estimators': 151, 'subsample': 0.7377197422747319, 'colsample_bytree': 0.6276049155997885}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:58,542] Trial 59 finished with value: 0.9736221083682658 and parameters: {'max_depth': 8, 'learning_rate': 0.1796396634452666, 'n_estimators': 234, 'subsample': 0.8796971754111141, 'colsample_bytree': 0.9436881097340095}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:59,020] Trial 60 finished with value: 0.9700978108989288 and parameters: {'max_depth': 8, 'learning_rate': 0.26767653494217664, 'n_estimators': 272, 'subsample': 0.7796210791067726, 'colsample_bytree': 0.7664780120152699}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:41:59,547] Trial 61 finished with value: 0.9736221083682658 and parameters: {'max_depth': 8, 'learning_rate': 0.1333820740014935, 'n_estimators': 176, 'subsample': 0.8102879881634274, 'colsample_bytree': 0.6030117553793672}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:00,112] Trial 62 finished with value: 0.9771464058376027 and parameters: {'max_depth': 7, 'learning_rate': 0.10402013352930867, 'n_estimators': 166, 'subsample': 0.8381800739549127, 'colsample_bytree': 0.619729671263893}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:00,747] Trial 63 finished with value: 0.9771464058376029 and parameters: {'max_depth': 9, 'learning_rate': 0.08534941045042066, 'n_estimators': 152, 'subsample': 0.7174882071265472, 'colsample_bytree': 0.6550313308438507}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:01,344] Trial 64 finished with value: 0.9666045645086166 and parameters: {'max_depth': 9, 'learning_rate': 0.0843699828136345, 'n_estimators': 130, 'subsample': 0.688806969145254, 'colsample_bytree': 0.7022784437964107}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:01,803] Trial 65 finished with value: 0.971883247942866 and parameters: {'max_depth': 10, 'learning_rate': 0.19843075138346053, 'n_estimators': 152, 'subsample': 0.7151250698139553, 'colsample_bytree': 0.7190040614455737}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:03,213] Trial 66 finished with value: 0.9613258810743673 and parameters: {'max_depth': 9, 'learning_rate': 0.024409100805594275, 'n_estimators': 249, 'subsample': 0.7304514287255752, 'colsample_bytree': 0.6447334490260511}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:03,868] Trial 67 finished with value: 0.9701288619779538 and parameters: {'max_depth': 9, 'learning_rate': 0.07246917345294623, 'n_estimators': 143, 'subsample': 0.6503593844986533, 'colsample_bytree': 0.6570406564677831}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:04,884] Trial 68 finished with value: 0.9701288619779538 and parameters: {'max_depth': 10, 'learning_rate': 0.0649820666624804, 'n_estimators': 289, 'subsample': 0.8647673973120087, 'colsample_bytree': 0.749555465247694}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:06,108] Trial 69 finished with value: 0.9665890389691041 and parameters: {'max_depth': 9, 'learning_rate': 0.043019331901201344, 'n_estimators': 222, 'subsample': 0.7458466563389254, 'colsample_bytree': 0.7808467716092463}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:08,388] Trial 70 finished with value: 0.9736376339077782 and parameters: {'max_depth': 8, 'learning_rate': 0.15137522567813086, 'n_estimators': 102, 'subsample': 0.7869775483547072, 'colsample_bytree': 0.670285123351212}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:08,973] Trial 71 finished with value: 0.9736376339077782 and parameters: {'max_depth': 8, 'learning_rate': 0.12706965163213868, 'n_estimators': 189, 'subsample': 0.8090344401134532, 'colsample_bytree': 0.6130391916017582}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:09,525] Trial 72 finished with value: 0.9789318428815401 and parameters: {'max_depth': 9, 'learning_rate': 0.1155665815037366, 'n_estimators': 176, 'subsample': 0.8254498780298855, 'colsample_bytree': 0.6018641345309029}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:10,103] Trial 73 finished with value: 0.9736531594472908 and parameters: {'max_depth': 9, 'learning_rate': 0.11477263819176549, 'n_estimators': 165, 'subsample': 0.8204701867173302, 'colsample_bytree': 0.6234251227228823}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:10,804] Trial 74 finished with value: 0.9753920198726906 and parameters: {'max_depth': 9, 'learning_rate': 0.08837881473113335, 'n_estimators': 240, 'subsample': 0.828859649908421, 'colsample_bytree': 0.6322729585704681}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:11,365] Trial 75 finished with value: 0.9736065828287532 and parameters: {'max_depth': 10, 'learning_rate': 0.10617109757333434, 'n_estimators': 158, 'subsample': 0.7626266891713743, 'colsample_bytree': 0.6027953880511476}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:12,265] Trial 76 finished with value: 0.9736376339077782 and parameters: {'max_depth': 9, 'learning_rate': 0.09583421459664267, 'n_estimators': 282, 'subsample': 0.9681236110237454, 'colsample_bytree': 0.8108843252453153}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:12,895] Trial 77 finished with value: 0.9753764943331781 and parameters: {'max_depth': 9, 'learning_rate': 0.14407422133210457, 'n_estimators': 259, 'subsample': 0.8442940682514462, 'colsample_bytree': 0.6551444332851933}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:13,602] Trial 78 finished with value: 0.9771464058376029 and parameters: {'max_depth': 5, 'learning_rate': 0.11815352005353716, 'n_estimators': 269, 'subsample': 0.7739033134149027, 'colsample_bytree': 0.6409047281569563}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:14,191] Trial 79 finished with value: 0.9630957925787922 and parameters: {'max_depth': 3, 'learning_rate': 0.06291836420756954, 'n_estimators': 147, 'subsample': 0.775767284902362, 'colsample_bytree': 0.6334167925078245}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:14,777] Trial 80 finished with value: 0.9771464058376029 and parameters: {'max_depth': 4, 'learning_rate': 0.11135622462494091, 'n_estimators': 246, 'subsample': 0.7856093983781478, 'colsample_bytree': 0.6134076270645856}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:15,413] Trial 81 finished with value: 0.9753920198726906 and parameters: {'max_depth': 4, 'learning_rate': 0.11968067055411064, 'n_estimators': 247, 'subsample': 0.789442348062388, 'colsample_bytree': 0.6396171328308015}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:16,060] Trial 82 finished with value: 0.9771619313771154 and parameters: {'max_depth': 5, 'learning_rate': 0.11210779245632985, 'n_estimators': 270, 'subsample': 0.802437034490045, 'colsample_bytree': 0.6144449351177088}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:16,424] Trial 83 finished with value: 0.9718987734823784 and parameters: {'max_depth': 5, 'learning_rate': 0.1131732464738306, 'n_estimators': 52, 'subsample': 0.8093460251055704, 'colsample_bytree': 0.6136753295466348}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:17,197] Trial 84 finished with value: 0.9753764943331781 and parameters: {'max_depth': 5, 'learning_rate': 0.10212355845664847, 'n_estimators': 273, 'subsample': 0.8021821874815749, 'colsample_bytree': 0.6493867826140095}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:17,515] Trial 85 finished with value: 0.9736376339077782 and parameters: {'max_depth': 4, 'learning_rate': 0.13578688456348467, 'n_estimators': 64, 'subsample': 0.8539476812953833, 'colsample_bytree': 0.608615930071648}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:18,213] Trial 86 finished with value: 0.9736221083682658 and parameters: {'max_depth': 3, 'learning_rate': 0.07879157433212146, 'n_estimators': 267, 'subsample': 0.7837104360376936, 'colsample_bytree': 0.667137647767668}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:20,876] Trial 87 finished with value: 0.9753764943331781 and parameters: {'max_depth': 6, 'learning_rate': 0.0948895517385212, 'n_estimators': 257, 'subsample': 0.8262049068212518, 'colsample_bytree': 0.619916715081458}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:21,547] Trial 88 finished with value: 0.9700978108989287 and parameters: {'max_depth': 4, 'learning_rate': 0.11839016691665007, 'n_estimators': 283, 'subsample': 0.7722104970223529, 'colsample_bytree': 0.6856605882845395}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:22,264] Trial 89 finished with value: 0.9736221083682658 and parameters: {'max_depth': 5, 'learning_rate': 0.11083973754380959, 'n_estimators': 243, 'subsample': 0.8023100113889093, 'colsample_bytree': 0.6414518938631838}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:22,985] Trial 90 finished with value: 0.971852196863841 and parameters: {'max_depth': 5, 'learning_rate': 0.08872843113209106, 'n_estimators': 261, 'subsample': 0.7548025088342339, 'colsample_bytree': 0.6317558742475151}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:23,688] Trial 91 finished with value: 0.9718677224033534 and parameters: {'max_depth': 6, 'learning_rate': 0.12812100669674342, 'n_estimators': 268, 'subsample': 0.7926234150295273, 'colsample_bytree': 0.8316440051104422}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:24,238] Trial 92 finished with value: 0.9736065828287532 and parameters: {'max_depth': 4, 'learning_rate': 0.13936733449121527, 'n_estimators': 253, 'subsample': 0.7384678759589675, 'colsample_bytree': 0.6034048847050544}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:24,612] Trial 93 finished with value: 0.9753764943331781 and parameters: {'max_depth': 5, 'learning_rate': 0.2293667992227765, 'n_estimators': 134, 'subsample': 0.7646657183410484, 'colsample_bytree': 0.6180416930814709}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:25,237] Trial 94 finished with value: 0.9718832479428661 and parameters: {'max_depth': 3, 'learning_rate': 0.157999290402515, 'n_estimators': 277, 'subsample': 0.8366937672209384, 'colsample_bytree': 0.9134877233236384}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:26,026] Trial 95 finished with value: 0.9753764943331781 and parameters: {'max_depth': 7, 'learning_rate': 0.10438635294445082, 'n_estimators': 228, 'subsample': 0.7976005715776495, 'colsample_bytree': 0.8822202569215186}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:26,717] Trial 96 finished with value: 0.9753920198726906 and parameters: {'max_depth': 4, 'learning_rate': 0.11933620243551718, 'n_estimators': 237, 'subsample': 0.8151587617242115, 'colsample_bytree': 0.8960765588758678}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:27,120] Trial 97 finished with value: 0.9683589504735289 and parameters: {'max_depth': 6, 'learning_rate': 0.24945012497041275, 'n_estimators': 179, 'subsample': 0.7822104525195507, 'colsample_bytree': 0.6761109102962652}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:27,729] Trial 98 finished with value: 0.9771153547585778 and parameters: {'max_depth': 8, 'learning_rate': 0.09499672019569487, 'n_estimators': 170, 'subsample': 0.7594094414715155, 'colsample_bytree': 0.6098135282086014}. Best is trial 24 with value: 0.9806551777674274.\n",
            "[I 2025-09-16 13:42:28,331] Trial 99 finished with value: 0.9701288619779538 and parameters: {'max_depth': 7, 'learning_rate': 0.14724976191911676, 'n_estimators': 201, 'subsample': 0.74770733626688, 'colsample_bytree': 0.8588953638113823}. Best is trial 24 with value: 0.9806551777674274.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': 9, 'learning_rate': 0.14122818833599388, 'n_estimators': 264, 'subsample': 0.8100562692908606, 'colsample_bytree': 0.6014627220349051}\n",
            "Best accuracy: 0.9806551777674274\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load sample data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "def objective(trial):\n",
        "    # Define hyperparameter search space\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "    }\n",
        "\n",
        "    # Create and evaluate model\n",
        "    model = xgb.XGBClassifier(**params, random_state=42)\n",
        "    score = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
        "\n",
        "    return score\n",
        "\n",
        "# Create study and optimize\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "print(\"Best parameters:\", study.best_params)\n",
        "print(\"Best accuracy:\", study.best_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 1: Manual Hyperparameters (Expert Intuition)"
      ],
      "metadata": {
        "id": "ZtkvkdePwY-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "manual_params = {\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 300,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'min_child_weight': 5,\n",
        "    'gamma': 1,\n",
        "    'reg_alpha': 1,\n",
        "    'reg_lambda': 1,\n",
        "    'scale_pos_weight': 1.5,  # Adjusted for class imbalance\n",
        "}"
      ],
      "metadata": {
        "id": "adtWFdnjwb4D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 2: RandomizedSearchCV (Random Sampling)\n",
        "RandomizedSearchCV showed significant improvement over manual tuning, demonstrating the value of exploring multiple parameter combinations. However, this approach treats each trial independently without learning from previous attempts."
      ],
      "metadata": {
        "id": "Axl6fHLTwe6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 3: Optuna with Intelligent Pruning\n",
        "```\n",
        "def objective_with_pruning(trial):\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "        # ... other parameters\n",
        "    }\n",
        "    \n",
        "    model = xgb.XGBClassifier(**params)\n",
        "    \n",
        "    # Cross-validation with pruning callback\n",
        "    scores = []\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "        # Train and evaluate\n",
        "        fold_score = model.score(X_fold_val, y_fold_val)\n",
        "        scores.append(fold_score)\n",
        "        \n",
        "        # Report for pruning\n",
        "        trial.report(fold_score, fold)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "    \n",
        "    return np.mean(scores)\n",
        "```"
      ],
      "metadata": {
        "id": "sGZyfCbYwmbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import optuna.visualization as vis\n",
        "import xgboost as xgb\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV\n",
        "from sklearn.datasets import fetch_covtype\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from scipy.stats import randint, uniform\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Load FOREST COVERTYPE DATASET - A genuinely challenging real-world dataset\n",
        "print(\"Loading Forest Covertype Dataset...\")\n",
        "print(\"This is a REAL complex dataset from UCI repository\")\n",
        "print(\"Predicting forest cover type from cartographic variables\")\n",
        "\n",
        "# Load the challenging Forest Covertype dataset\n",
        "data = fetch_covtype()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Convert to binary classification to make it even more challenging\n",
        "# Combine smaller classes to create class imbalance (realistic scenario)\n",
        "print(f\"Original classes: {np.unique(y)} with distribution: {np.bincount(y)}\")\n",
        "\n",
        "# Create imbalanced binary classification: Class 1&2 vs Others\n",
        "y_binary = np.where((y == 1) | (y == 2), 0, 1)  # 0: Spruce/Fir, 1: Others\n",
        "print(f\"Binary classes distribution: {np.bincount(y_binary)}\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42, stratify=y_binary)\n",
        "\n",
        "print(f\"\\nDATASET COMPLEXITY:\")\n",
        "print(f\"- Samples: {X.shape[0]:,}\")\n",
        "print(f\"- Features: {X.shape[1]} (mix of continuous and categorical)\")\n",
        "print(f\"- Train set: {X_train.shape[0]:,} samples\")\n",
        "print(f\"- Test set: {X_test.shape[0]:,} samples\")\n",
        "print(f\"- Class imbalance: {np.bincount(y_binary)[0]:,} vs {np.bincount(y_binary)[1]:,}\")\n",
        "print(f\"- Real-world geographic/environmental data with complex interactions\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Define EXTENSIVE parameter space - this is where Optuna should excel\n",
        "PARAM_RANGES = {\n",
        "    'max_depth': (3, 15),                    # Tree complexity\n",
        "    'learning_rate': (0.001, 0.3),          # Learning speed\n",
        "    'n_estimators': (100, 1000),            # Number of trees\n",
        "    'subsample': (0.5, 1.0),                # Row sampling\n",
        "    'colsample_bytree': (0.3, 1.0),         # Feature sampling per tree\n",
        "    'colsample_bylevel': (0.3, 1.0),        # Feature sampling per level\n",
        "    'colsample_bynode': (0.3, 1.0),         # Feature sampling per node\n",
        "    'min_child_weight': (1, 20),            # Minimum samples in leaf\n",
        "    'gamma': (0, 10),                       # Minimum split loss\n",
        "    'reg_alpha': (0, 10),                   # L1 regularization\n",
        "    'reg_lambda': (0, 10),                  # L2 regularization\n",
        "    'scale_pos_weight': (0.1, 5.0)          # Handle class imbalance\n",
        "}\n",
        "\n",
        "N_TRIALS = 100  # Increased for better pruning demonstration\n",
        "\n",
        "print(\"OPTIMIZATION CHALLENGE:\")\n",
        "print(f\"- {len(PARAM_RANGES)} hyperparameters to tune\")\n",
        "print(f\"- Complex parameter interactions\")\n",
        "print(f\"- Large dataset requiring longer training times\")\n",
        "print(f\"- {N_TRIALS} trials budget\")\n",
        "print(f\"- Real-world data with noise and complexity\")\n",
        "print()\n",
        "\n",
        "# ========================\n",
        "# 1. MANUAL HYPERPARAMETERS\n",
        "# ========================\n",
        "print(\"1. MANUAL HYPERPARAMETERS (Expert Intuition)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Expert parameters for large, imbalanced dataset\n",
        "manual_params = {\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 300,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'colsample_bylevel': 0.8,\n",
        "    'colsample_bynode': 0.8,\n",
        "    'min_child_weight': 5,\n",
        "    'gamma': 1,\n",
        "    'reg_alpha': 1,\n",
        "    'reg_lambda': 1,\n",
        "    'scale_pos_weight': 1.5,  # Adjust for class imbalance\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "start_time = time.time()\n",
        "manual_model = xgb.XGBClassifier(**manual_params)\n",
        "\n",
        "# Cross-validation for manual model (3-fold due to dataset size)\n",
        "manual_cv_scores = cross_val_score(manual_model, X_train, y_train, cv=3, scoring='accuracy')\n",
        "manual_cv_mean = manual_cv_scores.mean()\n",
        "manual_cv_std = manual_cv_scores.std()\n",
        "\n",
        "# Train on full training set and test\n",
        "manual_model.fit(X_train, y_train)\n",
        "manual_predictions = manual_model.predict(X_test)\n",
        "manual_test_accuracy = accuracy_score(y_test, manual_predictions)\n",
        "manual_time = time.time() - start_time\n",
        "\n",
        "print(f\"Manual Parameters: {len(manual_params)} parameters set\")\n",
        "print(f\"Cross-validation Accuracy: {manual_cv_mean:.4f} (+/- {manual_cv_std:.4f})\")\n",
        "print(f\"Test Accuracy: {manual_test_accuracy:.4f}\")\n",
        "print(f\"Training Time: {manual_time:.1f} seconds\")\n",
        "print(f\"Number of combinations tried: 1\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ========================\n",
        "# 2. RANDOMIZED SEARCH\n",
        "# ========================\n",
        "print(\"2. RANDOMIZED SEARCH (Random Sampling)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Define parameter distributions for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'max_depth': randint(PARAM_RANGES['max_depth'][0], PARAM_RANGES['max_depth'][1] + 1),\n",
        "    'learning_rate': uniform(PARAM_RANGES['learning_rate'][0],\n",
        "                           PARAM_RANGES['learning_rate'][1] - PARAM_RANGES['learning_rate'][0]),\n",
        "    'n_estimators': randint(PARAM_RANGES['n_estimators'][0], PARAM_RANGES['n_estimators'][1] + 1),\n",
        "    'subsample': uniform(PARAM_RANGES['subsample'][0],\n",
        "                        PARAM_RANGES['subsample'][1] - PARAM_RANGES['subsample'][0]),\n",
        "    'colsample_bytree': uniform(PARAM_RANGES['colsample_bytree'][0],\n",
        "                               PARAM_RANGES['colsample_bytree'][1] - PARAM_RANGES['colsample_bytree'][0]),\n",
        "    'colsample_bylevel': uniform(PARAM_RANGES['colsample_bylevel'][0],\n",
        "                                PARAM_RANGES['colsample_bylevel'][1] - PARAM_RANGES['colsample_bylevel'][0]),\n",
        "    'colsample_bynode': uniform(PARAM_RANGES['colsample_bynode'][0],\n",
        "                               PARAM_RANGES['colsample_bynode'][1] - PARAM_RANGES['colsample_bynode'][0]),\n",
        "    'min_child_weight': randint(PARAM_RANGES['min_child_weight'][0], PARAM_RANGES['min_child_weight'][1] + 1),\n",
        "    'gamma': uniform(PARAM_RANGES['gamma'][0], PARAM_RANGES['gamma'][1] - PARAM_RANGES['gamma'][0]),\n",
        "    'reg_alpha': uniform(PARAM_RANGES['reg_alpha'][0], PARAM_RANGES['reg_alpha'][1] - PARAM_RANGES['reg_alpha'][0]),\n",
        "    'reg_lambda': uniform(PARAM_RANGES['reg_lambda'][0], PARAM_RANGES['reg_lambda'][1] - PARAM_RANGES['reg_lambda'][0]),\n",
        "    'scale_pos_weight': uniform(PARAM_RANGES['scale_pos_weight'][0],\n",
        "                               PARAM_RANGES['scale_pos_weight'][1] - PARAM_RANGES['scale_pos_weight'][0])\n",
        "}\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Perform randomized search (reduced CV folds for speed)\n",
        "random_search = RandomizedSearchCV(\n",
        "    xgb.XGBClassifier(random_state=42),\n",
        "    param_distributions,\n",
        "    n_iter=N_TRIALS,\n",
        "    cv=3,  # Reduced due to large dataset\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(f\"Starting RandomizedSearchCV with {N_TRIALS} trials...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best model and evaluate\n",
        "random_best_params = random_search.best_params_\n",
        "random_cv_score = random_search.best_score_\n",
        "random_predictions = random_search.predict(X_test)\n",
        "random_test_accuracy = accuracy_score(y_test, random_predictions)\n",
        "random_time = time.time() - start_time\n",
        "\n",
        "print(f\"Best Parameters Found: {len(random_best_params)} parameters optimized\")\n",
        "print(f\"Best Cross-validation Accuracy: {random_cv_score:.4f}\")\n",
        "print(f\"Test Accuracy: {random_test_accuracy:.4f}\")\n",
        "print(f\"Total Search Time: {random_time:.1f} seconds ({random_time/60:.1f} minutes)\")\n",
        "print(f\"Number of combinations tried: {N_TRIALS}\")\n",
        "print(f\"Average time per trial: {random_time/N_TRIALS:.1f} seconds\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ========================\n",
        "# 3. OPTUNA WITH PRUNING\n",
        "# ========================\n",
        "print(\"3. OPTUNA WITH PRUNING (Smart Search + Early Stopping)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def objective_with_pruning(trial):\n",
        "    # Define comprehensive hyperparameter search space\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', *PARAM_RANGES['max_depth']),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', *PARAM_RANGES['learning_rate'], log=True),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', *PARAM_RANGES['n_estimators']),\n",
        "        'subsample': trial.suggest_float('subsample', *PARAM_RANGES['subsample']),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', *PARAM_RANGES['colsample_bytree']),\n",
        "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', *PARAM_RANGES['colsample_bylevel']),\n",
        "        'colsample_bynode': trial.suggest_float('colsample_bynode', *PARAM_RANGES['colsample_bynode']),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', *PARAM_RANGES['min_child_weight']),\n",
        "        'gamma': trial.suggest_float('gamma', *PARAM_RANGES['gamma']),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', *PARAM_RANGES['reg_alpha']),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', *PARAM_RANGES['reg_lambda']),\n",
        "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', *PARAM_RANGES['scale_pos_weight']),\n",
        "        'random_state': 42\n",
        "    }\n",
        "\n",
        "    # Create model\n",
        "    model = xgb.XGBClassifier(**params)\n",
        "\n",
        "    # Perform cross-validation with pruning callback\n",
        "    from sklearn.model_selection import KFold\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
        "        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "        # Train model\n",
        "        model.fit(X_fold_train, y_fold_train)\n",
        "\n",
        "        # Evaluate\n",
        "        fold_score = model.score(X_fold_val, y_fold_val)\n",
        "        scores.append(fold_score)\n",
        "\n",
        "        # Report intermediate value for pruning\n",
        "        trial.report(fold_score, fold)\n",
        "\n",
        "        # Check if trial should be pruned\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Suppress optuna logs for cleaner output\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# Create study with pruning\n",
        "start_time = time.time()\n",
        "print(f\"Starting Optuna optimization with PRUNING and {N_TRIALS} trials...\")\n",
        "\n",
        "# Use MedianPruner for early stopping of unpromising trials\n",
        "study = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    sampler=optuna.samplers.TPESampler(seed=42),\n",
        "    pruner=optuna.pruners.MedianPruner(\n",
        "        n_startup_trials=10,  # Don't prune first 10 trials\n",
        "        n_warmup_steps=1,     # Start pruning after 1 CV fold\n",
        "        interval_steps=1      # Check for pruning after each fold\n",
        "    )\n",
        ")\n",
        "\n",
        "study.optimize(objective_with_pruning, n_trials=N_TRIALS)\n",
        "\n",
        "# Get best parameters and train final model\n",
        "optuna_best_params = study.best_params.copy()\n",
        "optuna_best_params['random_state'] = 42\n",
        "\n",
        "optuna_model = xgb.XGBClassifier(**optuna_best_params)\n",
        "optuna_model.fit(X_train, y_train)\n",
        "optuna_predictions = optuna_model.predict(X_test)\n",
        "optuna_test_accuracy = accuracy_score(y_test, optuna_predictions)\n",
        "optuna_time = time.time() - start_time\n",
        "\n",
        "# Count pruned trials\n",
        "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
        "completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
        "\n",
        "print(f\"Optimization completed!\")\n",
        "print(f\"✓ Total trials: {len(study.trials)}\")\n",
        "print(f\"✓ Completed trials: {len(completed_trials)}\")\n",
        "print(f\"✓ Pruned trials: {len(pruned_trials)} ({len(pruned_trials)/len(study.trials)*100:.1f}%)\")\n",
        "print(f\"✓ Time saved by pruning: ~{len(pruned_trials) * (optuna_time/len(completed_trials)) * 0.5:.1f}s\")\n",
        "print(f\"Best Parameters Found: {len(study.best_params)} parameters optimized\")\n",
        "print(f\"Best Cross-validation Accuracy: {study.best_value:.4f}\")\n",
        "print(f\"Test Accuracy: {optuna_test_accuracy:.4f}\")\n",
        "print(f\"Total Optimization Time: {optuna_time:.1f} seconds ({optuna_time/60:.1f} minutes)\")\n",
        "print(f\"Average time per completed trial: {optuna_time/len(completed_trials):.1f} seconds\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ========================\n",
        "# 4. OPTUNA VISUALIZATIONS\n",
        "# ========================\n",
        "print(\"4. GENERATING OPTUNA VISUALIZATIONS\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Create a figure with multiple subplots for Optuna visualizations\n",
        "fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "# 1. Optimization History\n",
        "plt.subplot(2, 3, 1)\n",
        "try:\n",
        "    # Extract values for plotting\n",
        "    trial_numbers = [t.number for t in study.trials if t.value is not None]\n",
        "    trial_values = [t.value for t in study.trials if t.value is not None]\n",
        "\n",
        "    plt.plot(trial_numbers, trial_values, alpha=0.6, linewidth=1, label='Trial scores')\n",
        "\n",
        "    # Plot best score progression\n",
        "    best_scores = []\n",
        "    current_best = -1\n",
        "    for val in trial_values:\n",
        "        if val > current_best:\n",
        "            current_best = val\n",
        "        best_scores.append(current_best)\n",
        "\n",
        "    plt.plot(trial_numbers, best_scores, 'r-', linewidth=2, label='Best score')\n",
        "    plt.axhline(y=manual_cv_mean, color='orange', linestyle='--', alpha=0.7, label=f'Manual baseline ({manual_cv_mean:.4f})')\n",
        "    plt.xlabel('Trial Number')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Optimization History')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "except Exception as e:\n",
        "    plt.text(0.5, 0.5, f'Optimization History\\nError: {str(e)}', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "\n",
        "# 2. Parameter Importances\n",
        "plt.subplot(2, 3, 2)\n",
        "try:\n",
        "    param_importance = optuna.importance.get_param_importances(study)\n",
        "    params = list(param_importance.keys())[:8]  # Top 8 parameters\n",
        "    importances = [param_importance[p] for p in params]\n",
        "\n",
        "    y_pos = np.arange(len(params))\n",
        "    bars = plt.barh(y_pos, importances, color=plt.cm.viridis(np.linspace(0, 1, len(params))))\n",
        "    plt.yticks(y_pos, [p.replace('_', '\\n') for p in params])\n",
        "    plt.xlabel('Importance')\n",
        "    plt.title('Parameter Importance')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for i, (bar, imp) in enumerate(zip(bars, importances)):\n",
        "        plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,\n",
        "                f'{imp:.3f}', ha='left', va='center', fontsize=9)\n",
        "except Exception as e:\n",
        "    plt.text(0.5, 0.5, f'Parameter Importance\\nError: {str(e)}', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "\n",
        "# 3. Parallel Coordinate Plot (for top parameters)\n",
        "plt.subplot(2, 3, 3)\n",
        "try:\n",
        "    # Get top 4 most important parameters for cleaner visualization\n",
        "    top_params = list(optuna.importance.get_param_importances(study).keys())[:4]\n",
        "\n",
        "    # Get data for completed trials only\n",
        "    completed_trials_data = []\n",
        "    for trial in study.trials:\n",
        "        if trial.state == optuna.trial.TrialState.COMPLETE:\n",
        "            trial_data = [trial.params[p] for p in top_params]\n",
        "            trial_data.append(trial.value)\n",
        "            completed_trials_data.append(trial_data)\n",
        "\n",
        "    if completed_trials_data:\n",
        "        data_array = np.array(completed_trials_data)\n",
        "\n",
        "        # Normalize parameters for visualization\n",
        "        for i in range(len(top_params)):\n",
        "            col = data_array[:, i]\n",
        "            col_min, col_max = col.min(), col.max()\n",
        "            if col_max > col_min:\n",
        "                data_array[:, i] = (col - col_min) / (col_max - col_min)\n",
        "\n",
        "        # Plot lines\n",
        "        for i, row in enumerate(data_array):\n",
        "            color_intensity = row[-1]  # Use accuracy for coloring\n",
        "            plt.plot(range(len(top_params)), row[:-1], alpha=0.6,\n",
        "                    color=plt.cm.viridis(color_intensity), linewidth=1)\n",
        "\n",
        "        plt.xticks(range(len(top_params)), [p.replace('_', '\\n') for p in top_params])\n",
        "        plt.ylabel('Normalized Value')\n",
        "        plt.title(f'Parallel Coordinates\\n(Top {len(top_params)} Parameters)')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'No completed trials', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "except Exception as e:\n",
        "    plt.text(0.5, 0.5, f'Parallel Coordinates\\nError: {str(e)}', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "\n",
        "# 4. Pruning Analysis\n",
        "plt.subplot(2, 3, 4)\n",
        "try:\n",
        "    trial_states = [t.state.name for t in study.trials]\n",
        "    state_counts = pd.Series(trial_states).value_counts()\n",
        "\n",
        "    colors = ['#2ecc71', '#e74c3c', '#f39c12']  # Green, Red, Orange\n",
        "    wedges, texts, autotexts = plt.pie(state_counts.values, labels=state_counts.index,\n",
        "                                      autopct='%1.1f%%', colors=colors[:len(state_counts)])\n",
        "    plt.title(f'Trial States\\n({len(study.trials)} total trials)')\n",
        "\n",
        "    # Make percentage text more readable\n",
        "    for autotext in autotexts:\n",
        "        autotext.set_color('white')\n",
        "        autotext.set_weight('bold')\n",
        "except Exception as e:\n",
        "    plt.text(0.5, 0.5, f'Trial States\\nError: {str(e)}', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "\n",
        "# 5. Performance Distribution\n",
        "plt.subplot(2, 3, 5)\n",
        "try:\n",
        "    completed_values = [t.value for t in study.trials if t.value is not None]\n",
        "\n",
        "    plt.hist(completed_values, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    plt.axvline(study.best_value, color='red', linestyle='--', linewidth=2, label=f'Best: {study.best_value:.4f}')\n",
        "    plt.axvline(manual_cv_mean, color='orange', linestyle='--', linewidth=2, label=f'Manual: {manual_cv_mean:.4f}')\n",
        "    plt.axvline(np.mean(completed_values), color='green', linestyle='-', linewidth=2, label=f'Mean: {np.mean(completed_values):.4f}')\n",
        "    plt.xlabel('Accuracy')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Score Distribution')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "except Exception as e:\n",
        "    plt.text(0.5, 0.5, f'Score Distribution\\nError: {str(e)}', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "\n",
        "# 6. Time vs Performance\n",
        "plt.subplot(2, 3, 6)\n",
        "try:\n",
        "    trial_durations = []\n",
        "    trial_values_for_time = []\n",
        "\n",
        "    for trial in study.trials:\n",
        "        if trial.value is not None and trial.duration is not None:\n",
        "            trial_durations.append(trial.duration.total_seconds())\n",
        "            trial_values_for_time.append(trial.value)\n",
        "\n",
        "    if trial_durations:\n",
        "        scatter = plt.scatter(trial_durations, trial_values_for_time,\n",
        "                            c=trial_values_for_time, cmap='viridis', alpha=0.6, s=30)\n",
        "        plt.colorbar(scatter, label='Accuracy')\n",
        "        plt.xlabel('Trial Duration (seconds)')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Time vs Performance')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Highlight best trial\n",
        "        best_trial = study.best_trial\n",
        "        if best_trial.duration:\n",
        "            plt.scatter([best_trial.duration.total_seconds()], [best_trial.value],\n",
        "                       color='red', s=100, marker='*', label='Best Trial')\n",
        "            plt.legend()\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'No timing data available', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "except Exception as e:\n",
        "    plt.text(0.5, 0.5, f'Time vs Performance\\nError: {str(e)}', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Optuna Optimization Analysis', fontsize=16, y=0.98)\n",
        "plt.show()\n",
        "\n",
        "# ========================\n",
        "# 5. MANUAL vs OPTUNA COMPARISON\n",
        "# ========================\n",
        "print(\"\\n5. MANUAL vs OPTUNA PARAMETER COMPARISON\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Create comparison visualization\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Parameter comparison radar chart\n",
        "try:\n",
        "    # Select key parameters that exist in both\n",
        "    common_params = []\n",
        "    manual_values = []\n",
        "    optuna_values = []\n",
        "\n",
        "    for param in ['max_depth', 'learning_rate', 'n_estimators', 'subsample',\n",
        "                  'reg_alpha', 'reg_lambda', 'gamma', 'scale_pos_weight']:\n",
        "        if param in manual_params and param in optuna_best_params:\n",
        "            common_params.append(param)\n",
        "            # Normalize values for comparison\n",
        "            manual_val = manual_params[param]\n",
        "            optuna_val = optuna_best_params[param]\n",
        "\n",
        "            # Get parameter range for normalization\n",
        "            if param in PARAM_RANGES:\n",
        "                param_min, param_max = PARAM_RANGES[param]\n",
        "                manual_norm = (manual_val - param_min) / (param_max - param_min)\n",
        "                optuna_norm = (optuna_val - param_min) / (param_max - param_min)\n",
        "            else:\n",
        "                manual_norm = manual_val\n",
        "                optuna_norm = optuna_val\n",
        "\n",
        "            manual_values.append(manual_norm)\n",
        "            optuna_values.append(optuna_norm)\n",
        "\n",
        "    # Create radar chart\n",
        "    angles = np.linspace(0, 2 * np.pi, len(common_params), endpoint=False)\n",
        "    angles = np.concatenate((angles, [angles[0]]))  # Complete the circle\n",
        "\n",
        "    manual_values_plot = manual_values + [manual_values[0]]\n",
        "    optuna_values_plot = optuna_values + [optuna_values[0]]\n",
        "\n",
        "    ax1.plot(angles, manual_values_plot, 'o-', linewidth=2, label='Manual', color='orange')\n",
        "    ax1.fill(angles, manual_values_plot, alpha=0.25, color='orange')\n",
        "    ax1.plot(angles, optuna_values_plot, 'o-', linewidth=2, label='Optuna', color='blue')\n",
        "    ax1.fill(angles, optuna_values_plot, alpha=0.25, color='blue')\n",
        "\n",
        "    ax1.set_xticks(angles[:-1])\n",
        "    ax1.set_xticklabels([p.replace('_', '\\n') for p in common_params])\n",
        "    ax1.set_ylim(0, 1)\n",
        "    ax1.set_title('Parameter Comparison\\n(Normalized Values)', fontsize=12, pad=20)\n",
        "    ax1.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
        "    ax1.grid(True)\n",
        "\n",
        "except Exception as e:\n",
        "    ax1.text(0.5, 0.5, f'Parameter Radar Chart\\nError: {str(e)}', ha='center', va='center', transform=ax1.transAxes)\n",
        "\n",
        "# Performance comparison bar chart\n",
        "ax2.bar(['Manual\\nExpert', 'Random\\nSearch', 'Optuna\\nPruning'],\n",
        "        [manual_test_accuracy, random_test_accuracy, optuna_test_accuracy],\n",
        "        color=['orange', 'gray', 'blue'], alpha=0.7)\n",
        "ax2.set_ylabel('Test Accuracy')\n",
        "ax2.set_title('Final Performance Comparison')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (method, acc) in enumerate(zip(['Manual', 'Random', 'Optuna'],\n",
        "                                    [manual_test_accuracy, random_test_accuracy, optuna_test_accuracy])):\n",
        "    ax2.text(i, acc + 0.0005, f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Time comparison\n",
        "times = [manual_time/60, random_time/60, optuna_time/60]\n",
        "ax3.bar(['Manual\\n(1 trial)', 'Random\\n({} trials)'.format(N_TRIALS),\n",
        "         'Optuna\\n({} trials)'.format(N_TRIALS)],\n",
        "        times, color=['orange', 'gray', 'blue'], alpha=0.7)\n",
        "ax3.set_ylabel('Time (minutes)')\n",
        "ax3.set_title('Optimization Time Comparison')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, t in enumerate(times):\n",
        "    ax3.text(i, t + max(times) * 0.01, f'{t:.1f}m', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Efficiency plot (accuracy improvement vs time)\n",
        "efficiency_data = {\n",
        "    'Method': ['Manual', 'Random Search', 'Optuna'],\n",
        "    'Accuracy': [manual_test_accuracy, random_test_accuracy, optuna_test_accuracy],\n",
        "    'Time (min)': [manual_time/60, random_time/60, optuna_time/60],\n",
        "    'Improvement': [0,\n",
        "                   (random_test_accuracy - manual_test_accuracy) * 100,\n",
        "                   (optuna_test_accuracy - manual_test_accuracy) * 100]\n",
        "}\n",
        "\n",
        "colors = ['orange', 'gray', 'blue']\n",
        "sizes = [50, 100, 100]  # Manual gets smaller dot since it's just 1 trial\n",
        "\n",
        "scatter = ax4.scatter(efficiency_data['Time (min)'], efficiency_data['Improvement'],\n",
        "                     c=colors, s=sizes, alpha=0.7)\n",
        "ax4.set_xlabel('Time (minutes)')\n",
        "ax4.set_ylabel('Accuracy Improvement (%)')\n",
        "ax4.set_title('Efficiency: Improvement vs Time')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Add method labels\n",
        "for i, method in enumerate(efficiency_data['Method']):\n",
        "    ax4.annotate(method,\n",
        "                (efficiency_data['Time (min)'][i], efficiency_data['Improvement'][i]),\n",
        "                xytext=(5, 5), textcoords='offset points', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Manual vs Optuna Comprehensive Comparison', fontsize=16, y=0.98)\n",
        "plt.show()\n",
        "\n",
        "# ========================\n",
        "# 6. COMPREHENSIVE COMPARISON\n",
        "# ========================\n",
        "print(\"\\n6. COMPREHENSIVE COMPARISON WITH PRUNING\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Create detailed comparison\n",
        "comparison_data = {\n",
        "    'Method': ['Manual (Expert)', 'RandomSearch', 'Optuna + Pruning'],\n",
        "    'CV Accuracy': [manual_cv_mean, random_cv_score, study.best_value],\n",
        "    'Test Accuracy': [manual_test_accuracy, random_test_accuracy, optuna_test_accuracy],\n",
        "    'Time (min)': [manual_time/60, random_time/60, optuna_time/60],\n",
        "    'Trials': [1, N_TRIALS, len(completed_trials)],\n",
        "    'Pruned': [0, 0, len(pruned_trials)],\n",
        "    'Efficiency': [manual_test_accuracy/(manual_time/60),\n",
        "                   random_test_accuracy/(random_time/60),\n",
        "                   optuna_test_accuracy/(optuna_time/60)]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "print(f\"\\nPRUNING EFFECTIVENESS:\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"✓ Total trials attempted: {len(study.trials)}\")\n",
        "print(f\"✓ Trials completed: {len(completed_trials)} ({len(completed_trials)/len(study.trials)*100:.1f}%)\")\n",
        "print(f\"✓ Trials pruned: {len(pruned_trials)} ({len(pruned_trials)/len(study.trials)*100:.1f}%)\")\n",
        "if len(completed_trials) > 0:\n",
        "    estimated_time_without_pruning = optuna_time * len(study.trials) / len(completed_trials)\n",
        "    time_saved = estimated_time_without_pruning - optuna_time\n",
        "    print(f\"✓ Estimated time saved: {time_saved:.1f} seconds ({time_saved/60:.1f} minutes)\")\n",
        "    print(f\"✓ Speed improvement: {estimated_time_without_pruning/optuna_time:.1f}x faster\")\n",
        "\n",
        "print(f\"\\nKEY INSIGHTS:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Compare against manual baseline\n",
        "random_improvement = ((random_test_accuracy - manual_test_accuracy) / manual_test_accuracy) * 100\n",
        "optuna_improvement = ((optuna_test_accuracy - manual_test_accuracy) / manual_test_accuracy) * 100\n",
        "\n",
        "print(f\"RandomSearch vs Manual: {random_improvement:+.2f}% accuracy improvement\")\n",
        "print(f\"Optuna vs Manual: {optuna_improvement:+.2f}% accuracy improvement\")\n",
        "\n",
        "# Compare RandomSearch vs Optuna (THE KEY COMPARISON)\n",
        "optuna_vs_random = ((optuna_test_accuracy - random_test_accuracy) / random_test_accuracy) * 100\n",
        "print(f\"Optuna vs RandomSearch: {optuna_vs_random:+.2f}% accuracy improvement\")\n",
        "\n",
        "# Statistical significance on this large dataset\n",
        "accuracy_diff = abs(optuna_test_accuracy - random_test_accuracy)\n",
        "print(f\"Absolute accuracy difference: {accuracy_diff:.4f}\")\n",
        "print(f\"On {len(y_test):,} test samples, this is {int(accuracy_diff * len(y_test))} fewer/more correct predictions\")\n",
        "\n",
        "# ========================\n",
        "# 7. PARAMETER INSIGHTS WITH PRUNING\n",
        "# ========================\n",
        "print(f\"\\n7. ENHANCED PARAMETER INSIGHTS\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"TOP 5 MOST IMPORTANT PARAMETERS:\")\n",
        "param_importance = optuna.importance.get_param_importances(study)\n",
        "for i, (param, importance) in enumerate(sorted(param_importance.items(), key=lambda x: x[1], reverse=True)[:5]):\n",
        "    print(f\"{i+1}. {param:20}: {importance:.3f} importance\")\n",
        "\n",
        "print(f\"\\nOPTIMAL vs MANUAL PARAMETERS (detailed comparison):\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Parameter':<20} {'Manual':<12} {'Optuna':<12} {'Difference':<15} {'% Change'}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "key_params = ['max_depth', 'learning_rate', 'n_estimators', 'subsample',\n",
        "              'reg_alpha', 'reg_lambda', 'gamma', 'scale_pos_weight']\n",
        "\n",
        "for param in key_params:\n",
        "    if param in optuna_best_params and param in manual_params:\n",
        "        manual_val = manual_params[param]\n",
        "        optimal_val = optuna_best_params[param]\n",
        "        diff = optimal_val - manual_val\n",
        "        pct_change = (diff / manual_val) * 100 if manual_val != 0 else float('inf')\n",
        "\n",
        "        print(f\"{param:<20} {manual_val:<12.3f} {optimal_val:<12.3f} {diff:<15.3f} {pct_change:>+7.1f}%\")\n",
        "\n",
        "# ========================\n",
        "# 8. TRIAL ANALYSIS\n",
        "# ========================\n",
        "print(f\"\\n8. DETAILED TRIAL ANALYSIS\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"OPTIMIZATION PROGRESSION:\")\n",
        "completed_values = [t.value for t in study.trials if t.value is not None]\n",
        "if completed_values:\n",
        "    print(f\"✓ Best score: {max(completed_values):.4f}\")\n",
        "    print(f\"✓ Worst score: {min(completed_values):.4f}\")\n",
        "    print(f\"✓ Mean score: {np.mean(completed_values):.4f}\")\n",
        "    print(f\"✓ Standard deviation: {np.std(completed_values):.4f}\")\n",
        "\n",
        "    # Find when best score was achieved\n",
        "    best_trial_number = None\n",
        "    for trial in study.trials:\n",
        "        if trial.value == study.best_value:\n",
        "            best_trial_number = trial.number\n",
        "            break\n",
        "\n",
        "    if best_trial_number is not None:\n",
        "        print(f\"✓ Best score found at trial: {best_trial_number + 1}/{len(study.trials)}\")\n",
        "        convergence_point = (best_trial_number + 1) / len(study.trials)\n",
        "        if convergence_point < 0.5:\n",
        "            print(f\"✓ Early convergence: Found best solution in first {convergence_point*100:.1f}% of trials\")\n",
        "        elif convergence_point < 0.8:\n",
        "            print(f\"✓ Good convergence: Found best solution at {convergence_point*100:.1f}% of trials\")\n",
        "        else:\n",
        "            print(f\"✓ Late convergence: Found best solution at {convergence_point*100:.1f}% of trials\")\n",
        "\n",
        "print(f\"\\nPRUNING ANALYSIS:\")\n",
        "if len(pruned_trials) > 0:\n",
        "    pruned_steps = [len(t.intermediate_values) for t in pruned_trials if hasattr(t, 'intermediate_values')]\n",
        "    if pruned_steps:\n",
        "        print(f\"✓ Average steps before pruning: {np.mean(pruned_steps):.1f}\")\n",
        "        print(f\"✓ Earliest pruning at step: {min(pruned_steps)}\")\n",
        "        print(f\"✓ Latest pruning at step: {max(pruned_steps)}\")\n",
        "\n",
        "    # Analyze pruning effectiveness\n",
        "    if completed_values:\n",
        "        pruning_threshold = np.median(completed_values)\n",
        "        print(f\"✓ Median score of completed trials: {pruning_threshold:.4f}\")\n",
        "        print(f\"✓ Pruning likely saved time on {len(pruned_trials)} unpromising trials\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(f\"EXPERIMENT SUMMARY:\")\n",
        "print(f\"Dataset: Forest Covertype ({X.shape[0]:,} samples, {X.shape[1]} features)\")\n",
        "print(f\"Task: Binary classification with class imbalance\")\n",
        "print(f\"Hyperparameters tuned: {len(PARAM_RANGES)}\")\n",
        "print(f\"Total optimization budget: {N_TRIALS} trials\")\n",
        "print(f\"Best method: {'Optuna' if optuna_test_accuracy >= max(manual_test_accuracy, random_test_accuracy) else 'Manual' if manual_test_accuracy >= random_test_accuracy else 'RandomSearch'}\")\n",
        "print(f\"Best accuracy: {max(manual_test_accuracy, random_test_accuracy, optuna_test_accuracy):.4f}\")\n",
        "print(f\"Pruning effectiveness: {len(pruned_trials)}/{len(study.trials)} trials pruned\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXyehk0vwddP",
        "outputId": "f61c8b26-81c5-4200-8d31-ca42e21d5713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Forest Covertype Dataset...\n",
            "This is a REAL complex dataset from UCI repository\n",
            "Predicting forest cover type from cartographic variables\n",
            "Original classes: [1 2 3 4 5 6 7] with distribution: [     0 211840 283301  35754   2747   9493  17367  20510]\n",
            "Binary classes distribution: [495141  85871]\n",
            "\n",
            "DATASET COMPLEXITY:\n",
            "- Samples: 581,012\n",
            "- Features: 54 (mix of continuous and categorical)\n",
            "- Train set: 464,809 samples\n",
            "- Test set: 116,203 samples\n",
            "- Class imbalance: 495,141 vs 85,871\n",
            "- Real-world geographic/environmental data with complex interactions\n",
            "================================================================================\n",
            "OPTIMIZATION CHALLENGE:\n",
            "- 12 hyperparameters to tune\n",
            "- Complex parameter interactions\n",
            "- Large dataset requiring longer training times\n",
            "- 100 trials budget\n",
            "- Real-world data with noise and complexity\n",
            "\n",
            "1. MANUAL HYPERPARAMETERS (Expert Intuition)\n",
            "--------------------------------------------------\n",
            "Manual Parameters: 13 parameters set\n",
            "Cross-validation Accuracy: 0.9850 (+/- 0.0006)\n",
            "Test Accuracy: 0.9850\n",
            "Training Time: 91.9 seconds\n",
            "Number of combinations tried: 1\n",
            "================================================================================\n",
            "2. RANDOMIZED SEARCH (Random Sampling)\n",
            "--------------------------------------------------\n",
            "Starting RandomizedSearchCV with 100 trials...\n"
          ]
        }
      ]
    }
  ]
}