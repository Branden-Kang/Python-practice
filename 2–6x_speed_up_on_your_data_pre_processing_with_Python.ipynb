{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2â€“6x speed-up on your data pre-processing with Python.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPgKfka7xbZuh186QrER2LB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypXLl0CG9IyN"
      },
      "source": [
        "[Reference](https://towardsdatascience.com/heres-how-you-can-get-a-2-6x-speed-up-on-your-data-pre-processing-with-python-847887e63be5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXOdccNV9EoT"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "import cv2\n",
        "import concurrent.futures\n",
        "\n",
        "\n",
        "def load_and_resize(image_filename):\n",
        "  ### Read in the image data\n",
        "  img = cv2.imread(image_filename)\n",
        "  \n",
        "  ### Resize the image\n",
        "  img = cv2.resize(img, (600, 600)) \n",
        "  \n",
        "\n",
        "### Create a pool of processes. By default, one is created for each CPU in your machine.\n",
        "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "    ### Get a list of files to process\n",
        "    image_files = glob.glob(\"*.jpg\")\n",
        "\n",
        "    ### Process the list of files, but split the work across the process pool to use all CPUs\n",
        "    ### Loop through all jpg files in the current folder \n",
        "    ### Resize each one to size 600x600\n",
        "    executor.map(load_and_resize, image_files)"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}