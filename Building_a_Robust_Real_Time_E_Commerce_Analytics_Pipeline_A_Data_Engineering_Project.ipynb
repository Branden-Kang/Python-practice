{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgnRHFUgK0dA36/oOoHttE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@simardeep.oberoi/building-a-robust-real-time-e-commerce-analytics-pipeline-a-data-engineering-project-b57db9e9bfc4)"
      ],
      "metadata": {
        "id": "9EqIfXJeBr8M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5kZ7dZXOBnql"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "from faker import Faker\n",
        "from kafka import KafkaProducer\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "\n",
        "fake = Faker()\n",
        "# Kafka configuration\n",
        "kafka_broker = os.getenv('KAFKA_BROKER', 'broker:9092')\n",
        "producer = KafkaProducer(bootstrap_servers=[kafka_broker],\n",
        "                         value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
        "customers = []\n",
        "products = []\n",
        "# Generate Customer Data\n",
        "def generate_customer():\n",
        "    customer = {\n",
        "        \"customer_id\": fake.uuid4(),\n",
        "        \"name\": fake.name(),\n",
        "        \"email\": fake.email(),\n",
        "        \"location\": fake.address(),\n",
        "        \"age\": random.randint(18, 70),\n",
        "        \"gender\": random.choice([\"Male\", \"Female\", \"Other\"]),\n",
        "        \"account_created\": fake.past_date().isoformat(),\n",
        "        \"last_login\": fake.date_time_this_month().isoformat()\n",
        "    }\n",
        "    customers.append(customer[\"customer_id\"])\n",
        "    return customer\n",
        "# Generate Product Data\n",
        "def generate_product():\n",
        "    categories = ['Electronics', 'Books', 'Clothing', 'Home & Garden']\n",
        "    product = {\n",
        "        \"product_id\": fake.uuid4(),\n",
        "        \"name\": fake.word().title(),\n",
        "        \"category\": random.choice(categories),\n",
        "        \"price\": round(random.uniform(10, 500), 2),\n",
        "        \"stock_quantity\": random.randint(0, 100),\n",
        "        \"supplier\": fake.company(),\n",
        "        \"rating\": round(random.uniform(1, 5), 1)\n",
        "    }\n",
        "    products.append(product[\"product_id\"])\n",
        "    return product\n",
        "# Generate Transaction Data\n",
        "def generate_transaction():\n",
        "    customer_id = random.choice(customers)\n",
        "    product_id = random.choice(products)\n",
        "    return {\n",
        "        \"transaction_id\": fake.uuid4(),\n",
        "        \"customer_id\": customer_id,\n",
        "        \"product_id\": product_id,\n",
        "        \"quantity\": random.randint(1, 5),\n",
        "        \"date_time\": fake.date_time_this_year().isoformat(),\n",
        "        \"status\": random.choice([\"completed\", \"pending\", \"canceled\"]),\n",
        "        \"payment_method\": random.choice([\"credit card\", \"PayPal\", \"bank transfer\"])\n",
        "    }\n",
        "# Generate Product View Data\n",
        "def generate_product_view():\n",
        "    return {\n",
        "        \"view_id\": fake.uuid4(),\n",
        "        \"customer_id\": random.choice(customers),\n",
        "        \"product_id\": random.choice(products),\n",
        "        \"timestamp\": fake.date_time_this_year().isoformat(),\n",
        "        \"view_duration\": random.randint(10, 300)  # Duration in seconds\n",
        "    }\n",
        "# Generate System Log Data\n",
        "def generate_system_log():\n",
        "    log_levels = [\"INFO\", \"WARNING\", \"ERROR\"]\n",
        "    return {\n",
        "        \"log_id\": fake.uuid4(),\n",
        "        \"timestamp\": fake.date_time_this_year().isoformat(),\n",
        "        \"level\": random.choice(log_levels),\n",
        "        \"message\": fake.sentence()\n",
        "    }\n",
        "# Generate User Interaction Data\n",
        "def generate_user_interaction():\n",
        "    interaction_types = [\"wishlist_addition\", \"review\", \"rating\"]\n",
        "    return {\n",
        "        \"interaction_id\": fake.uuid4(),\n",
        "        \"customer_id\": random.choice(customers),\n",
        "        \"product_id\": random.choice(products),\n",
        "        \"timestamp\": fake.date_time_this_year().isoformat(),\n",
        "        \"interaction_type\": random.choice(interaction_types),\n",
        "        \"details\": fake.sentence() if interaction_types == \"review\" else None\n",
        "    }\n",
        "# Function to send data to Kafka\n",
        "def send_data():\n",
        "    # Occasionally add new customers or products\n",
        "    if random.random() < 0.5:\n",
        "        customer = generate_customer()\n",
        "        producer.send('ecommerce_customers', value=customer)\n",
        "    else:\n",
        "        product = generate_product()\n",
        "        producer.send('ecommerce_products', value=product)\n",
        "    # Higher chance to create transactions and interactions\n",
        "    if customers and products:\n",
        "        transaction = generate_transaction()\n",
        "        producer.send('ecommerce_transactions', value=transaction)\n",
        "        product_view = generate_product_view()\n",
        "        if product_view:\n",
        "            producer.send('ecommerce_product_views', value=product_view)\n",
        "        user_interaction = generate_user_interaction()\n",
        "        if user_interaction:\n",
        "            producer.send('ecommerce_user_interactions', value=user_interaction)\n",
        "    producer.send('ecommerce_system_logs', value=generate_system_log())\n",
        "# Parallel Data Generation\n",
        "with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "    while True:\n",
        "        executor.submit(send_data)\n",
        "        time.sleep(random.uniform(0.01, 0.1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
        "from pyspark.sql import DataFrame\n",
        "import logging\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\\\n",
        "    .appName(\"Ecommerce Data Analysis\") \\\\\n",
        "    .config(\"spark.es.nodes\", \"elasticsearch\") \\\\\n",
        "    .config(\"spark.es.port\", \"9200\") \\\\\n",
        "    .config(\"spark.es.nodes.wan.only\", \"true\") \\\\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "# Kafka configuration\n",
        "kafka_bootstrap_servers = \"broker:29092,broker2:29094\"\n",
        "\n",
        "\n",
        "customerSchema = StructType([\n",
        "    StructField(\"customer_id\", StringType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"email\", StringType(), True),\n",
        "    StructField(\"location\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"gender\", StringType(), True),\n",
        "    StructField(\"account_created\", StringType(), True),\n",
        "    StructField(\"last_login\", TimestampType(), True)\n",
        "])\n",
        "customerDF = (spark.readStream\n",
        "              .format(\"kafka\")\n",
        "              .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers)\n",
        "              .option(\"subscribe\", \"ecommerce_customers\")\n",
        "              .option(\"startingOffsets\", \"earliest\")  # Start from the earliest records\n",
        "              .load()\n",
        "              .selectExpr(\"CAST(value AS STRING)\")\n",
        "              .select(from_json(\"value\", customerSchema).alias(\"data\"))\n",
        "              .select(\"data.*\")\n",
        "              .withWatermark(\"last_login\", \"2 hours\")\n",
        "             )\n",
        "\n",
        "\n",
        "# Read data from 'ecommerce_products' topic\n",
        "productSchema = StructType([\n",
        "    StructField(\"product_id\", StringType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"category\", StringType(), True),\n",
        "    StructField(\"price\", DoubleType(), True),\n",
        "    StructField(\"stock_quantity\", IntegerType(), True),\n",
        "    StructField(\"supplier\", StringType(), True),\n",
        "    StructField(\"rating\", DoubleType(), True)\n",
        "])\n",
        "productDF = spark.readStream \\\\\n",
        "    .format(\"kafka\") \\\\\n",
        "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\\\n",
        "    .option(\"subscribe\", \"ecommerce_products\") \\\\\n",
        "    .option(\"startingOffsets\", \"earliest\") \\\\\n",
        "    .load() \\\\\n",
        "    .selectExpr(\"CAST(value AS STRING)\") \\\\\n",
        "    .select(from_json(\"value\", productSchema).alias(\"data\")) \\\\\n",
        "    .select(\"data.*\") \\\\\n",
        "    .withColumn(\"processingTime\", current_timestamp())  # Add processing timestamp\n",
        "productDF = productDF.withWatermark(\"processingTime\", \"2 hours\")\n",
        "\n",
        "\n",
        "# Read data from 'ecommerce_transactions' topic\n",
        "transactionSchema = StructType([\n",
        "    StructField(\"transaction_id\", StringType(), True),\n",
        "    StructField(\"customer_id\", StringType(), True),\n",
        "    StructField(\"product_id\", StringType(), True),\n",
        "    StructField(\"quantity\", IntegerType(), True),\n",
        "    StructField(\"date_time\", TimestampType(), True),\n",
        "    StructField(\"status\", StringType(), True),\n",
        "    StructField(\"payment_method\", StringType(), True)\n",
        "])\n",
        "transactionDF = spark.readStream \\\\\n",
        "    .format(\"kafka\") \\\\\n",
        "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\\\n",
        "    .option(\"subscribe\", \"ecommerce_transactions\") \\\\\n",
        "    .option(\"startingOffsets\", \"earliest\") \\\\\n",
        "    .load() \\\\\n",
        "    .selectExpr(\"CAST(value AS STRING)\") \\\\\n",
        "    .select(from_json(\"value\", transactionSchema).alias(\"data\")) \\\\\n",
        "    .select(\"data.*\")\n",
        "transactionDF = transactionDF.withColumn(\"processingTime\", current_timestamp())\n",
        "transactionDF = transactionDF.withWatermark(\"processingTime\", \"2 hours\")\n",
        "\n",
        "\n",
        "# Read data from 'ecommerce_product_views' topic\n",
        "productViewSchema = StructType([\n",
        "    StructField(\"view_id\", StringType(), True),\n",
        "    StructField(\"customer_id\", StringType(), True),\n",
        "    StructField(\"product_id\", StringType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"view_duration\", IntegerType(), True)\n",
        "])\n",
        "productViewDF = (spark.readStream\n",
        "                 .format(\"kafka\")\n",
        "                 .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers)\n",
        "                 .option(\"subscribe\", \"ecommerce_product_views\")\n",
        "                 .option(\"startingOffsets\", \"earliest\")\n",
        "                 .load()\n",
        "                 .selectExpr(\"CAST(value AS STRING)\")\n",
        "                 .select(from_json(\"value\", productViewSchema).alias(\"data\"))\n",
        "                 .select(\"data.*\")\n",
        "                 .withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
        "                 .withWatermark(\"timestamp\", \"1 hour\")\n",
        "                 )\n",
        "productViewDF = productViewDF.withColumn(\"processingTime\", current_timestamp())\n",
        "productViewDF = productViewDF.withWatermark(\"processingTime\", \"2 hours\")\n",
        "\n",
        "\n",
        "# Read data from 'ecommerce_system_logs' topic\n",
        "systemLogSchema = StructType([\n",
        "    StructField(\"log_id\", StringType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"level\", StringType(), True),\n",
        "    StructField(\"message\", StringType(), True)\n",
        "])\n",
        "systemLogDF = spark.readStream \\\\\n",
        "    .format(\"kafka\") \\\\\n",
        "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\\\n",
        "    .option(\"subscribe\", \"ecommerce_system_logs\") \\\\\n",
        "    .option(\"startingOffsets\", \"earliest\") \\\\\n",
        "    .load() \\\\\n",
        "    .selectExpr(\"CAST(value AS STRING)\") \\\\\n",
        "    .select(from_json(\"value\", systemLogSchema).alias(\"data\")) \\\\\n",
        "    .select(\"data.*\")\n",
        "systemLogDF = systemLogDF.withColumn(\"processingTime\", current_timestamp())\n",
        "systemLogDF = systemLogDF.withWatermark(\"processingTime\", \"2 hours\")\n",
        "\n",
        "\n",
        "# Read data from 'ecommerce_user_interactions' topic\n",
        "userInteractionSchema = StructType([\n",
        "    StructField(\"interaction_id\", StringType(), True),\n",
        "    StructField(\"customer_id\", StringType(), True),\n",
        "    StructField(\"product_id\", StringType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"interaction_type\", StringType(), True),\n",
        "    StructField(\"details\", StringType(), True)\n",
        "])\n",
        "userInteractionDF = spark.readStream \\\\\n",
        "    .format(\"kafka\") \\\\\n",
        "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\\\n",
        "    .option(\"subscribe\", \"ecommerce_user_interactions\") \\\\\n",
        "    .option(\"startingOffsets\", \"earliest\") \\\\\n",
        "    .load() \\\\\n",
        "    .selectExpr(\"CAST(value AS STRING)\") \\\\\n",
        "    .select(from_json(\"value\", userInteractionSchema).alias(\"data\")) \\\\\n",
        "    .select(\"data.*\")\n",
        "userInteractionDF = userInteractionDF.withColumn(\"processingTime\", current_timestamp())\n",
        "userInteractionDF = userInteractionDF.withWatermark(\"processingTime\", \"2 hours\")\n",
        "\n",
        "\n",
        "#This analysis  focus on demographics and account activity.\n",
        "customerAnalysisDF = (customerDF\n",
        "                      .groupBy(\n",
        "                          window(col(\"last_login\"), \"1 day\"),  # Windowing based on last_login\n",
        "                          \"gender\"\n",
        "                      )\n",
        "                      .agg(\n",
        "                          count(\"customer_id\").alias(\"total_customers\"),\n",
        "                          max(\"last_login\").alias(\"last_activity\")\n",
        "                      )\n",
        "                     )\n",
        "# Analyzing product popularity and stock status with windowing\n",
        "productAnalysisDF = productDF \\\\\n",
        "    .groupBy(\n",
        "        window(col(\"processingTime\"), \"1 hour\"),  # Window based on processingTime\n",
        "        \"category\"\n",
        "    ) \\\\\n",
        "    .agg(\n",
        "        avg(\"price\").alias(\"average_price\"),\n",
        "        sum(\"stock_quantity\").alias(\"total_stock\")\n",
        "    ) \\\\\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        col(\"category\"),\n",
        "        col(\"average_price\"),\n",
        "        col(\"total_stock\")\n",
        "    )\n",
        "\n",
        "\n",
        "#Analyzing sales data\n",
        "salesAnalysisDF = transactionDF \\\\\n",
        "    .groupBy(\n",
        "        window(col(\"processingTime\"), \"1 hour\"),  # Window based on processingTime\n",
        "        \"product_id\"\n",
        "    ) \\\\\n",
        "    .agg(\n",
        "        count(\"transaction_id\").alias(\"number_of_sales\"),\n",
        "        sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
        "        approx_count_distinct(\"customer_id\").alias(\"unique_customers\")  # Use approx_count_distinct\n",
        "    ) \\\\\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        col(\"product_id\"),\n",
        "        col(\"number_of_sales\"),\n",
        "        col(\"total_quantity_sold\"),\n",
        "        col(\"unique_customers\")\n",
        "    )\n",
        "\n",
        "\n",
        "# Understanding customer interest in products.\n",
        "productViewsAnalysisDF = productViewDF \\\\\n",
        "    .withWatermark(\"timestamp\", \"2 hours\") \\\\\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"1 hour\"),\n",
        "        \"product_id\"\n",
        "    ) \\\\\n",
        "    .agg(\n",
        "        count(\"view_id\").alias(\"total_views\"),\n",
        "        avg(\"view_duration\").alias(\"average_view_duration\")\n",
        "    ) \\\\\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        col(\"product_id\"),\n",
        "        col(\"total_views\"),\n",
        "        col(\"average_view_duration\")\n",
        "    )\n",
        "\n",
        "\n",
        "# User Interaction Analysis\n",
        "interactionAnalysisDF = userInteractionDF \\\\\n",
        "    .withWatermark(\"timestamp\", \"2 hours\") \\\\\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"1 hour\"),\n",
        "        \"interaction_type\"\n",
        "    ) \\\\\n",
        "    .agg(\n",
        "        count(\"interaction_id\").alias(\"total_interactions\"),\n",
        "        approx_count_distinct(\"customer_id\").alias(\"unique_users_interacted\")  # Use approx_count_distinct\n",
        "    ) \\\\\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        col(\"interaction_type\"),\n",
        "        col(\"total_interactions\"),\n",
        "        col(\"unique_users_interacted\")\n",
        "    )\n",
        "\n",
        "\n",
        "def writeToElasticsearch(df, index_name):\n",
        "    def write_and_log(batch_df: DataFrame, batch_id: int):\n",
        "        logger.info(f\"Attempting to write batch {batch_id} to Elasticsearch index {index_name}.\")\n",
        "        try:\n",
        "            if not batch_df.isEmpty():\n",
        "                logger.info(f\"Batch {batch_id} has data. Writing to Elasticsearch.\")\n",
        "                batch_df.write \\\\\n",
        "                    .format(\"org.elasticsearch.spark.sql\") \\\\\n",
        "                    .option(\"checkpointLocation\", f\"/opt/bitnami/spark/checkpoint/{index_name}/{batch_id}\") \\\\\n",
        "                    .option(\"es.resource\", f\"{index_name}/doc\") \\\\\n",
        "                    .option(\"es.nodes\", \"elasticsearch\") \\\\\n",
        "                    .option(\"es.port\", \"9200\") \\\\\n",
        "                    .option(\"es.nodes.wan.only\", \"true\") \\\\\n",
        "                    .save()\n",
        "                logger.info(f\"Batch {batch_id} written successfully.\")\n",
        "            else:\n",
        "                logger.info(f\"Batch {batch_id} is empty. Skipping write.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error writing batch {batch_id} to Elasticsearch: {e}\")\n",
        "    return df.writeStream \\\\\n",
        "             .outputMode(\"append\") \\\\\n",
        "             .foreachBatch(write_and_log) \\\\\n",
        "             .start()\n",
        "writeToElasticsearch(customerAnalysisDF, \"customer_analysis\")\n",
        "writeToElasticsearch(productAnalysisDF, \"product_analysis\")\n",
        "writeToElasticsearch(salesAnalysisDF, \"sales_analysis\")\n",
        "writeToElasticsearch(productViewsAnalysisDF, \"product_views_analysis\")\n",
        "writeToElasticsearch(interactionAnalysisDF, \"interaction_analysis\")\n",
        "spark.streams.awaitAnyTermination()# Initialize logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "QtvbnBKmBv4G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "version: \"3.8\"\n",
        "services:\n",
        "  ####################\n",
        "  # Python Data generator\n",
        "  ####################\n",
        "  data-generator:\n",
        "    build: ./python-data-generator\n",
        "    container_name: data-generator\n",
        "    depends_on:\n",
        "      - broker\n",
        "      - zookeeper\n",
        "      - init-kafka\n",
        "      # - schema-registry\n",
        "      # - connect\n",
        "    networks:\n",
        "      - backend\n",
        "    environment:\n",
        "      - KAFKA_BROKER=broker:${BROKER_INTERNAL_PORT}\n",
        "    restart: on-failure\n",
        "  ####################\n",
        "  # Elasticsearch\n",
        "  ####################\n",
        "  elasticsearch:\n",
        "    image: docker.elastic.co/elasticsearch/elasticsearch:7.13.2\n",
        "    container_name: elasticsearch\n",
        "    environment:\n",
        "      - \"discovery.type=single-node\"\n",
        "      - ES_JAVA_OPTS=-Xms4g -Xmx4g\n",
        "    ports:\n",
        "      - \"9200:9200\"\n",
        "    networks:\n",
        "      - backend\n",
        "    volumes:\n",
        "      - $PWD/esdata1:/usr/share/elasticsearch/data\n",
        "    healthcheck:\n",
        "      test: [\"CMD-SHELL\", \"curl --silent --fail localhost:9200/_cluster/health || exit 1\"]\n",
        "      interval: 30s\n",
        "      timeout: 10s\n",
        "      retries: 3\n",
        "    restart: unless-stopped\n",
        "  ####################\n",
        "  # Kibana\n",
        "  ####################\n",
        "  kibana:\n",
        "    image: docker.elastic.co/kibana/kibana:7.13.2\n",
        "    container_name: kibana\n",
        "    ports:\n",
        "      - \"5601:5601\"\n",
        "    environment:\n",
        "      - ELASTICSEARCH_URL=http://elasticsearch:9200\n",
        "      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200\n",
        "    depends_on:\n",
        "      - elasticsearch\n",
        "    networks:\n",
        "      - backend\n",
        "  ####################\n",
        "  # Apache Spark Master Node\n",
        "  ####################\n",
        "  spark_master:\n",
        "    image: bitnami/spark:3\n",
        "    command: /opt/bitnami/spark/bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1,org.elasticsearch:elasticsearch-spark-30_2.12:7.13.1 /opt/bitnami/spark/app/spark-processing.py\n",
        "    container_name: spark_master\n",
        "    ports:\n",
        "      - \"8077:8080\"\n",
        "    environment:\n",
        "      - SPARK_MODE=master\n",
        "      - SPARK_RPC_AUTHENTICATION_ENABLED=no\n",
        "      - SPARK_RPC_ENCRYPTION_ENABLED=no\n",
        "      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n",
        "      - SPARK_SSL_ENABLED=no\n",
        "    volumes:\n",
        "      - $PWD/spark-processing:/opt/bitnami/spark/app\n",
        "      - $PWD/spark-checkpoint:/opt/bitnami/spark/checkpoint\n",
        "    networks:\n",
        "      - backend\n",
        "    depends_on:\n",
        "      - zookeeper\n",
        "      - broker\n",
        "      - elasticsearch\n",
        "      - data-generator\n",
        "  ####################\n",
        "  # zookeeper\n",
        "  ####################\n",
        "  zookeeper:\n",
        "    image: confluentinc/cp-zookeeper:${KAFKA_VERSION}\n",
        "    hostname: zookeeper\n",
        "    container_name: zookeeper\n",
        "    networks:\n",
        "      - backend\n",
        "    ports:\n",
        "      - ${ZOOKEEPER_CLIENT_PORT}:2181\n",
        "    environment:\n",
        "      ZOOKEEPER_CLIENT_PORT: ${ZOOKEEPER_CLIENT_PORT}\n",
        "      ZOOKEEPER_TICK_TIME: ${ZOOKEEPER_TICK_TIME}\n",
        "      ZOOKEEPER_LOG4J_ROOT_LOGLEVEL: WARN\n",
        "      ZOOKEEPER_TOOLS_LOG4J_LOGLEVEL: ERROR\n",
        "    volumes:\n",
        "      - $PWD/kafka-ce/zk/data:/var/lib/zookeeper/data\n",
        "      - $PWD/kafka-ce/zk/txn-logs:/var/lib/zookeeper/log\n",
        "    restart: always\n",
        "  ####################\n",
        "  # broker\n",
        "  ####################\n",
        "  broker:\n",
        "    image: confluentinc/cp-kafka:${KAFKA_VERSION}\n",
        "    hostname: broker\n",
        "    container_name: broker\n",
        "    depends_on:\n",
        "      - zookeeper\n",
        "    networks:\n",
        "      - backend\n",
        "    ports:\n",
        "      - ${BROKER_EXTERNAL_PORT}:${BROKER_EXTERNAL_PORT}\n",
        "      - ${BROKER_LOCAL_PORT}:${BROKER_LOCAL_PORT}\n",
        "      - ${BROKER_JMX_PORT}:9101\n",
        "    environment:\n",
        "      KAFKA_BROKER_ID: 1\n",
        "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:${ZOOKEEPER_CLIENT_PORT}\n",
        "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
        "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:${BROKER_INTERNAL_PORT},PLAINTEXT_HOST://localhost:${BROKER_LOCAL_PORT}\n",
        "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: ${REPLICATION_FACTOR}\n",
        "      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 1000\n",
        "      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: ${REPLICATION_FACTOR}\n",
        "      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: ${REPLICATION_FACTOR}\n",
        "      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: ${REPLICATION_FACTOR}\n",
        "      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: ${REPLICATION_FACTOR}\n",
        "      KAFKA_DELETE_TOPIC_ENABLE: true\n",
        "      KAFKA_JMX_PORT: 9101\n",
        "      KAFKA_JMX_HOSTNAME: broker\n",
        "      KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: <http://schema-registry>:${SCHEMA_REGISTRY_PORT}\n",
        "      KAFKA_LOG4J_ROOT_LOGLEVEL: WARN\n",
        "      KAFKA_TOOLS_LOG4J_LOGLEVEL: ERROR\n",
        "      KAFKA_LOG_RETENTION_MS: -1\n",
        "      KAFKA_LOG4J_LOGGERS: org.apache.zookeeper=WARN,org.apache.kafka=WARN,kafka=WARN,kafka.cluster=WARN,kafka.controller=WARN,kafka.coordinator=WARN,kafka.log=WARN,kafka.server=WARN,kafka.zookeeper=WARN,state.change.logger=WARN\n",
        "    volumes:\n",
        "      - $PWD/kafka-ce/broker/data:/var/lib/kafka/data\n",
        "    restart: always\n",
        "  ####################\n",
        "  # broker2\n",
        "  ####################\n",
        "  broker2:\n",
        "    image: confluentinc/cp-kafka:${KAFKA_VERSION}\n",
        "    hostname: broker2\n",
        "    container_name: broker2\n",
        "    depends_on:\n",
        "      - zookeeper\n",
        "      - broker\n",
        "    networks:\n",
        "      - backend\n",
        "    ports:\n",
        "      - ${BROKER2_EXTERNAL_PORT}:${BROKER2_EXTERNAL_PORT}\n",
        "      - ${BROKER2_LOCAL_PORT}:${BROKER2_LOCAL_PORT}\n",
        "      - ${BROKER2_JMX_PORT}:9101\n",
        "    environment:\n",
        "      KAFKA_BROKER_ID: 2\n",
        "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:${ZOOKEEPER_CLIENT_PORT}\n",
        "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
        "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker2:${BROKER2_INTERNAL_PORT},PLAINTEXT_HOST://localhost:${BROKER2_LOCAL_PORT}\n",
        "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: ${REPLICATION_FACTOR}\n",
        "      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 1000\n",
        "      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: ${REPLICATION_FACTOR}\n",
        "      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: ${REPLICATION_FACTOR}\n",
        "      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: ${REPLICATION_FACTOR}\n",
        "      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: ${REPLICATION_FACTOR}\n",
        "      KAFKA_JMX_PORT: 9101\n",
        "      KAFKA_JMX_HOSTNAME: broker2\n",
        "      KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: <http://schema-registry>:${SCHEMA_REGISTRY_PORT}\n",
        "      KAFKA_LOG4J_ROOT_LOGLEVEL: WARN\n",
        "      KAFKA_TOOLS_LOG4J_LOGLEVEL: ERROR\n",
        "      KAFKA_LOG_RETENTION_MS: -1\n",
        "      KAFKA_LOG4J_LOGGERS: org.apache.zookeeper=WARN,org.apache.kafka=WARN,kafka=WARN,kafka.cluster=WARN,kafka.controller=WARN,kafka.coordinator=WARN,kafka.log=WARN,kafka.server=WARN,kafka.zookeeper=WARN,state.change.logger=WARN\n",
        "    volumes:\n",
        "      - $PWD/kafka-ce/broker2/data:/var/lib/kafka/data\n",
        "    restart: always\n",
        "  ####################\n",
        "  # schema-registry\n",
        "  ####################\n",
        "  schema-registry:\n",
        "    image: confluentinc/cp-schema-registry:${KAFKA_VERSION}\n",
        "    hostname: schema-registry\n",
        "    container_name: schema-registry\n",
        "    depends_on:\n",
        "      - zookeeper\n",
        "      - broker\n",
        "    networks:\n",
        "      - backend\n",
        "    ports:\n",
        "      - ${SCHEMA_REGISTRY_PORT}:8081\n",
        "    environment:\n",
        "      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n",
        "      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker:${BROKER_INTERNAL_PORT}'\n",
        "      SCHEMA_REGISTRY_LISTENERS: <http://$>{SCHEMA_REGISTRY_PUBLIC_HOST}:${SCHEMA_REGISTRY_PORT}\n",
        "      SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: WARN\n",
        "      SCHEMA_REGISTRY_TOOLS_LOG4J_LOGLEVEL: ERROR\n",
        "    volumes:\n",
        "      - $PWD/kafka-ce/schema-registry/data:/data\n",
        "    restart: always\n",
        "  ####################\n",
        "  # connect\n",
        "  ####################\n",
        "  connect:\n",
        "    image: confluentinc/cp-kafka-connect:${KAFKA_VERSION}\n",
        "    hostname: connect\n",
        "    container_name: connect\n",
        "    depends_on:\n",
        "      - zookeeper\n",
        "      - broker\n",
        "      - schema-registry\n",
        "    networks:\n",
        "      - backend\n",
        "    ports:\n",
        "      - ${CONNECT_PORT}:8083\n",
        "    environment:\n",
        "      CONNECT_BOOTSTRAP_SERVERS: broker:${BROKER_INTERNAL_PORT}\n",
        "      CONNECT_REST_PORT: 8083\n",
        "      CONNECT_REST_ADVERTISED_HOST_NAME: connect\n",
        "      CONNECT_GROUP_ID: connect-distributed-group\n",
        "      CONNECT_CONFIG_STORAGE_TOPIC: _kafka-connect-configs\n",
        "      CONNECT_OFFSET_STORAGE_TOPIC: _kafka-connect-offsets\n",
        "      CONNECT_STATUS_STORAGE_TOPIC: _kafka-connect-status\n",
        "      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: ${REPLICATION_FACTOR}\n",
        "      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: ${REPLICATION_FACTOR}\n",
        "      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: ${REPLICATION_FACTOR}\n",
        "      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 1000\n",
        "      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter\n",
        "      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: '<http://schema-registry>:${SCHEMA_REGISTRY_PORT}'\n",
        "      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter\n",
        "      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: <http://schema-registry>:${SCHEMA_REGISTRY_PORT}\n",
        "      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter\n",
        "      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter\n",
        "      CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-${KAFKA_VERSION}.jar\n",
        "      CONNECT_PRODUCER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor\n",
        "      CONNECT_CONSUMER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\n",
        "      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components\n",
        "      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR\n",
        "      CONNECT_LOG4J_ROOT_LOGLEVEL: WARN\n",
        "      CONNECT_TOOLS_LOG4J_LOGLEVEL: ERROR\n",
        "    volumes:\n",
        "      - $PWD/kafka-ce/connect/plugins:/usr/share/confluent-hub-components\n",
        "      - $PWD/kafka-ce/connect/data:/data\n",
        "    restart: always\n",
        "  ####################\n",
        "  # ksqldb-server\n",
        "  ####################\n",
        "  ksqldb-server:\n",
        "    image: confluentinc/cp-ksqldb-server:${KAFKA_VERSION}\n",
        "    hostname: ksqldb-server\n",
        "    container_name: ksqldb-server\n",
        "    depends_on:\n",
        "      - zookeeper\n",
        "      - broker\n",
        "      - connect\n",
        "    networks:\n",
        "      - backend\n",
        "    ports:\n",
        "      - ${KSQLDB_PORT}:8088\n",
        "    environment:\n",
        "      KSQL_CONFIG_DIR: /etc/ksql\n",
        "      KSQL_BOOTSTRAP_SERVERS: broker:${BROKER_INTERNAL_PORT}\n",
        "      KSQL_HOST_NAME: ksqldb-server\n",
        "      KSQL_LISTENERS: <http://0.0.0.0>:${KSQLDB_PORT}\n",
        "      KSQL_CACHE_MAX_BYTES_BUFFERING: 0\n",
        "      KSQL_KSQL_SCHEMA_REGISTRY_URL: <http://schema-registry>:${SCHEMA_REGISTRY_PORT}\n",
        "      KSQL_PRODUCER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor\n",
        "      KSQL_CONSUMER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\n",
        "      KSQL_KSQL_CONNECT_URL: <http://connect>:${CONNECT_PORT}\n",
        "      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: ${REPLICATION_FACTOR}\n",
        "      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: true\n",
        "      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: true\n",
        "      KSQL_LOG4J_ROOT_LOGLEVEL: WARN\n",
        "      KSQL_TOOLS_LOG4J_LOGLEVEL: ERROR\n",
        "    restart: always\n",
        "  ####################\n",
        "  # ksqldb-cli\n",
        "  ####################\n",
        "  ksqldb-cli:\n",
        "    image: confluentinc/cp-ksqldb-cli:${KAFKA_VERSION}\n",
        "    hostname: ksqldb-cli\n",
        "    container_name: ksqldb-cli\n",
        "    depends_on:\n",
        "      - zookeeper\n",
        "      - broker\n",
        "      - connect\n",
        "      - ksqldb-server\n",
        "    networks:\n",
        "      - backend\n",
        "    entrypoint: /bin/sh\n",
        "    tty: true\n",
        "    volumes:\n",
        "      - $PWD/kafka-ce/ksqldb-cli/scripts:/data/scripts\n",
        "    restart: always    \n",
        "  ####################\n",
        "  # rest-proxy\n",
        "  ####################\n",
        "  rest-proxy:\n",
        "    image: confluentinc/cp-kafka-rest:${KAFKA_VERSION}\n",
        "    hostname: rest-proxy\n",
        "    container_name: rest-proxy\n",
        "    depends_on:\n",
        "      - broker\n",
        "      # - schema-registry\n",
        "    networks:\n",
        "      - backend\n",
        "    ports:\n",
        "      - ${REST_PROXY_PORT}:8082\n",
        "    environment:\n",
        "      KAFKA_REST_HOST_NAME: rest-proxy\n",
        "      KAFKA_REST_BOOTSTRAP_SERVERS: 'broker:${BROKER_INTERNAL_PORT}'\n",
        "      KAFKA_REST_LISTENERS: <http://0.0.0.0>:${REST_PROXY_PORT}\n",
        "      KAFKA_REST_SCHEMA_REGISTRY_URL: <http://schema-registry>:${SCHEMA_REGISTRY_PORT}\n",
        "      KAFKA_REST_LOG4J_ROOT_LOGLEVEL: WARN\n",
        "      KAFKA_REST_TOOLS_LOG4J_LOGLEVEL: ERROR\n",
        "    restart: always\n",
        "    \n",
        "  ####################\n",
        "  # kafka-ui\n",
        "  ####################\n",
        "  kafka-ui:\n",
        "    hostname: kafka-ui\n",
        "    container_name: kafka-ui\n",
        "    image: provectuslabs/kafka-ui:latest\n",
        "    ports:\n",
        "      - ${KAFKA_UI_PORT}:8080\n",
        "    networks:\n",
        "      - backend\n",
        "    depends_on:\n",
        "      - zookeeper\n",
        "      - broker\n",
        "      # - schema-registry\n",
        "      # - connect\n",
        "    environment:\n",
        "      KAFKA_CLUSTERS_0_NAME: local\n",
        "      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: broker:${BROKER_INTERNAL_PORT}\n",
        "      KAFKA_CLUSTERS_0_METRICS_PORT: ${KAFKA_UI_METRIC_PORT}\n",
        "      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: <http://schema-registry>:${SCHEMA_REGISTRY_PORT}\n",
        "      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: connect\n",
        "      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: <http://connect>:${CONNECT_PORT}\n",
        "      KAFKA_CLUSTERS_0_KSQLDBSERVER: <http://ksqldb-server>:${KSQLDB_PORT}\n",
        "      KAFKA_CLUSTERS_0_READONLY: ${KAFKA_UI_READONLY}\n",
        "      AUTH_TYPE: ${KAFKA_UI_AUTH_TYPE}\n",
        "      SPRING_SECURITY_USER_NAME: ${KAFKA_UI_SPRING_SECURITY_USER_NAME}\n",
        "      SPRING_SECURITY_USER_PASSWORD: ${KAFKA_UI_SPRING_SECURITY_USER_PASSWORD}\n",
        "    restart: always\n",
        "  ####################\n",
        "  # init-kafka\n",
        "  ####################\n",
        "  init-kafka:\n",
        "    image: confluentinc/cp-kafka:${KAFKA_VERSION}\n",
        "    container_name: init-kafka\n",
        "    depends_on:\n",
        "      - zookeeper\n",
        "      - broker\n",
        "      # - schema-registry\n",
        "      # - connect\n",
        "    networks:\n",
        "      - backend\n",
        "    entrypoint: [ '/bin/sh', '-c' ]\n",
        "    command: |\n",
        "      \"\n",
        "      kafka-topics --bootstrap-server broker:${BROKER_INTERNAL_PORT} --list\n",
        "      echo -e 'Creating kafka topics'\n",
        "      kafka-topics --bootstrap-server broker:${BROKER_INTERNAL_PORT} --create --if-not-exists --topic ecommerce_customers --replication-factor ${REPLICATION_FACTOR} --partitions ${PARTITIONS}\n",
        "      kafka-topics --bootstrap-server broker:${BROKER_INTERNAL_PORT} --create --if-not-exists --topic ecommerce_products --replication-factor ${REPLICATION_FACTOR} --partitions ${PARTITIONS}\n",
        "      kafka-topics --bootstrap-server broker:${BROKER_INTERNAL_PORT} --create --if-not-exists --topic ecommerce_transactions --replication-factor ${REPLICATION_FACTOR} --partitions ${PARTITIONS}\n",
        "      kafka-topics --bootstrap-server broker:${BROKER_INTERNAL_PORT} --create --if-not-exists --topic ecommerce_user_interactions --replication-factor ${REPLICATION_FACTOR} --partitions ${PARTITIONS}\n",
        "      kafka-topics --bootstrap-server broker:${BROKER_INTERNAL_PORT} --create --if-not-exists --topic ecommerce_product_views --replication-factor ${REPLICATION_FACTOR} --partitions ${PARTITIONS}\n",
        "      kafka-topics --bootstrap-server broker:${BROKER_INTERNAL_PORT} --create --if-not-exists --topic ecommerce_system_logs --replication-factor ${REPLICATION_FACTOR} --partitions ${PARTITIONS}\n",
        "      echo -e 'Successfully created the following topics:'\n",
        "      kafka-topics --bootstrap-server broker:${BROKER_INTERNAL_PORT} --list\n",
        "      \"\n",
        "################################################################################\n",
        "#\n",
        "# networks\n",
        "# - backend\n",
        "#\n",
        "################################################################################\n",
        "networks:\n",
        "  backend:\n",
        "    name: backend\n",
        "```"
      ],
      "metadata": {
        "id": "r0rzIwZgB40l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "#!/bin/bash\n",
        "\n",
        "sudo rm -r esdata1\n",
        "sudo rm -r kafka-ce\n",
        "sudo rm -r spark-processing\n",
        "sudo rm -r spark-checkpoint\n",
        "\n",
        "# Function to create volumes for various services\n",
        "create_volumes() {\n",
        "    service=$1\n",
        "    shift\n",
        "    echo \"Creating volumes for ${service} ...\"\n",
        "    for item in \"$@\"\n",
        "    do\n",
        "        echo \"$item\"\n",
        "        mkdir -p \"$item\"\n",
        "        sudo chown -R \"$(id -u)\" \"$item\"\n",
        "        sudo chgrp -R \"$(id -g)\" \"$item\"\n",
        "        sudo chmod -R u+rwX,g+rX,o+wrx \"$item\"\n",
        "        echo \"$item volume is created.\"\n",
        "    done\n",
        "    echo \"Volumes for ${service} created ✅\"\n",
        "    echo\n",
        "}\n",
        "# Load environment variables from .env file\n",
        "source .env\n",
        "# Create volumes for different services\n",
        "create_volumes zookeeper kafka-ce/zk/data kafka-ce/zk/txn-logs\n",
        "create_volumes brokers kafka-ce/broker/data kafka-ce/broker2/data kafka-ce/broker3/data kafka-ce/broker4/data\n",
        "create_volumes schema-registry kafka-ce/schema-registry/data\n",
        "create_volumes connect kafka-ce/connect/data kafka-ce/connect/plugins\n",
        "create_volumes ksqldb-cli kafka-ce/ksqldb-cli/scripts\n",
        "create_volumes filepulse kafka-ce/connect/data/filepulse/xml\n",
        "create_volumes elasticsearch esdata1\n",
        "create_volumes spark_master spark-processing\n",
        "create_volumes spark_master spark-checkpoint\n",
        "cp spark-processing.py spark-processing\n",
        "export PWD=$(pwd)\n",
        "# Start all services using Docker Compose\n",
        "echo \"Starting all services ...\"\n",
        "docker compose -f docker-compose.yaml up -d\n",
        "# Set timeout for readiness checks\n",
        "timeout=600\n",
        "echo ''\n",
        "# Check readiness for Zookeeper\n",
        "zookeeper=\"zookeeper:${ZOOKEEPER_CLIENT_PORT}\"\n",
        "echo \"Wait for ${zookeeper} ...\"\n",
        "docker exec -it zookeeper cub zk-ready \"$zookeeper\" $timeout > /dev/null\n",
        "echo \"${zookeeper} is ready ✅\"\n",
        "echo ''\n",
        "# Check readiness for Kafka brokers\n",
        "for item in broker:${BROKER_INTERNAL_PORT} broker2:${BROKER2_INTERNAL_PORT}\n",
        "do\n",
        "    broker=\"$item\"\n",
        "    echo \"Wait for ${broker} ...\"\n",
        "    docker exec -it zookeeper cub kafka-ready -b \"$broker\" 1 $timeout > /dev/null\n",
        "    echo \"${broker} is ready ✅\"\n",
        "    echo ''\n",
        "done\n",
        "# Check readiness for Schema Registry\n",
        "schema_registry_host=\"schema-registry\"\n",
        "schema_registry_port=\"${SCHEMA_REGISTRY_PORT}\"\n",
        "echo \"Wait for ${schema_registry_host}:${schema_registry_port} ...\"\n",
        "docker exec -it zookeeper cub sr-ready \"$schema_registry_host\" $schema_registry_port $timeout > /dev/null\n",
        "echo \"${schema_registry_host}:${schema_registry_port} is ready ✅\"\n",
        "echo ''\n",
        "# Check readiness for Kafka Connect\n",
        "# for item in connect connect2 connect3\n",
        "for item in connect\n",
        "do\n",
        "    connect_host=\"$item\"\n",
        "    connect_port=\"${CONNECT_PORT}\"\n",
        "    echo \"Wait for ${connect_host}:${connect_port} ...\"\n",
        "    docker exec -it zookeeper cub connect-ready \"$connect_host\" $connect_port $timeout > /dev/null\n",
        "    echo \"${connect_host}:${connect_port} is ready ✅\"\n",
        "    echo ''\n",
        "done\n",
        "echo \"Kafka cluster is ready ✅\"\n",
        "```"
      ],
      "metadata": {
        "id": "sQd6QKvDEIbr"
      }
    }
  ]
}