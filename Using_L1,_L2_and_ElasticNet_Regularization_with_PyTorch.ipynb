{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrjoWhZqJqcnhoraVl2QKO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@francescofranco_39234/using-l1-l2-and-elasticnet-regularization-with-pytorch-633e79b863e0)"
      ],
      "metadata": {
        "id": "m_XMc4VRGODD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcAyMOTPGKgX",
        "outputId": "eb98bc53-3c02-4c80-de06-cf91836c8aae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 6.08MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 160kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.30MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.27MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after mini-batch   500: 65.46807 (of which 63.18528 L1 loss)\n",
            "Loss after mini-batch  1000: 13.96880 (of which 11.65321 L1 loss)\n",
            "Loss after mini-batch  1500: 3.68697 (of which 1.38204 L1 loss)\n",
            "Loss after mini-batch  2000: 2.94059 (of which 0.63800 L1 loss)\n",
            "Loss after mini-batch  2500: 2.93674 (of which 0.63414 L1 loss)\n",
            "Loss after mini-batch  3000: 2.94184 (of which 0.63925 L1 loss)\n",
            "Loss after mini-batch  3500: 2.93983 (of which 0.63725 L1 loss)\n",
            "Loss after mini-batch  4000: 2.93638 (of which 0.63378 L1 loss)\n",
            "Loss after mini-batch  4500: 2.93949 (of which 0.63691 L1 loss)\n",
            "Loss after mini-batch  5000: 2.93907 (of which 0.63648 L1 loss)\n",
            "Loss after mini-batch  5500: 2.93750 (of which 0.63491 L1 loss)\n",
            "Loss after mini-batch  6000: 2.93818 (of which 0.63559 L1 loss)\n",
            "Starting epoch 2\n",
            "Loss after mini-batch   500: 2.94051 (of which 0.63793 L1 loss)\n",
            "Loss after mini-batch  1000: 2.93816 (of which 0.63558 L1 loss)\n",
            "Loss after mini-batch  1500: 2.94031 (of which 0.63773 L1 loss)\n",
            "Loss after mini-batch  2000: 2.93879 (of which 0.63620 L1 loss)\n",
            "Loss after mini-batch  2500: 2.93623 (of which 0.63365 L1 loss)\n",
            "Loss after mini-batch  3000: 2.93797 (of which 0.63539 L1 loss)\n",
            "Loss after mini-batch  3500: 2.93959 (of which 0.63701 L1 loss)\n",
            "Loss after mini-batch  4000: 2.93828 (of which 0.63570 L1 loss)\n",
            "Loss after mini-batch  4500: 2.94164 (of which 0.63905 L1 loss)\n",
            "Loss after mini-batch  5000: 2.94002 (of which 0.63744 L1 loss)\n",
            "Loss after mini-batch  5500: 2.94011 (of which 0.63752 L1 loss)\n",
            "Loss after mini-batch  6000: 2.94016 (of which 0.63757 L1 loss)\n",
            "Starting epoch 3\n",
            "Loss after mini-batch   500: 2.94074 (of which 0.63816 L1 loss)\n",
            "Loss after mini-batch  1000: 2.93632 (of which 0.63373 L1 loss)\n",
            "Loss after mini-batch  1500: 2.93939 (of which 0.63680 L1 loss)\n",
            "Loss after mini-batch  2000: 2.93848 (of which 0.63589 L1 loss)\n",
            "Loss after mini-batch  2500: 2.93671 (of which 0.63413 L1 loss)\n",
            "Loss after mini-batch  3000: 2.94108 (of which 0.63850 L1 loss)\n",
            "Loss after mini-batch  3500: 2.93878 (of which 0.63619 L1 loss)\n",
            "Loss after mini-batch  4000: 2.93821 (of which 0.63563 L1 loss)\n",
            "Loss after mini-batch  4500: 2.93967 (of which 0.63708 L1 loss)\n",
            "Loss after mini-batch  5000: 2.94084 (of which 0.63825 L1 loss)\n",
            "Loss after mini-batch  5500: 2.93688 (of which 0.63429 L1 loss)\n",
            "Loss after mini-batch  6000: 2.93894 (of which 0.63636 L1 loss)\n",
            "Starting epoch 4\n",
            "Loss after mini-batch   500: 2.94096 (of which 0.63838 L1 loss)\n",
            "Loss after mini-batch  1000: 2.93816 (of which 0.63558 L1 loss)\n",
            "Loss after mini-batch  1500: 2.94003 (of which 0.63745 L1 loss)\n",
            "Loss after mini-batch  2000: 2.93791 (of which 0.63533 L1 loss)\n",
            "Loss after mini-batch  2500: 2.93960 (of which 0.63701 L1 loss)\n",
            "Loss after mini-batch  3000: 2.94142 (of which 0.63884 L1 loss)\n",
            "Loss after mini-batch  3500: 2.93999 (of which 0.63740 L1 loss)\n",
            "Loss after mini-batch  4000: 2.93798 (of which 0.63540 L1 loss)\n",
            "Loss after mini-batch  4500: 2.94045 (of which 0.63786 L1 loss)\n",
            "Loss after mini-batch  5000: 2.93893 (of which 0.63633 L1 loss)\n",
            "Loss after mini-batch  5500: 2.93721 (of which 0.63463 L1 loss)\n",
            "Loss after mini-batch  6000: 2.94211 (of which 0.63953 L1 loss)\n",
            "Starting epoch 5\n",
            "Loss after mini-batch   500: 2.93964 (of which 0.63706 L1 loss)\n",
            "Loss after mini-batch  1000: 2.93815 (of which 0.63556 L1 loss)\n",
            "Loss after mini-batch  1500: 2.94051 (of which 0.63792 L1 loss)\n",
            "Loss after mini-batch  2000: 2.93783 (of which 0.63524 L1 loss)\n",
            "Loss after mini-batch  2500: 2.93778 (of which 0.63521 L1 loss)\n",
            "Loss after mini-batch  3000: 2.93851 (of which 0.63592 L1 loss)\n",
            "Loss after mini-batch  3500: 2.94097 (of which 0.63838 L1 loss)\n",
            "Loss after mini-batch  4000: 2.93794 (of which 0.63535 L1 loss)\n",
            "Loss after mini-batch  4500: 2.93944 (of which 0.63686 L1 loss)\n",
            "Loss after mini-batch  5000: 2.93922 (of which 0.63663 L1 loss)\n",
            "Loss after mini-batch  5500: 2.93809 (of which 0.63551 L1 loss)\n",
            "Loss after mini-batch  6000: 2.94008 (of which 0.63748 L1 loss)\n",
            "Training process has finished.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(28 * 28 * 1, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 10)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)\n",
        "\n",
        "  def compute_l1_loss(self, w):\n",
        "      return torch.abs(w).sum()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "\n",
        "  # Prepare CIFAR-10 dataset\n",
        "  dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
        "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "\n",
        "  # Initialize the MLP\n",
        "  mlp = MLP()\n",
        "\n",
        "  # Define the loss function and optimizer\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "  # Run the training loop\n",
        "  for epoch in range(0, 5): # 5 epochs at maximum\n",
        "\n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "      # Get inputs\n",
        "      inputs, targets = data\n",
        "\n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Perform forward pass\n",
        "      outputs = mlp(inputs)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = loss_function(outputs, targets)\n",
        "\n",
        "      # Compute L1 loss component\n",
        "      l1_weight = 1.0\n",
        "      l1_parameters = []\n",
        "      for parameter in mlp.parameters():\n",
        "          l1_parameters.append(parameter.view(-1))\n",
        "      l1 = l1_weight * mlp.compute_l1_loss(torch.cat(l1_parameters))\n",
        "\n",
        "      # Add L1 loss component\n",
        "      loss += l1\n",
        "\n",
        "      # Perform backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # Perform optimization\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print statistics\n",
        "      minibatch_loss = loss.item()\n",
        "      if i % 500 == 499:\n",
        "          print('Loss after mini-batch %5d: %.5f (of which %.5f L1 loss)' %\n",
        "                (i + 1, minibatch_loss, l1))\n",
        "          current_loss = 0.0\n",
        "\n",
        "  # Process is complete.\n",
        "  print('Training process has finished.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(28 * 28 * 1, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 10)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)\n",
        "\n",
        "  def compute_l2_loss(self, w):\n",
        "      return torch.square(w).sum()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "\n",
        "  # Prepare CIFAR-10 dataset\n",
        "  dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
        "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "\n",
        "  # Initialize the MLP\n",
        "  mlp = MLP()\n",
        "\n",
        "  # Define the loss function and optimizer\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "  # Run the training loop\n",
        "  for epoch in range(0, 5): # 5 epochs at maximum\n",
        "\n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "      # Get inputs\n",
        "      inputs, targets = data\n",
        "\n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Perform forward pass\n",
        "      outputs = mlp(inputs)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = loss_function(outputs, targets)\n",
        "\n",
        "      # Compute l2 loss component\n",
        "      l2_weight = 1.0\n",
        "      l2_parameters = []\n",
        "      for parameter in mlp.parameters():\n",
        "          l2_parameters.append(parameter.view(-1))\n",
        "      l2 = l2_weight * mlp.compute_l2_loss(torch.cat(l2_parameters))\n",
        "\n",
        "      # Add l2 loss component\n",
        "      loss += l2\n",
        "\n",
        "      # Perform backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # Perform optimization\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print statistics\n",
        "      minibatch_loss = loss.item()\n",
        "      if i % 500 == 499:\n",
        "          print('Loss after mini-batch %5d: %.5f (of which %.5f l2 loss)' %\n",
        "                (i + 1, minibatch_loss, l2))\n",
        "          current_loss = 0.0\n",
        "\n",
        "  # Process is complete.\n",
        "  print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGI-EgYEGVrY",
        "outputId": "19635baa-6cb9-4d3e-a9de-0dc52582d63a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 6.90672 (of which 4.62813 l2 loss)\n",
            "Loss after mini-batch  1000: 3.57362 (of which 1.24972 l2 loss)\n",
            "Loss after mini-batch  1500: 2.59496 (of which 0.29211 l2 loss)\n",
            "Loss after mini-batch  2000: 2.35297 (of which 0.05843 l2 loss)\n",
            "Loss after mini-batch  2500: 2.30740 (of which 0.00982 l2 loss)\n",
            "Loss after mini-batch  3000: 2.30312 (of which 0.00131 l2 loss)\n",
            "Loss after mini-batch  3500: 2.30289 (of which 0.00020 l2 loss)\n",
            "Loss after mini-batch  4000: 2.30143 (of which 0.00008 l2 loss)\n",
            "Loss after mini-batch  4500: 2.30402 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  5000: 2.30044 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  5500: 2.30264 (of which 0.00006 l2 loss)\n",
            "Loss after mini-batch  6000: 2.30348 (of which 0.00009 l2 loss)\n",
            "Starting epoch 2\n",
            "Loss after mini-batch   500: 2.30326 (of which 0.00006 l2 loss)\n",
            "Loss after mini-batch  1000: 2.30304 (of which 0.00006 l2 loss)\n",
            "Loss after mini-batch  1500: 2.30264 (of which 0.00004 l2 loss)\n",
            "Loss after mini-batch  2000: 2.30267 (of which 0.00012 l2 loss)\n",
            "Loss after mini-batch  2500: 2.29971 (of which 0.00015 l2 loss)\n",
            "Loss after mini-batch  3000: 2.30323 (of which 0.00008 l2 loss)\n",
            "Loss after mini-batch  3500: 2.30171 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  4000: 2.30261 (of which 0.00013 l2 loss)\n",
            "Loss after mini-batch  4500: 2.30159 (of which 0.00008 l2 loss)\n",
            "Loss after mini-batch  5000: 2.30144 (of which 0.00009 l2 loss)\n",
            "Loss after mini-batch  5500: 2.30351 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  6000: 2.30123 (of which 0.00007 l2 loss)\n",
            "Starting epoch 3\n",
            "Loss after mini-batch   500: 2.30285 (of which 0.00009 l2 loss)\n",
            "Loss after mini-batch  1000: 2.30252 (of which 0.00009 l2 loss)\n",
            "Loss after mini-batch  1500: 2.30218 (of which 0.00008 l2 loss)\n",
            "Loss after mini-batch  2000: 2.30202 (of which 0.00009 l2 loss)\n",
            "Loss after mini-batch  2500: 2.30251 (of which 0.00006 l2 loss)\n",
            "Loss after mini-batch  3000: 2.30207 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  3500: 2.30398 (of which 0.00004 l2 loss)\n",
            "Loss after mini-batch  4000: 2.30179 (of which 0.00006 l2 loss)\n",
            "Loss after mini-batch  4500: 2.30308 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  5000: 2.30398 (of which 0.00010 l2 loss)\n",
            "Loss after mini-batch  5500: 2.30274 (of which 0.00010 l2 loss)\n",
            "Loss after mini-batch  6000: 2.30284 (of which 0.00009 l2 loss)\n",
            "Starting epoch 4\n",
            "Loss after mini-batch   500: 2.30105 (of which 0.00010 l2 loss)\n",
            "Loss after mini-batch  1000: 2.30233 (of which 0.00009 l2 loss)\n",
            "Loss after mini-batch  1500: 2.30122 (of which 0.00012 l2 loss)\n",
            "Loss after mini-batch  2000: 2.30151 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  2500: 2.30151 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  3000: 2.30214 (of which 0.00011 l2 loss)\n",
            "Loss after mini-batch  3500: 2.30338 (of which 0.00008 l2 loss)\n",
            "Loss after mini-batch  4000: 2.30284 (of which 0.00008 l2 loss)\n",
            "Loss after mini-batch  4500: 2.30178 (of which 0.00011 l2 loss)\n",
            "Loss after mini-batch  5000: 2.30343 (of which 0.00006 l2 loss)\n",
            "Loss after mini-batch  5500: 2.30285 (of which 0.00008 l2 loss)\n",
            "Loss after mini-batch  6000: 2.30251 (of which 0.00004 l2 loss)\n",
            "Starting epoch 5\n",
            "Loss after mini-batch   500: 2.30324 (of which 0.00011 l2 loss)\n",
            "Loss after mini-batch  1000: 2.30488 (of which 0.00011 l2 loss)\n",
            "Loss after mini-batch  1500: 2.30274 (of which 0.00004 l2 loss)\n",
            "Loss after mini-batch  2000: 2.30191 (of which 0.00004 l2 loss)\n",
            "Loss after mini-batch  2500: 2.30102 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  3000: 2.30194 (of which 0.00008 l2 loss)\n",
            "Loss after mini-batch  3500: 2.30228 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  4000: 2.30383 (of which 0.00009 l2 loss)\n",
            "Loss after mini-batch  4500: 2.30140 (of which 0.00006 l2 loss)\n",
            "Loss after mini-batch  5000: 2.30330 (of which 0.00009 l2 loss)\n",
            "Loss after mini-batch  5500: 2.30312 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  6000: 2.30295 (of which 0.00011 l2 loss)\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(28 * 28 * 1, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 10)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)\n",
        "\n",
        "  def compute_l1_loss(self, w):\n",
        "      return torch.abs(w).sum()\n",
        "\n",
        "  def compute_l2_loss(self, w):\n",
        "      return torch.square(w).sum()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "\n",
        "  # Prepare CIFAR-10 dataset\n",
        "  dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
        "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "\n",
        "  # Initialize the MLP\n",
        "  mlp = MLP()\n",
        "\n",
        "  # Define the loss function and optimizer\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "  # Run the training loop\n",
        "  for epoch in range(0, 5): # 5 epochs at maximum\n",
        "\n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "      # Get inputs\n",
        "      inputs, targets = data\n",
        "\n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Perform forward pass\n",
        "      outputs = mlp(inputs)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = loss_function(outputs, targets)\n",
        "\n",
        "      # Specify L1 and L2 weights\n",
        "      l1_weight = 0.3\n",
        "      l2_weight = 0.7\n",
        "\n",
        "      # Compute L1 and L2 loss component\n",
        "      parameters = []\n",
        "      for parameter in mlp.parameters():\n",
        "          parameters.append(parameter.view(-1))\n",
        "      l1 = l1_weight * mlp.compute_l1_loss(torch.cat(parameters))\n",
        "      l2 = l2_weight * mlp.compute_l2_loss(torch.cat(parameters))\n",
        "\n",
        "      # Add L1 and L2 loss components\n",
        "      loss += l1\n",
        "      loss += l2\n",
        "\n",
        "      # Perform backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # Perform optimization\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print statistics\n",
        "      minibatch_loss = loss.item()\n",
        "      if i % 500 == 499:\n",
        "          print('Loss after mini-batch %5d: %.5f (of which %.5f L1 loss; %0.5f L2 loss)' %\n",
        "                (i + 1, minibatch_loss, l1, l2))\n",
        "          current_loss = 0.0\n",
        "\n",
        "  # Process is complete.\n",
        "  print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeEwQI7jGX2e",
        "outputId": "8224b388-9bfb-4a5d-bdd7-0ace07076518"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 24.79406 (of which 19.77073 L1 loss; 2.74106 L2 loss)\n",
            "Loss after mini-batch  1000: 7.52531 (of which 4.81137 L1 loss; 0.39591 L2 loss)\n",
            "Loss after mini-batch  1500: 3.03654 (of which 0.69882 L1 loss; 0.03131 L2 loss)\n",
            "Loss after mini-batch  2000: 2.48222 (of which 0.17963 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2500: 2.48142 (of which 0.17883 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3000: 2.48055 (of which 0.17796 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3500: 2.47980 (of which 0.17721 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4000: 2.47945 (of which 0.17687 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4500: 2.47878 (of which 0.17619 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5000: 2.47902 (of which 0.17643 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5500: 2.47924 (of which 0.17665 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  6000: 2.48023 (of which 0.17764 L1 loss; 0.00001 L2 loss)\n",
            "Starting epoch 2\n",
            "Loss after mini-batch   500: 2.48148 (of which 0.17889 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  1000: 2.48102 (of which 0.17842 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  1500: 2.48108 (of which 0.17849 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2000: 2.48203 (of which 0.17944 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2500: 2.48219 (of which 0.17960 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3000: 2.48230 (of which 0.17970 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3500: 2.48387 (of which 0.18128 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4000: 2.48484 (of which 0.18225 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4500: 2.48541 (of which 0.18282 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5000: 2.48757 (of which 0.18498 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5500: 2.48770 (of which 0.18511 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  6000: 2.48732 (of which 0.18473 L1 loss; 0.00001 L2 loss)\n",
            "Starting epoch 3\n",
            "Loss after mini-batch   500: 2.48765 (of which 0.18507 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  1000: 2.48776 (of which 0.18517 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  1500: 2.48717 (of which 0.18458 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2000: 2.48719 (of which 0.18460 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2500: 2.48740 (of which 0.18481 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3000: 2.48723 (of which 0.18463 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3500: 2.48787 (of which 0.18529 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4000: 2.48764 (of which 0.18503 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4500: 2.48755 (of which 0.18496 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5000: 2.48803 (of which 0.18544 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5500: 2.48769 (of which 0.18511 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  6000: 2.48745 (of which 0.18485 L1 loss; 0.00001 L2 loss)\n",
            "Starting epoch 4\n",
            "Loss after mini-batch   500: 2.48761 (of which 0.18502 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  1000: 2.48751 (of which 0.18491 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  1500: 2.48701 (of which 0.18443 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2000: 2.48796 (of which 0.18537 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2500: 2.48794 (of which 0.18536 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3000: 2.48723 (of which 0.18464 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3500: 2.48811 (of which 0.18552 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4000: 2.48819 (of which 0.18558 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4500: 2.48739 (of which 0.18480 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5000: 2.48724 (of which 0.18465 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5500: 2.48737 (of which 0.18477 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  6000: 2.48697 (of which 0.18438 L1 loss; 0.00001 L2 loss)\n",
            "Starting epoch 5\n",
            "Loss after mini-batch   500: 2.48780 (of which 0.18520 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  1000: 2.48807 (of which 0.18547 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  1500: 2.48759 (of which 0.18500 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2000: 2.48831 (of which 0.18572 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2500: 2.48774 (of which 0.18516 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3000: 2.48727 (of which 0.18467 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3500: 2.48755 (of which 0.18496 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4000: 2.48714 (of which 0.18454 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4500: 2.48713 (of which 0.18453 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5000: 2.48802 (of which 0.18542 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5500: 2.48797 (of which 0.18539 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  6000: 2.48745 (of which 0.18486 L1 loss; 0.00001 L2 loss)\n",
            "Training process has finished.\n"
          ]
        }
      ]
    }
  ]
}