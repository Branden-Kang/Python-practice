{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pandas for Data Engineers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNBqpjSMJXYrNAzQlr5aKcR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DImAjRVq2bf"
      },
      "source": [
        "[Reference](https://ronnie-joshua.medium.com/pandas-for-data-engineers-part-i-85aa18e7e296)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxopNpjbpano"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# Copyright 2021 Ronnie Joshua \n",
        "# Email: ron.juden@gmail.com\n",
        "# Linkedin: https://www.linkedin.com/in/ronnie-joshua/\n",
        "# Vist: http://www.webanalyticsinfo.com/\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "# Built in Libraries\n",
        "import os\n",
        "from typing import List, Callable\n",
        "import functools\n",
        "import time\n",
        "import multiprocessing as mp\n",
        "\n",
        "\n",
        "# External Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import psutil\n",
        "\n",
        "def memory_footprint():\n",
        "    '''Returns memory (in MB) being used by Python process'''\n",
        "    process_object = psutil.Process(os.getpid())\n",
        "    mem = process_object.memory_info().rss\n",
        "    return (mem/(1024**2))\n",
        "\n",
        "def memory_footprint_calc(func: Callable):\n",
        "    \"\"\" Calcuates the Memory footprint of the python process \n",
        "        Use it as a function decorator.\n",
        "        Memory Calculations are in MBs\n",
        "    \"\"\"\n",
        "    \n",
        "    @functools.wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        mem_before = psutil.Process(os.getpid()).memory_info().rss/(1024**2)\n",
        "        print(f\"Python Process Memory Consumption (Before): {mem_before}\")\n",
        "        value = func(*args, **kwargs)\n",
        "        mem_after = psutil.Process(os.getpid()).memory_info().rss/(1024**2)\n",
        "        print(f\"Python Process Memory Consumption (After): {mem_after}\")\n",
        "        print(f\"Difference: {(mem_after - mem_before)}\")\n",
        "        return value\n",
        "    \n",
        "    return wrapper\n",
        "\n",
        "\n",
        "# Utility Functions\n",
        "# Using a decorator to time the function execution time\n",
        "def timer(func: Callable):\n",
        "    \"\"\"Print the runtime of the decorated function\"\"\"\n",
        "\n",
        "    @functools.wraps(func)\n",
        "    def wrapper_timer(*args, **kwargs):\n",
        "        start_time = time.perf_counter()\n",
        "        value = func(*args, **kwargs)\n",
        "        end_time = time.perf_counter()\n",
        "        run_time = end_time - start_time\n",
        "        print(f\"Finished {func.__name__!r} in {run_time:.4f} secs\")\n",
        "        return value\n",
        "\n",
        "    return wrapper_timer\n",
        "\n",
        "\n",
        "# Managing Data with Generators\n",
        "# Using Generator for Lazy Evaluation of Data\n",
        "\n",
        "def generate_filenames():\n",
        "    return (f\"data_{k}.csv\" for k in range(1,5))\n",
        "\n",
        "def read_df_chunks(file):\n",
        "    return (chunk for chunk in pd.read_csv(file, chunksize=1000))\n",
        "\n",
        "def generate_chunks():\n",
        "    chunks = (read_df_chunks(file) for file in generate_filenames())\n",
        "    return chunks\n",
        "\n",
        "@timer\n",
        "@memory_footprint_calc\n",
        "def consume_generator(dfs):\n",
        "    df_chunks_list = list()\n",
        "    for df in dfs:\n",
        "        for df_chunk in df:\n",
        "            # Perfor Operation on each chunk\n",
        "            df_chunk.set_index('lead_id', inplace=True)\n",
        "            df_chunk.loc[:,'row_total'] = df_chunk.sum(numeric_only=True, axis=1)\n",
        "            df_chunk = df_chunk[['registration_date', 'country', 'row_total']]\n",
        "            df_chunks_list.append(df_chunk)\n",
        "    return pd.concat(df_chunks_list)\n",
        "\n",
        "\n",
        "def total_system_resources() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "        Returns a Dataframe showing the system resources stats.\n",
        "    Returns:\n",
        "         pd.DataFrame: Dataframe with information about the system resources\n",
        "    \"\"\"\n",
        "    svmem = psutil.virtual_memory()\n",
        "    sys_resource = {\n",
        "        'total' : svmem.total,\n",
        "        'available':svmem.available,\n",
        "        'used':svmem.used,\n",
        "        'free':svmem.free,\n",
        "        'active':svmem.active,\n",
        "        'inactive':svmem.inactive,\n",
        "        'wired':svmem.wired\n",
        "    }\n",
        "    df = pd.Series(sys_resource, name=\"usage\").map(lambda x: f'{x/1000**2:.2f} MB').to_frame()\n",
        "    df.loc['percent'] = svmem.percent\n",
        "    df.loc['cpu_count_cores'] = psutil.cpu_count(logical=False)\n",
        "    # Attributed to Hyperthreading\n",
        "    df.loc['logical_cpu_count'] = psutil.cpu_count()\n",
        "    return df\n",
        "\n",
        "\n",
        "  \n",
        "def inspect_dataframe(df: pd.DataFrame):\n",
        "    \n",
        "    print(\"\\nInformation about the dataframe\")\n",
        "    print('*'*31)\n",
        "    df.info(memory_usage = 'deep')\n",
        "    \n",
        "    criterias = {\n",
        "    \"\\nCheck if Dataframe is Empty\" : df.empty,\n",
        "    \"\\nData Types and thier Value Counts\" : df.dtypes.value_counts(),\n",
        "    \"\\nThe Dimensions of the Dataframe\" : df.shape,\n",
        "    \"\\nDataframe's total missing values\" : df.isnull().sum().sum(),\n",
        "    \"\\nDataframe's missing values column-wise\" : df.isnull().sum(),\n",
        "    \"\\nDataframe's Size\" : df.size, \n",
        "    \"\\nDataframe's Columns\" : df.columns,\n",
        "    \"\\nDataframe's Col Memory Usage (MB's)\" : df.memory_usage(deep=True)/2**20,\n",
        "    \"\\nDataframe's Memory Usage\" : f'{df.memory_usage(deep=True).sum()/2**20:.5f} MB'\n",
        "    }\n",
        "    \n",
        "    for criteria in criterias:\n",
        "        print(criteria)\n",
        "        print('*'*len(criteria))\n",
        "        print(criterias.get(criteria))\n",
        "\n",
        "\n",
        "# https://medium.com/bigdatarepublic/advanced-pandas-optimize-speed-and-memory-a654b53be6c2\n",
        "# I have use the code from the above attached link, however I have refectored the following \n",
        "# functions: optimize_objects and df_optimize_pipe\n",
        "# I have also tried to implement the Pandas piping operator\n",
        "\n",
        "def optimize_floats(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    floats = df.select_dtypes(include=['float64']).columns.tolist()\n",
        "    df[floats] = df[floats].apply(pd.to_numeric, downcast='float')\n",
        "    return df\n",
        "\n",
        "\n",
        "def optimize_ints(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    ints = df.select_dtypes(include=['int64']).columns.tolist()\n",
        "    df[ints] = df[ints].apply(pd.to_numeric, downcast='integer')\n",
        "    return df\n",
        "\n",
        "\n",
        "def optimize_objects(df: pd.DataFrame, datetime_features: List[str] = None) -> pd.DataFrame:\n",
        "    for col in df.select_dtypes(include=['object']):\n",
        "        if not datetime_features or col not in datetime_features:\n",
        "            num_unique_values = len(df[col].unique())\n",
        "            num_total_values = len(df[col])\n",
        "            if float(num_unique_values) / num_total_values < 0.5:\n",
        "                df[col] = df[col].astype('category')\n",
        "        else:\n",
        "            df[col] = pd.to_datetime(df[col])\n",
        "    return df\n",
        "\n",
        "  \n",
        "def df_optimize_pipe(df: pd.DataFrame):\n",
        "    \n",
        "    df_size_before = df.memory_usage(deep=True).sum()/2**20\n",
        "    print(f'Total Size of Dataframe in MBs Before Optimization: {df_size_before:.5f} MB')\n",
        "    \n",
        "    (df.pipe(optimize_floats)\n",
        "        .pipe(optimize_ints)\n",
        "        .pipe(optimize_objects, datetime_features=None))\n",
        "    \n",
        "    df_size_after = df.memory_usage(deep=True).sum()/2**20\n",
        "    \n",
        "    print(f'Total Size of Dataframe in MBs After Optimization: {df_size_after:.5f} MB')\n",
        "    print(f'Percent of Resources Saved: {(df_size_after/df_size_before - 1)*100:.3f} %')\n",
        "\n",
        "\n",
        "    \n",
        "def pandas_df_chunksize(file_path, check_nrows=100000, frac_mem_to_use=0.25, \n",
        "                        size_each_df_mb=250, num_dfs_iterator=None):\n",
        "  \n",
        "    # Determine the size of each df\n",
        "    if size_each_df_mb is None:\n",
        "      assert num_dfs_iterator is not None and isinstance(num_dfs_iterator, int)\n",
        "      size_each_df_mb = int(((psutil.virtual_memory().available/2**20)*(1*frac_mem_to_use)) / num_dfs_iterator)\n",
        "\n",
        "    cost_n_rows_mb = (pd.read_csv(file_path,low_memory=False, nrows=check_nrows)\n",
        "                    .memory_usage(index=True)\n",
        "                    .sum()/2**20)\n",
        "    \n",
        "    # print(int((size_each_df_mb/cost_n_rows_mb)*check_nrows))\n",
        "    return int((size_each_df_mb/cost_n_rows_mb)*check_nrows)\n",
        "\n",
        "\n",
        "def parallelize_dataframe(df: pd.DataFrame, transform_func: Callable) -> pd.DataFrame:\n",
        "    df_split = np.array_split(df, os.cpu_count())\n",
        "    with mp.Pool(os.cpu_count()) as p:\n",
        "        P_df = pd.concat(p.map(transform_func, df_split))\n",
        "    return P_df\n",
        "\n",
        "def transform_func(df):\n",
        "    # Perfor Operation on each chunk\n",
        "    df.set_index('lead_id', inplace=True)\n",
        "    df.loc[:,'row_total'] = df.sum(numeric_only=True, axis=1)\n",
        "    df = df[['registration_date', 'country', 'row_total']]\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main___\":\n",
        "    \n",
        "    # Checking the system resources\n",
        "    total_system_resources()\n",
        "\n",
        "    # Loading the data \n",
        "    df = pd.read_csv('./datasets/flights.csv',low_memory=False)\n",
        "    pd.set_option('display.max_columns', df.shape[1])\n",
        "\n",
        "    # Inspecting the loaded dataframe\n",
        "    inspect_dataframe(df)\n",
        "\n",
        "    # Optimizing the loaded dataframe\n",
        "    df_optimize_pipe(df)\n",
        "\n",
        "    # Reading a dataframe in chunks\n",
        "    iter_csv = pd.read_csv(\n",
        "        './datasets/flights.csv', \n",
        "        low_memory=False, \n",
        "        iterator=True, \n",
        "        chunksize=pandas_df_chunksize('./datasets/flights.csv'))\n",
        "  \n",
        "    for chunk in iter_csv:\n",
        "        print(f\"Size: {chunk.memory_usage(index=True).sum()/2**20} Shape: {chunk.shape}\")"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}