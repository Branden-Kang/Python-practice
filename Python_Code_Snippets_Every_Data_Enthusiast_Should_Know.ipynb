{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhI1w4Lnx4r5b+gL1zUFSa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@sarowar.saurav10/22-python-code-snippets-every-data-enthusiast-should-know-a7cea27da3aa)"
      ],
      "metadata": {
        "id": "ri-h9IHE6fDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Forecasting Data"
      ],
      "metadata": {
        "id": "sHjfPXk_6hlX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A10mdwn36ctG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Load and Prepare Data\n",
        "# Assume 'monthly_sales.csv' has columns 'date' and 'sales'\n",
        "df = pd.read_csv('monthly_sales.csv', parse_dates=['date'], index_col='date')\n",
        "\n",
        "# Step 2: Visualize the Data\n",
        "df['sales'].plot(figsize=(12, 6))\n",
        "plt.title('Monthly Sales Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Check for Stationarity (using ADF test)\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "result = adfuller(df['sales'])\n",
        "print(f'ADF Statistic: {result[0]}')\n",
        "print(f'p-value: {result[1]}')\n",
        "\n",
        "# If the p-value is less than 0.05, the time series is stationary\n",
        "# Otherwise, you might need to difference the data\n",
        "\n",
        "# Step 4: Fit the SARIMA Model\n",
        "# SARIMA(p, d, q)(P, D, Q, s)\n",
        "# where (P, D, Q, s) are the seasonal parameters and s is the length of the seasonal cycle\n",
        "# Let's assume initial parameters: (1, 1, 1)(1, 1, 1, 12)\n",
        "model = SARIMAX(df['sales'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
        "model_fit = model.fit(disp=False)\n",
        "\n",
        "# Print model summary\n",
        "print(model_fit.summary())\n",
        "\n",
        "# Step 5: Make Forecasts\n",
        "# Forecast the next 12 months\n",
        "forecast = model_fit.get_forecast(steps=12)\n",
        "forecast_index = pd.date_range(start=df.index[-1], periods=12, freq='M')\n",
        "forecast_series = pd.Series(forecast.predicted_mean, index=forecast_index)\n",
        "\n",
        "# Plot the forecast\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['sales'], label='Observed')\n",
        "plt.plot(forecast_series, label='Forecast', color='red')\n",
        "plt.title('Sales Forecast')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 6: Evaluate the Model\n",
        "# If you have a test set, you can compare the forecast against the actual values\n",
        "# For this example, let's use the last 12 months as a test set\n",
        "train = df['sales'][:-12]\n",
        "test = df['sales'][-12:]\n",
        "\n",
        "# Fit the model on the training set\n",
        "model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
        "model_fit = model.fit(disp=False)\n",
        "\n",
        "# Forecast the test set period\n",
        "forecast = model_fit.get_forecast(steps=12)\n",
        "forecast_series = pd.Series(forecast.predicted_mean, index=test.index)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train, label='Training')\n",
        "plt.plot(test, label='Test', color='green')\n",
        "plt.plot(forecast_series, label='Forecast', color='red')\n",
        "plt.title('Sales Forecast vs Actual')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(test, forecast_series))\n",
        "print(f'Root Mean Squared Error: {rmse}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Predicting a Churn"
      ],
      "metadata": {
        "id": "N4FqUI7b6laW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_csv('telecom_churn_data.csv')\n",
        "\n",
        "# Data Cleaning\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Feature Engineering\n",
        "df['total_calls'] = df['day_calls'] + df['evening_calls'] + df['night_calls'] + df['international_calls']\n",
        "df['total_charge'] = df['day_charge'] + df['evening_charge'] + df['night_charge'] + df['international_charge']\n",
        "\n",
        "# Encoding categorical variables\n",
        "df = pd.get_dummies(df, columns=['state', 'area_code', 'international_plan', 'voice_mail_plan'])\n",
        "\n",
        "# Split Data\n",
        "X = df.drop('churn', axis=1)\n",
        "y = df['churn']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Model Training\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "vlbk-fVY6kC6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Time Series Forecasting"
      ],
      "metadata": {
        "id": "CHFIuxMo6o9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_csv('monthly_sales.csv', parse_dates=['date'], index_col='date')\n",
        "\n",
        "# Decompose Time Series\n",
        "result = seasonal_decompose(df['sales'], model='additive')\n",
        "result.plot()\n",
        "plt.show()\n",
        "\n",
        "# Train ARIMA Model\n",
        "model = ARIMA(df['sales'], order=(1, 1, 1))\n",
        "model_fit = model.fit()\n",
        "\n",
        "# Forecast\n",
        "forecast = model_fit.forecast(steps=12)\n",
        "forecast.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nwiFLTTO6nPA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Market Basket Analysis"
      ],
      "metadata": {
        "id": "SxBHyURM6tU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_csv('ecommerce_transactions.csv')\n",
        "\n",
        "# Preprocess Data\n",
        "basket = (df.groupby(['transaction_id', 'product_name'])['quantity']\n",
        "          .sum().unstack().reset_index().fillna(0)\n",
        "          .set_index('transaction_id'))\n",
        "\n",
        "def encode_units(x):\n",
        "    return 1 if x >= 1 else 0\n",
        "\n",
        "basket = basket.applymap(encode_units)\n",
        "\n",
        "# Apriori Algorithm\n",
        "frequent_itemsets = apriori(basket, min_support=0.01, use_colnames=True)\n",
        "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
        "\n",
        "# Display Rules\n",
        "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])"
      ],
      "metadata": {
        "id": "cUnoV7SQ6rOR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Sentiment Analysis on Social Media"
      ],
      "metadata": {
        "id": "iWcHlm786wS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_csv('social_media_reviews.csv')\n",
        "\n",
        "# Clean Data\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
        "    return text\n",
        "\n",
        "df['cleaned_review'] = df['review'].apply(clean_text)\n",
        "\n",
        "# Preprocess Text\n",
        "vectorizer = CountVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(df['cleaned_review']).toarray()\n",
        "y = df['sentiment']\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "EoV7bbgB6vFQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Anomaly Detection in Network Traffic"
      ],
      "metadata": {
        "id": "YmEZqQiu6ztl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_csv('network_traffic.csv')\n",
        "\n",
        "# Feature Selection\n",
        "features = ['duration', 'src_bytes', 'dst_bytes', 'count', 'srv_count']\n",
        "\n",
        "X = df[features]\n",
        "\n",
        "# Train Isolation Forest\n",
        "model = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n",
        "model.fit(X)\n",
        "\n",
        "# Predict Anomalies\n",
        "df['anomaly'] = model.predict(X)\n",
        "\n",
        "# Visualize Anomalies\n",
        "anomalies = df[df['anomaly'] == -1]\n",
        "\n",
        "plt.scatter(df['src_bytes'], df['dst_bytes'], c=df['anomaly'], cmap='coolwarm')\n",
        "plt.xlabel('Source Bytes')\n",
        "plt.ylabel('Destination Bytes')\n",
        "plt.title('Anomaly Detection in Network Traffic')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3Vn8V_GS6ydf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load a CSV File"
      ],
      "metadata": {
        "id": "2IlP9a8G67tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data.csv')"
      ],
      "metadata": {
        "id": "zY5gtLLf62BD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mutate Strings"
      ],
      "metadata": {
        "id": "N9WR8r5q6-Z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a string named str\n",
        "str = \"Rahim and Karim\"\n",
        "\n",
        "# Convert a string to uppercase\n",
        "str.upper() # 'RAHIM AND KARIM'\n",
        "\n",
        "# Convert a string to lowercase\n",
        "str.lower() # 'rahim and karim'\n",
        "\n",
        "# Convert a string to title case\n",
        "str.title() # 'Rahim And Karim'\n",
        "\n",
        "# Replaces matches of a substring with another\n",
        "str.replace(\"J\", \"P\") # 'Kahim and Rarim'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "phLtwdEv69HK",
        "outputId": "1ccf63d3-2e6e-4f24-beb3-5a62f9229ed4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Rahim and Karim'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handle Missing Values & Remove Duplicates"
      ],
      "metadata": {
        "id": "MlUISKI77CIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)  # Drop missing values\n",
        "df.fillna(df.mean(), inplace=True)  # Fill missing values with mean\n",
        "\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "-bUIJrF-7BFY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rename Columns"
      ],
      "metadata": {
        "id": "mY6hY0wY7FZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={'old_name': 'new_name'}, inplace=True)"
      ],
      "metadata": {
        "id": "S7mLdqm77EDN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group By and Aggregate"
      ],
      "metadata": {
        "id": "yqfptt5Z7IjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df = df.groupby('column').agg({'col1': 'sum', 'col2': 'mean'})"
      ],
      "metadata": {
        "id": "GYTTjHPM7G_N"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge DataFrames"
      ],
      "metadata": {
        "id": "QEk0e9yv7Me2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(df1, df2, on='common_column', how='inner')"
      ],
      "metadata": {
        "id": "khhT4S3F7KIS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pivot Table"
      ],
      "metadata": {
        "id": "9hj2WyCu7QDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pivot = df.pivot_table(values='value', index='index', columns='columns', aggfunc='mean')"
      ],
      "metadata": {
        "id": "f-J7mfrY7Oon"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply Function to Column"
      ],
      "metadata": {
        "id": "58l7z1oB7V8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['new_column'] = df['column'].apply(lambda x: x*2)"
      ],
      "metadata": {
        "id": "xFHFKG1b7Rnv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation — MSE and R²"
      ],
      "metadata": {
        "id": "geH_q0rM7YrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print(f'MSE: {mse}, R²: {r2}')"
      ],
      "metadata": {
        "id": "70byp3457XV_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model"
      ],
      "metadata": {
        "id": "8sj0Gx2v7c75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(model, 'model.pkl')"
      ],
      "metadata": {
        "id": "bGy77WAo7bID"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time Series — Convert to Datetime"
      ],
      "metadata": {
        "id": "RFgs4DA68LIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df.set_index('date', inplace=True)"
      ],
      "metadata": {
        "id": "4BJGFp_O7uAS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rolling Window Calculation"
      ],
      "metadata": {
        "id": "k3zX5B8C8O2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['rolling_mean'] = df['column'].rolling(window=12).mean()"
      ],
      "metadata": {
        "id": "m9tSMn0m8NSm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal Component Analysis (PCA)"
      ],
      "metadata": {
        "id": "hy56iQI88Rg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "principal_components = pca.fit_transform(df_scaled)"
      ],
      "metadata": {
        "id": "VoRk2z668QT9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Means Clustering"
      ],
      "metadata": {
        "id": "TBTjBOQ-8U_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "kmeans.fit(df_scaled)\n",
        "df['cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "2-tYe06d8TB_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform SQL Query on DataFrame"
      ],
      "metadata": {
        "id": "L5G0Gbky8Xyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandasql as psql\n",
        "query = \"SELECT * FROM df WHERE column > value\"\n",
        "result = psql.sqldf(query, locals())"
      ],
      "metadata": {
        "id": "yl6vIO8b8WWa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Dashboard with Plotly"
      ],
      "metadata": {
        "id": "ttiSHtjQ8cPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "fig = px.line(df, x='date', y='value')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "4AMpN3ad8avy"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}