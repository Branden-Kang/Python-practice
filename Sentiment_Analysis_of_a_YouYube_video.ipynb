{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis of a YouYube video.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3oJmQv+Q9oCMwgCkJkZEK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZgzm8BWOLzt"
      },
      "source": [
        "[Reference](https://ai.plainenglish.io/sentiment-analysis-of-a-youtube-video-part-3-b648d19bd666)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG05LGF2N6uu"
      },
      "source": [
        "import os\n",
        "import googleapiclient.discovery\n",
        "\n",
        "# Fetching the data from Youtube API\n",
        "def google_api(key,vidId):\n",
        "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
        "\n",
        "    api_service_name = \"youtube\"\n",
        "    api_version = \"v3\"\n",
        "    DEVELOPER_KEY = key\n",
        "\n",
        "    youtube = googleapiclient.discovery.build(\n",
        "        api_service_name, api_version, developerKey = DEVELOPER_KEY)\n",
        "\n",
        "    request = youtube.commentThreads().list(\n",
        "        part=\"id,snippet\",\n",
        "        maxResults=100,\n",
        "        order=\"relevance\",\n",
        "        videoId=vidId\n",
        "    )\n",
        "    response = request.execute()\n",
        "    return response\n",
        "key = \"Your_API_KEY\"  \n",
        "vidId = \"621oD2zBSbI\"\n",
        "response = google_api(key,vidId)\n",
        "print(response)\n",
        "\n",
        "# Creation of Data Frame\n",
        "import pandas as pd \n",
        "def create_df_author_comments(response):\n",
        "  authorname = []\n",
        "  comments = []\n",
        "  for i in range(len(response[\"items\"])):\n",
        "    authorname.append(response[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorDisplayName\"])\n",
        "    comments.append(response[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textOriginal\"])\n",
        "  df_1 = pd.DataFrame(comments, index = authorname,columns=[\"Comments\"])\n",
        "  return df_1 \n",
        "df = create_df_author_comments(response)\n",
        "print(df)\n",
        "\n",
        "# Cleaning Round 1\n",
        "import re\n",
        "def cleaning_comments(comment):\n",
        "  comment = re.sub(\"[🤣|🤭|🤣|😁|🤭|❤️|👍|🏴|😣|😠|💪|🙏]+\",'',comment)\n",
        "  comment = re.sub(\"[0-9]+\",\"\",comment)\n",
        "  comment = re.sub(\"[\\:|\\@|\\)|\\*|\\.|\\$|\\!|\\?|\\,|\\%|\\\"]+\",\" \",comment)\n",
        "  return comment\n",
        "df[\"Comments\"]= df[\"Comments\"].apply(cleaning_comments)\n",
        "print(df)\n",
        "\n",
        "# Cleaning Round 2\n",
        "def cleaning_comments1(comment):\n",
        "  comment = re.sub(\"[💁|🌾|😎|♥|🤷‍♂]+\",\"\",comment)\n",
        "  comment = re.sub(\"[\\(|\\-|\\”|\\“|\\#|\\!|\\/|\\«|\\»|\\&]+\",\"\",comment)\n",
        "  return comment \n",
        "\n",
        "df[\"Comments\"] = df[\"Comments\"].apply(cleaning_comments1)\n",
        "print(df)\n",
        "\n",
        "# Cleaning Round 3\n",
        "def cleaning_comments3(comment):\n",
        "  comment = re.sub(\"\\n\",\" \",comment)\n",
        "  comment = re.sub('[\\'|🇵🇰|\\;|\\！]+','',comment)\n",
        "  return comment \n",
        "df[\"Comments\"] = df[\"Comments\"].apply(cleaning_comments3)\n",
        "print(df)\n",
        "\n",
        "#Cleaning Round 4 \n",
        "lower = lambda comment: comment.lower()\n",
        "df['Comments'] = df['Comments'].apply(lower)\n",
        "print(df)\n",
        "\n",
        "# Final DataFrame after cleaning\n",
        "print(df)\n",
        "\n",
        "#Removing Empty Comments\n",
        "def remove_comments(df):\n",
        "  # Checks for comments which has zero length in a dataframe\n",
        "  zero_length_comments = df[df[\"Comments\"].map(len) == 0]\n",
        "  # taking all the indexes of the filtered comments in a list\n",
        "  zero_length_comments_index = [ind for ind in zero_length_comments.index]\n",
        "  # removing those rows from dataframe whose indexes matches \n",
        "  df.drop(zero_length_comments_index, inplace = True)\n",
        "  return df\n",
        "df = remove_comments(df)\n",
        "df\n",
        "\n",
        "# Removing Non English Comments\n",
        "from textblob.blob import TextBlob\n",
        "\n",
        "lang_detection = lambda text: TextBlob(text).detect_language()\n",
        "\n",
        "def remove_non_english_comments(df):\n",
        "  comment = df[df[\"Comments\"].map(lang_detection) != 'en']\n",
        "  authors = [author for author in comment.index]\n",
        "  df.drop(authors,inplace = True)\n",
        "  return df\n",
        "\n",
        "df = remove_non_english_comments(df)\n",
        "df  \n",
        "\n",
        "#Finding Subjectivity\n",
        "\n",
        "def find_subjectivity_on_single_comment(text):\n",
        "  return TextBlob(text).sentiment.subjectivity\n",
        "   \n",
        "def apply_subjectivity_on_all_comments(df):\n",
        "  df['Subjectivity'] = df['Comments'].apply(find_subjectivity_on_single_comment)\n",
        "  return df \n",
        "\n",
        "df = apply_subjectivity_on_all_comments(df)\n",
        "df\n",
        "\n",
        "#Finding Polarity\n",
        "\n",
        "def find_polarity_of_single_comment(text):\n",
        "   return  TextBlob(text).sentiment.polarity\n",
        "\n",
        "def find_polarity_of_every_comment(df):  \n",
        "  df['Polarity'] = df['Comments'].apply(find_polarity_of_single_comment)\n",
        "  return df \n",
        "\n",
        "df = find_polarity_of_every_comment(df)\n",
        "df\n",
        "\n",
        "# Analysis Based on Polarity\n",
        "analysis = lambda polarity: 'Positive' if polarity > 0 else 'Neutral' if polarity == 0 else 'Negative' \n",
        "\n",
        "def analysis_based_on_polarity(df):\n",
        "  df['Analysis'] = df['Polarity'].apply(analysis)\n",
        "  return df\n",
        "  \n",
        "df = analysis_based_on_polarity(df)\n",
        "df\n",
        "\n",
        "# Printing Positive Comments\n",
        "\n",
        "def print_positive_comments():\n",
        "  print('Printing positive comments:\\n')\n",
        "  sortedDF = df.sort_values(by=['Polarity']) \n",
        "  for i in range(0, sortedDF.shape[0] ):\n",
        "    if( sortedDF['Analysis'][i] == 'Positive'):\n",
        "      print(str(i+1) + '> '+ sortedDF['Comments'][i])\n",
        "      print()\n",
        "print_positive_comments()\n",
        "\n",
        "# Printing Negative Comments\n",
        "\n",
        "def print_negative_comments():\n",
        "  print('Printing negative comments:\\n')\n",
        "  sortedDF = df.sort_values(by=['Polarity']) \n",
        "  for i in range(0, sortedDF.shape[0] ):\n",
        "    if( sortedDF['Analysis'][i] == 'Negative'):\n",
        "      print(str(i+1) + '> '+ sortedDF['Comments'][i])\n",
        "      print()\n",
        "print_negative_comments()\n",
        "\n",
        "# Printing Neutral Comments\n",
        "\n",
        "def print_neutral_comments():\n",
        "  print('Printing neutral comments:\\n')\n",
        "  sortedDF = df.sort_values(by=['Polarity']) \n",
        "  for i in range(0, sortedDF.shape[0] ):\n",
        "    if( sortedDF['Analysis'][i] == 'Neutral'):\n",
        "      print(str(i+1) + '> '+ sortedDF['Comments'][i])\n",
        "      print()\n",
        "print_neutral_comments()\n",
        "\n",
        "# Forming WordClouds\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction import text \n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def generate_word_clouds(df):\n",
        "  allWords = ' '.join([twts for twts in df['Comments']])\n",
        "  wordCloud = WordCloud(stopwords = text.ENGLISH_STOP_WORDS ,width=1000, height=600, random_state=21, max_font_size=110).generate(allWords)\n",
        "  plt.imshow(wordCloud, interpolation=\"bilinear\")\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "generate_word_clouds(df)"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}