{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1EjdbyEgdDNBkfyqjrBTh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@felixpratama242/etl-using-python-and-apache-airflow-aec67acda9b3)"
      ],
      "metadata": {
        "id": "3JiDN1f69w8Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cj6VXedJ9voH"
      },
      "outputs": [],
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.bash import BashOperator\n",
        "from datetime import datetime, date, timedelta\n",
        "\n",
        "#dag arguments\n",
        "default_dag = {\n",
        "    \"owner\": \"Felix Pratamasan\",\n",
        "    \"start_date\": date.today().isoformat(),\n",
        "    \"email\": [\"felixpratama242@gmail.com\"],\n",
        "    \"email_on_failure\": True,\n",
        "    \"email_on_entry\": True,\n",
        "    \"retries\":1,\n",
        "    \"retry_delay\": timedelta(minutes=5)\n",
        "}\n",
        "\n",
        "#define dag\n",
        "dag = DAG('ETL_toll_data',\n",
        "          schedule= timedelta(days=1),\n",
        "          default_args= default_dag,\n",
        "          description=\"Apache Airflow Final Assignment\"\n",
        "          )\n",
        "\n",
        "#task to unzip data\n",
        "unzip_data = BashOperator(\n",
        "    task_id= \"unzip_data\",\n",
        "    bash_command = \"tar -xvzf /mnt/c/Users/ASUS/Documents/projects/IBM-Data-Engineer/ETL-and-Data-Pipelines-with-Shell-Airflow-and-Kafka/Project5/tolldata.tgz\",\n",
        "    dag = dag\n",
        ")\n",
        "\n",
        "# task to extract_data_from_csv\n",
        "extract_data_from_csv = BashOperator(\n",
        "    task_id = \"extract_data_from_csv\",\n",
        "    bash_command = \"cut -d, -f1,2,3,4 /mnt/c/Users/ASUS/Documents/projects/IBM-Data-Engineer/ETL-and-Data-Pipelines-with-Shell-Airflow-and-Kafka/Project5/vehicle-data.csv > /mnt/c/Users/ASUS/Documents/projects/IBM-Data-Engineer/ETL-and-Data-Pipelines-with-Shell-Airflow-and-Kafka/Project5/csv_data.csv\", # -d for delimiter\n",
        "    dag = dag\n",
        ")\n",
        "\n",
        "#task to extract data from tsv\n",
        "extract_data_from_tsv = BashOperator(\n",
        "    task_id= \"extract_data_from_tsv\",\n",
        "    bash_command = \"cut -d$'\\t' -f 5,6,7 /mnt/c/Users/ASUS/Documents/projects/IBM-Data-Engineer/ETL-and-Data-Pipelines-with-Shell-Airflow-and-Kafka/Project5/tollplaza-data.tsv > /mnt/c/Users/ASUS/Documents/projects/IBM-Data-Engineer/ETL-and-Data-Pipelines-with-Shell-Airflow-and-Kafka/Project5/tsv_data.csv\", # -d$'\\t' for delimiter tab\n",
        "    dag = dag\n",
        ")\n",
        "\n",
        "# task to extract_data_from_fixed_width\n",
        "extract_data_from_fixed_width = BashOperator(\n",
        "    task_id = \"extract_data_from_fixed_width\",\n",
        "    bash_command = \"cut -c 59-62,63-67 /mnt/c/Users/ASUS/Documents/projects/IBM-Data-Engineer/ETL-and-Data-Pipelines-with-Shell-Airflow-and-Kafka/Project5/payment-data.txt > /mnt/c/Users/ASUS/Documents/projects/IBM-Data-Engineer/ETL-and-Data-Pipelines-with-Shell-Airflow-and-Kafka/Project5/fixed_width_data.csv\", # -c for --characters=LIST\n",
        "    dag = dag\n",
        ")\n",
        "\n",
        "csv_data = \"/mnt/c/Users/ASUS/Documents/projects/IBM-Data-Engineer/ETL-and-Data-Pipelines-with-Shell-Airflow-and-Kafka/Project5/csv_data.csv\"\n",
        "tsv_data = \"/mnt/c/Users/ASUS/Documents/projects/IBM-Data-Engineer/ETL-and-Data-Pipelines-with-Shell-Airflow-and-Kafka/Project5/tsv_data.csv\"\n",
        "fixed_width_data = \"/mnt/c/Users/ASUS/Documents/projects/IBM-Data-Engineer/ETL-and-Data-Pipelines-with-Shell-Airflow-and-Kafka/Project5/fixed_width_data.csv\"\n",
        "extracted_data = \"/mnt/c/Users/ASUS/Documents/projects/IBM-Data-Engineer/ETL-and-Data-Pipelines-with-Shell-Airflow-and-Kafka/Project5/extracted_data.csv\"\n",
        "# task to consolidate_data\n",
        "consolidate_data = BashOperator(\n",
        "    task_id = \"consolidate_data\",\n",
        "    bash_command = f\"paste {csv_data} {tsv_data} {fixed_width_data} > {extracted_data}\", # paste for merge files\n",
        "    dag = dag\n",
        ")\n",
        "\n",
        "# task to Transform and load the data\n",
        "transform_data = BashOperator(\n",
        "    task_id = \"transform_data\",\n",
        "    bash_command = \"awk 'BEGIN {FS=OFS=\\\",\\\"} { $4= toupper($4) } 1' /mnt/c/Users/ASUS/Documents/projects/IBM-Data-Engineer/ETL-and-Data-Pipelines-with-Shell-Airflow-and-Kafka/Project5/extracted_data.csv > /mnt/c/Users/ASUS/Documents/projects/IBM-Data-Engineer/ETL-and-Data-Pipelines-with-Shell-Airflow-and-Kafka/Project5/transformed_data.csv\",\n",
        "    dag = dag\n",
        ")\n",
        "\n",
        "# awk is command for text processing tool with various options that allow you to customize its behavior\n",
        "# FS=OFS=\",\": Sets the input and output field separator to a comma (,), assuming your CSV is comma-separated\n",
        "# $4 = toupper($4): Modifies the second field ($4) to its uppercase version using the toupper function\n",
        "# 1: A common awk pattern that evaluates to true and triggers the default action, which is to print the modified line.\n",
        "\n",
        "# Define task pipelines\n",
        "unzip_data >> extract_data_from_csv >> extract_data_from_tsv >> extract_data_from_fixed_width \\\n",
        "    >> consolidate_data >> transform_data"
      ]
    }
  ]
}