{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAf7FO0EklhJwHQRe4BPkR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://pub.towardsai.net/data-leakage-your-99-accuracy-model-is-a-lie-87b9cf473eff)"
      ],
      "metadata": {
        "id": "Y5z2q9dQdYR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X = df.drop(['churned', 'customer_id'], axis=1)\n",
        "y = df['churned']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "print(f\"Training Accuracy: {model.score(X_train, y_train)}\")  # 99.8%\n",
        "print(f\"Test Accuracy: {model.score(X_test, y_test)}\")        # 98.5%"
      ],
      "metadata": {
        "id": "k8PxMG-sdJWB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You left cancellation_date in the features.\n",
        "In production, you don’t have cancellation_date until AFTER they churn. Your model is useless.\n",
        "This is data leakage."
      ],
      "metadata": {
        "id": "HTPwW_padoSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Type 1: Target Leakage — The Smoking Gun\n",
        "## Real Example: Credit Card Fraud Detection"
      ],
      "metadata": {
        "id": "Wa8cJ8o2drNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THE LEAK (Wrong)\n",
        "features = ['transaction_amount', 'merchant', 'time',\n",
        "            'fraud_investigation_opened',  # ← LEAK!\n",
        "            'account_frozen']              # ← LEAK!"
      ],
      "metadata": {
        "id": "UTd7DHZBdfwj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NO LEAK (Correct)\n",
        "features = ['transaction_amount', 'merchant', 'time',\n",
        "            'historical_fraud_rate', 'location',\n",
        "            'device_fingerprint']"
      ],
      "metadata": {
        "id": "D0S0MMX0dvNS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to detect:"
      ],
      "metadata": {
        "id": "rFY-eL7pd2o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check feature importance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "importance = model.feature_importances_\n",
        "features_names = X.columns\n",
        "plt.barh(features_names, importance)\n",
        "# If ONE feature has 90%+ importance → INVESTIGATE\n",
        "# It's likely leakage"
      ],
      "metadata": {
        "id": "iG8u0QNzdx-r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Type 2: Train-Test Contamination — The Double Agent\n",
        "## Real Example: Feature Scaling Gone Wrong"
      ],
      "metadata": {
        "id": "FhrLU1hreA_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THE LEAK (Wrong)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale ALL data together\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)  # ← LEAK! Used test data stats\n",
        "# Then split\n",
        "X_train, X_test = train_test_split(X_scaled)"
      ],
      "metadata": {
        "id": "8sYQBemVd5Na"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NO LEAK (Correct)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Split FIRST\n",
        "X_train, X_test = train_test_split(X)\n",
        "# Scale SEPARATELY\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)    # Fit on train only\n",
        "X_test_scaled = scaler.transform(X_test)          # Transform test using train stats"
      ],
      "metadata": {
        "id": "NesNaCKueLb2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Type 3: Temporal Leakage — The Time Traveler"
      ],
      "metadata": {
        "id": "ZWXJQMfPhd93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THE LEAK (Wrong)\n",
        "df['price_tomorrow'] = df['price'].shift(-1)  # Tomorrow's price\n",
        "df['7day_future_avg'] = df['price'].rolling(7).mean().shift(-7)  # Future average\n",
        "\n",
        "X = df[['price_tomorrow', '7day_future_avg', 'volume']]\n",
        "y = df['buy_signal']"
      ],
      "metadata": {
        "id": "YQimYOQHeSMl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NO LEAK (Correct)\n",
        "df['price_yesterday'] = df['price'].shift(1)   # Past price\n",
        "df['7day_past_avg'] = df['price'].rolling(7).mean()  # Past average\n",
        "\n",
        "X = df[['price_yesterday', '7day_past_avg', 'volume']]\n",
        "y = df['buy_signal']"
      ],
      "metadata": {
        "id": "t86kie_8ecH3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your data should be sorted by time\n",
        "df = df.sort_values('date')\n",
        "\n",
        "# Your split should respect time order\n",
        "train_size = int(0.8 * len(df))\n",
        "train = df[:train_size]\n",
        "test = df[train_size:]\n",
        "# NEVER shuffle time-series data\n",
        "# NEVER use random split for time-series"
      ],
      "metadata": {
        "id": "vfdd5EPQef-_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Type 4: Duplicate Data Leakage — The Clone\n",
        "## Real Example: Image Classification"
      ],
      "metadata": {
        "id": "oTZAPlHwetP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THE LEAK (Wrong)\n",
        "images = load_images()  # Contains duplicates\n",
        "X_train, X_test = train_test_split(images)\n",
        "\n",
        "# Problem: Same image in both train and test\n",
        "# Model memorizes, doesn't generalize"
      ],
      "metadata": {
        "id": "JClI6gtEejfa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NO LEAK (Correct)\n",
        "# Remove duplicates BEFORE splitting\n",
        "images_df['image_hash'] = images_df['image'].apply(hash_image)\n",
        "images_df = images_df.drop_duplicates(subset=['image_hash'])"
      ],
      "metadata": {
        "id": "W23PPi-Ie2Gy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicates\n",
        "def find_duplicates(X_train, X_test):\n",
        "    train_hashes = set(X_train.apply(hash, axis=1))\n",
        "    test_hashes = set(X_test.apply(hash, axis=1))\n",
        "\n",
        "    overlap = train_hashes.intersection(test_hashes)\n",
        "\n",
        "    if overlap:\n",
        "        print(f\"WARNING: {len(overlap)} duplicates between train/test!\")\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "lKDDiSiFe5Gd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Type 5: Group Leakage — The Family Secret"
      ],
      "metadata": {
        "id": "yyR0KT-Qe-mM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THE LEAK (Wrong)\n",
        "# Patient 123 has 5 scans\n",
        "# 3 scans in training, 2 in test\n",
        "# Model learns patient-specific patterns, not disease patterns\n",
        "\n",
        "X_train, X_test = train_test_split(medical_scans)"
      ],
      "metadata": {
        "id": "UsfUeLlqe8nY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NO LEAK (Correct)\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "patients = medical_scans['patient_id']\n",
        "X = medical_scans.drop('patient_id', axis=1)\n",
        "y = medical_scans['disease']\n",
        "splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)\n",
        "for train_idx, test_idx in splitter.split(X, y, groups=patients):\n",
        "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]"
      ],
      "metadata": {
        "id": "EXcX3UY0fCdk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Type 6: Preprocessing Leakage — The Hidden Influencer"
      ],
      "metadata": {
        "id": "dXeNS9m4fJMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THE LEAK (Wrong)\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Fill missing values with mean of ALL data\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_filled = imputer.fit_transform(X)  # ← Used test data!\n",
        "X_train, X_test = train_test_split(X_filled)"
      ],
      "metadata": {
        "id": "-1gyCCI1fGPe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NO LEAK (Correct)\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Split first\n",
        "X_train, X_test = train_test_split(X)\n",
        "# Impute separately\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_filled = imputer.fit_transform(X_train)    # Fit on train\n",
        "X_test_filled = imputer.transform(X_test)          # Use train statistics"
      ],
      "metadata": {
        "id": "qPsqBTw0fMQ5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Type 7: Label Leakage — The Obvious Clue"
      ],
      "metadata": {
        "id": "0xlTphmWfjkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THE LEAK (Wrong)\n",
        "features = ['customer_lifetime_value',  # ← Only known after customer leaves!\n",
        "            'total_revenue',            # ← Includes future revenue!\n",
        "            'refund_requested',         # ← Happens during churn!\n",
        "            'account_status']           # ← Shows if churned!"
      ],
      "metadata": {
        "id": "LeLJcNWRfdp4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NO LEAK (Correct)\n",
        "features = ['customer_lifetime_value_to_date',  # Only past value\n",
        "            'total_revenue_last_month',         # Historical only\n",
        "            'support_ticket_count',             # Past behavior\n",
        "            'login_frequency']                  # Past behavior"
      ],
      "metadata": {
        "id": "KqjE74xnfnv9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate correlation with target\n",
        "correlation = X.corrwith(y).abs().sort_values(ascending=False)\n",
        "\n",
        "# If any feature has >0.9 correlation → INVESTIGATE\n",
        "high_corr = correlation[correlation > 0.9]\n",
        "print(\"Suspicious features (possible leakage):\")\n",
        "print(high_corr)"
      ],
      "metadata": {
        "id": "mCDfwWKIfopf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Type 8: Sample Leakage — The Repeated Witness\n",
        "## Real Example: Time-Series Cross-Validation"
      ],
      "metadata": {
        "id": "NW4fPW6sfuuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THE LEAK (Wrong)\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Standard K-Fold on time-series\n",
        "kfold = KFold(n_splits=5, shuffle=True)  # ← LEAK! Shuffles time order\n",
        "for train_idx, val_idx in kfold.split(X):\n",
        "    # Future data in training, past data in validation\n",
        "    # Breaks time causality\n",
        "    pass"
      ],
      "metadata": {
        "id": "9EbgHGz7fsjX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NO LEAK (Correct)\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Time-aware split\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "for train_idx, val_idx in tscv.split(X):\n",
        "    # Always: training data < validation data in time\n",
        "    # Respects causality\n",
        "    pass"
      ],
      "metadata": {
        "id": "QsrOEPIDf4i3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Type 9: External Data Leakage — The Insider Information\n",
        "\n",
        "## Real Example: Real Estate Price Prediction"
      ],
      "metadata": {
        "id": "u7eIsr4Lf-_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THE LEAK (Wrong)\n",
        "features = ['bedrooms', 'sqft',\n",
        "            'recent_sale_price',        # ← Not available for unlisted homes!\n",
        "            'zillow_estimate',          # ← Uses similar homes (includes target!)\n",
        "            'appraisal_value']          # ← Only done after sale interest!"
      ],
      "metadata": {
        "id": "NTOtiKXNf74a"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NO LEAK (Correct)\n",
        "features = ['bedrooms', 'sqft',\n",
        "            'neighborhood_avg_price',   # Historical average only\n",
        "            'property_age',\n",
        "            'school_rating']"
      ],
      "metadata": {
        "id": "5UAIITnigCGf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Type 10: Oversampling Leakage — The Synthetic Spy\n",
        "\n",
        "## Real Example: Fraud Detection\n"
      ],
      "metadata": {
        "id": "jNW-vdIugbjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THE LEAK (Wrong)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Oversample to balance classes\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)  # ← Created synthetic samples\n",
        "# Then split\n",
        "X_train, X_test = train_test_split(X_resampled, y_resampled)\n",
        "# Problem: Synthetic samples related to real samples in both sets"
      ],
      "metadata": {
        "id": "shKS1GvpgZZM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NO LEAK (Correct)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Split FIRST\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "# Oversample ONLY training data\n",
        "smote = SMOTE()\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "# Test data remains original (no synthetic samples)"
      ],
      "metadata": {
        "id": "f0D3UPSwgg3u"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Type 11: Feature Engineering Leakage — The Calculated Betrayal\n",
        "\n",
        "### Real Example: Target Encoding"
      ],
      "metadata": {
        "id": "dQ-ulxvhgnls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THE LEAK (Wrong)\n",
        "# Calculate mean target per category using ALL data\n",
        "category_means = df.groupby('category')['target'].mean()\n",
        "\n",
        "# Apply to entire dataset\n",
        "df['category_encoded'] = df['category'].map(category_means)\n",
        "# Then split\n",
        "X_train, X_test = train_test_split(df)"
      ],
      "metadata": {
        "id": "M-6IxokNgl14"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NO LEAK (Correct)\n",
        "# Split first\n",
        "train, test = train_test_split(df)\n",
        "\n",
        "# Calculate encoding ONLY on train\n",
        "category_means = train.groupby('category')['target'].mean()\n",
        "# Apply to both (using train statistics)\n",
        "train['category_encoded'] = train['category'].map(category_means)\n",
        "test['category_encoded'] = test['category'].map(category_means)\n",
        "# Handle unseen categories in test\n",
        "test['category_encoded'].fillna(train['target'].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "pv72sYKFgsQD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Type 12: Cross-Validation Leakage — The Fold Conspiracy\n",
        "## Real Example: Feature Selection"
      ],
      "metadata": {
        "id": "LiN8tWDsgyKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THE LEAK (Wrong)\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Select features using ALL data\n",
        "selector = SelectKBest(k=10)\n",
        "X_selected = selector.fit_transform(X, y)  # ← Used all data!\n",
        "# Then cross-validate\n",
        "scores = cross_val_score(model, X_selected, y, cv=5)"
      ],
      "metadata": {
        "id": "hRvslBULgvhQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NO LEAK (Correct)\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('selector', SelectKBest(k=10)),\n",
        "    ('model', RandomForestClassifier())\n",
        "])\n",
        "# Feature selection happens INSIDE each fold\n",
        "scores = cross_val_score(pipeline, X, y, cv=5)"
      ],
      "metadata": {
        "id": "TEAQunIxg3_y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Leak Detector: Automated Tool"
      ],
      "metadata": {
        "id": "rZpm-TaJhHgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeakageDetector:\n",
        "    def __init__(self, X_train, X_test, y_train, y_test):\n",
        "        self.X_train = X_train\n",
        "        self.X_test = X_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "\n",
        "    def check_all(self):\n",
        "        print(\"=\" * 50)\n",
        "        print(\"LEAKAGE DETECTION REPORT\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        self.check_duplicates()\n",
        "        self.check_target_correlation()\n",
        "        self.check_feature_importance()\n",
        "        self.check_perfect_accuracy()\n",
        "\n",
        "    def check_duplicates(self):\n",
        "        train_hashes = set(pd.util.hash_pandas_object(self.X_train))\n",
        "        test_hashes = set(pd.util.hash_pandas_object(self.X_test))\n",
        "        overlap = train_hashes.intersection(test_hashes)\n",
        "\n",
        "        if overlap:\n",
        "            print(f\"⚠️  DUPLICATE LEAK: {len(overlap)} samples in both sets\")\n",
        "        else:\n",
        "            print(\"✅ No duplicates found\")\n",
        "\n",
        "    def check_target_correlation(self):\n",
        "        correlations = self.X_train.corrwith(self.y_train).abs()\n",
        "        high_corr = correlations[correlations > 0.9]\n",
        "\n",
        "        if len(high_corr) > 0:\n",
        "            print(f\"⚠️  HIGH CORRELATION LEAK: {len(high_corr)} features\")\n",
        "            print(high_corr)\n",
        "        else:\n",
        "            print(\"✅ No suspicious correlations\")\n",
        "\n",
        "    def check_feature_importance(self, model):\n",
        "        model.fit(self.X_train, self.y_train)\n",
        "        importance = model.feature_importances_\n",
        "\n",
        "        if max(importance) > 0.8:\n",
        "            dominant_feature = self.X_train.columns[np.argmax(importance)]\n",
        "            print(f\"⚠️  DOMINANCE LEAK: '{dominant_feature}' has {max(importance)*100:.1f}% importance\")\n",
        "        else:\n",
        "            print(\"✅ Feature importance distributed\")\n",
        "\n",
        "    def check_perfect_accuracy(self, model):\n",
        "        model.fit(self.X_train, self.y_train)\n",
        "        train_acc = model.score(self.X_train, self.y_train)\n",
        "        test_acc = model.score(self.X_test, self.y_test)\n",
        "\n",
        "        if test_acc > 0.99:\n",
        "            print(f\"⚠️  TOO GOOD LEAK: {test_acc*100:.1f}% test accuracy\")\n",
        "        elif train_acc - test_acc > 0.2:\n",
        "            print(f\"⚠️  OVERFITTING: {(train_acc-test_acc)*100:.1f}% gap\")\n",
        "        else:\n",
        "            print(f\"✅ Reasonable accuracy: train={train_acc:.3f}, test={test_acc:.3f}\")\n",
        "\n",
        "            # Usage\n",
        "detector = LeakageDetector(X_train, X_test, y_train, y_test)\n",
        "detector.check_all()"
      ],
      "metadata": {
        "id": "RnflFNS7g7Gb"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}
