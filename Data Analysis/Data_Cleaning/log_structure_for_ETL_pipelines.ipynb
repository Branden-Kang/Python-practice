{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "log structure for ETL pipelines.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMXvQlYu5F+S+tuZEBo9L71"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://towardsdatascience.com/what-to-log-from-python-etl-pipelines-9e0cfe29950e)"
      ],
      "metadata": {
        "id": "V76K61T5BkRo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qwkltdHGBaZK"
      },
      "outputs": [],
      "source": [
        "def extract():\n",
        "    pass\n",
        "\n",
        "def transformation():\n",
        "    pass\n",
        "\n",
        "def load():\n",
        "    pass\n",
        "\n",
        "def main():\n",
        "    extract()\n",
        "    transformation()\n",
        "    load()\n",
        "    \n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging.config\n",
        "import time\n",
        "import psutil\n",
        "import configparser\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "##basic config\n",
        "##logging.config.fileConfig('logging.conf')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "\n",
        "#job parameters config\n",
        "config = configparser.ConfigParser()\n",
        "config.read('https://raw.githubusercontent.com/Shivakoreddi/ETL-Log-Structure/main/etlConfig.ini')\n",
        "JobConfig = config['ETL_Log_Job']\n",
        "\n",
        "\n",
        "formatter = logging.Formatter('%(levelname)s:  %(asctime)s:  %(process)s:  %(funcName)s:  %(message)s')\n",
        "##creating handler\n",
        "stream_handler = logging.StreamHandler()\n",
        "file_handler = logging.FileHandler(JobConfig['LogName'])\n",
        "file_handler.setFormatter(formatter)\n",
        "logger.addHandler(file_handler)\n",
        "\n",
        "\n",
        "def extract():\n",
        "\n",
        "    logger.info('Start Extract Session')\n",
        "    logger.info('Source Filename: {}'.format(JobConfig['SrcObject']))\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(JobConfig['SrcObject'])\n",
        "        logger.info('Records count in source file: {}'.format(len(df.index)))\n",
        "    except ValueError as e:\n",
        "        logger.error(e)\n",
        "        return\n",
        "    logger.info(\"Read completed!!\")\n",
        "    return df\n",
        "\n",
        "def transformation(tdf):\n",
        "    try:\n",
        "        tdf = pd.read_csv(JobConfig['SrcObject'])\n",
        "        tdf[['fname', 'lname']] = tdf.NAME.str.split(expand=True)\n",
        "        ndf = tdf[['ID', 'fname', 'lname', 'ADDRESS']]\n",
        "        logger.info('Transformation completed, data ready to load!')\n",
        "    except Exception as e:\n",
        "        logger.error(e)\n",
        "        return\n",
        "    return ndf\n",
        "\n",
        "def load(ldf):\n",
        "    logger.info('Start Load Session')\n",
        "    try:\n",
        "        conn = sqlite3.connect(JobConfig['TgtConnection'])\n",
        "        cursor = conn.cursor()\n",
        "        logger.info('Connection to {} database established'.format(JobConfig['TgtConnection1']))\n",
        "    except Exception as e:\n",
        "        logger.error(e)\n",
        "        return\n",
        "    #3Load dataframe to table\n",
        "    try:\n",
        "        for index,row in ldf.iterrows():\n",
        "            query = \"\"\"INSERT OR REPLACE INTO {0}(id,fname,lname,address) VALUES('{1}','{2}','{3}','{4}')\"\"\".format(JobConfig['TgtObject'],row['ID'],row['fname'],row['lname'],row['ADDRESS'])\n",
        "            cursor.execute(query)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(e)\n",
        "        return\n",
        "    conn.commit()\n",
        "    logger.info(\"Data Loaded into target table: {}\".format(JobConfig['TgtObject']))\n",
        "    return\n",
        "\n",
        "def main():\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    ##extract\n",
        "    start1 = time.time()\n",
        "    tdf = extract()\n",
        "    end1 = time.time() - start1\n",
        "    logger.info('Extract CPU usage {}%'.format(psutil.cpu_percent()))\n",
        "    logger.info(\"Extract function took : {} seconds\".format(end1))\n",
        "\n",
        "    ##transformation\n",
        "    start2 = time.time()\n",
        "    ldf = transformation(tdf)\n",
        "    end2 = time.time() - start2\n",
        "    logger.info('Transform CPU usage {}%'.format(psutil.cpu_percent()))\n",
        "    logger.info(\"Transformation took : {} seconds\".format(end2))\n",
        "\n",
        "    ##load\n",
        "    start3 = time.time()\n",
        "    load(ldf)\n",
        "    end3 = time.time() - start3\n",
        "    logger.info('Load CPU usage {}%'.format(psutil.cpu_percent()))\n",
        "    logger.info(\"Load took : {} seconds\".format(end3))\n",
        "    end = time.time() - start\n",
        "    logger.info(\"ETL Job took : {} seconds\".format(end))\n",
        "    ##p = psutil.Process()\n",
        "    ##ls = p.as_dict()\n",
        "    ##print(p.as_dict())\n",
        "    logger.info('Session Summary')\n",
        "    logger.info('RAM memory {}% used:'.format(psutil.virtual_memory().percent))\n",
        "    logger.info('CPU usage {}%'.format(psutil.cpu_percent()))\n",
        "    print(\"multiple threads took : {} seconds\".format(end))\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    logger.info('ETL Process Initialized')\n",
        "    main()"
      ],
      "metadata": {
        "id": "iG3Bh5cfBeKo"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}