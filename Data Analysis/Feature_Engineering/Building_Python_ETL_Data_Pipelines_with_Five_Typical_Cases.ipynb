{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODvDJIcOJxAwDL3LMGzELc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@tubelwj/building-python-etl-data-pipelines-with-five-typical-cases-bcf130c27bfa)"
      ],
      "metadata": {
        "id": "KoWUAH1-lsw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning and transforming CSV files"
      ],
      "metadata": {
        "id": "ehKu5GKYl0mg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h9CoDts4lq8Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def clean_csv_data(input_file, output_file):\n",
        "\n",
        "    \"\"\"\n",
        "    Cleans and processes a CSV file and saves the cleaned data to a new file.\n",
        "\n",
        "    Parameters:\n",
        "    - input_file: str, path to the input CSV file\n",
        "    - output_file: str, path to save the cleaned CSV file\n",
        "    \"\"\"\n",
        "\n",
        "    # Read the CSV file, skipping any malformed rows\n",
        "    df = pd.read_csv(input_file, on_bad_lines='skip')\n",
        "\n",
        "    # Remove duplicate rows\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    # Fill missing values with appropriate defaults\n",
        "    df = df.fillna({\n",
        "        'total_price': df['total_price'].mean(),   # Fill missing 'total_price' with the mean value\n",
        "        'product': 'ordinary product',              # Fill missing 'product' with 'ordinary product'\n",
        "        'address': 'unknown'         # Fill missing 'city' with 'unknown'\n",
        "    })\n",
        "\n",
        "    # Filter out rows with price values outside the valid range (0 to 1000)\n",
        "    df = df[df['total_price'].between(0, 1000)]\n",
        "\n",
        "    # Save the cleaned data to the specified output file\n",
        "    df.to_csv(output_file, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrating Data from Multiple Sources"
      ],
      "metadata": {
        "id": "BkV5lp_ml5HZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def integrate_multiple_sources():\n",
        "    \"\"\"\n",
        "    Integrates data from multiple sources: API, Excel file, and database.\n",
        "\n",
        "    Steps:\n",
        "    1. Fetch data from a web API.\n",
        "    2. Read data from a local Excel file.\n",
        "    3. Query data from a database.\n",
        "    4. Combine the data into a single DataFrame.\n",
        "    5. Remove duplicate entries based on 'order_id'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Fetch data from the web API\n",
        "    api_data = requests.get('https://api.example.com/online_order_data').json()\n",
        "\n",
        "    # Read data from a local Excel file\n",
        "    excel_data = pd.read_excel('local_orders_data.xlsx')\n",
        "\n",
        "    # Query data from a database\n",
        "    db_data = pd.read_sql('SELECT * FROM orders', db_engine)\n",
        "\n",
        "    # Combine all sources into a single DataFrame\n",
        "    combined_data = pd.concat(\n",
        "        [pd.DataFrame(api_data), excel_data, db_data],\n",
        "        ignore_index=True\n",
        "    )\n",
        "\n",
        "    # Remove duplicates based on 'order_id'\n",
        "    combined_data = combined_data.drop_duplicates(subset=['order_id'])\n",
        "\n",
        "    return combined_data"
      ],
      "metadata": {
        "id": "C7jDCmdFl3Ck"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real-Time Data Stream Processing"
      ],
      "metadata": {
        "id": "B9UGn5Zjl81F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kafka"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkQ_mt0MmF7B",
        "outputId": "60594b13-264a-45e4-897d-5fb477050008"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kafka\n",
            "  Downloading kafka-1.3.5-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Downloading kafka-1.3.5-py2.py3-none-any.whl (207 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/207.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kafka\n",
            "Successfully installed kafka-1.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kafka import KafkaConsumer\n",
        "import json\n",
        "\n",
        "def process_stream_data():\n",
        "    \"\"\"\n",
        "    Processes real-time data from a Kafka topic.\n",
        "\n",
        "    Steps:\n",
        "    1. Consume messages from the 'order_topic' Kafka topic.\n",
        "    2. Deserialize the incoming data from JSON format.\n",
        "    3. Perform a simple transformation on the data.\n",
        "    4. Save the transformed data to a database.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the Kafka consumer to consume messages from 'order_topic'\n",
        "    consumer = KafkaConsumer(\n",
        "        'order_topic',\n",
        "        bootstrap_servers=['localhost:9092'],\n",
        "        value_deserializer=lambda x: json.loads(x.decode('utf-8'))  # Decode and parse JSON\n",
        "    )\n",
        "\n",
        "    # Process messages as they arrive\n",
        "    for msg in consumer:\n",
        "        data = msg.value  # Extract the message content\n",
        "\n",
        "        # Transform the data into a structured format\n",
        "        transformed = {\n",
        "            'order_id': data['id'],          # Map 'id' to 'order_id'\n",
        "            'order_amount': float(data['amt']),    # Convert 'amt' to float\n",
        "            'timestamp': data['ts']          # Retain the 'ts' as 'timestamp'\n",
        "        }\n",
        "\n",
        "        # Save the transformed data to the database\n",
        "        save_to_database(transformed)"
      ],
      "metadata": {
        "id": "c1NvJSSrl7ON"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Database Incremental Sync"
      ],
      "metadata": {
        "id": "B97QLpJImHa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine\n",
        "import pandas as pd\n",
        "\n",
        "def sync_incremental_data():\n",
        "    \"\"\"\n",
        "    Synchronizes incremental data from a source database to a target database.\n",
        "\n",
        "    Steps:\n",
        "    1. Connect to the source and target databases.\n",
        "    2. Retrieve the last synchronization timestamp.\n",
        "    3. Query only the new or updated data since the last sync.\n",
        "    4. Process the data in chunks and insert it into the target database.\n",
        "    \"\"\"\n",
        "\n",
        "    # Connect to the source database (MySQL)\n",
        "    source = create_engine('mysql://user:pass@localhost/source_db')\n",
        "\n",
        "    # Connect to the target database (PostgreSQL)\n",
        "    target = create_engine('postgresql://user:pass@localhost/target_db')\n",
        "\n",
        "    # Retrieve the last synchronization timestamp\n",
        "    last_sync = get_last_sync_time()\n",
        "\n",
        "    # Query to fetch only the incremental data\n",
        "    query = f\"\"\"\n",
        "    SELECT * FROM orders\n",
        "    WHERE update_time > '{last_sync}'\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the chunk size for batch processing\n",
        "    chunk_size = 100\n",
        "\n",
        "    # Read data in chunks from the source database and insert into the target database\n",
        "    for chunk in pd.read_sql(query, source, chunksize=chunk_size):\n",
        "        # Append each chunk to the 'orders' table in the target database\n",
        "        chunk.to_sql('orders', target, if_exists='append', index=False)"
      ],
      "metadata": {
        "id": "dEgMxnOtl_cc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporting Data into Reports"
      ],
      "metadata": {
        "id": "wMJxxnlUmTeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_report(df):\n",
        "    \"\"\"\n",
        "    Generates a report summarizing sales and quantity data, creates an Excel report,\n",
        "    and saves a bar chart visualization.\n",
        "\n",
        "    Parameters:\n",
        "    - df (DataFrame): Input data containing sales and quantity information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Aggregate data: Summarize total and average sales, and total quantity per category\n",
        "    summary = (\n",
        "        df.groupby('category')\n",
        "        .agg({\n",
        "            'order_amout': ['sum', 'mean'],  # Calculate total and average sales\n",
        "            'quantity': 'sum'         # Calculate total quantity\n",
        "        })\n",
        "        .round(2)  # Round values to 2 decimal places\n",
        "    )\n",
        "\n",
        "    # Create an Excel report with the summary data\n",
        "    with pd.ExcelWriter('report.xlsx') as writer:\n",
        "        summary.to_excel(writer, sheet_name='Summary')  # Write summary to the 'Summary' sheet\n",
        "\n",
        "    # Generate a bar chart for total sales per category\n",
        "    plt.figure(figsize=(12, 8))  # Set the figure size\n",
        "    summary['order_amount']['sum'].plot(kind='bar', title='Total order_amount by Category')  # Create a bar chart\n",
        "    plt.xlabel('Category')  # Label the x-axis\n",
        "    plt.ylabel('Total Sales')  # Label the y-axis\n",
        "    plt.tight_layout()  # Adjust layout for better appearance\n",
        "    plt.savefig('order_amount_chart.png')  # Save the chart as a PNG file"
      ],
      "metadata": {
        "id": "EA5j9-kzmQKE"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}
