{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7r/FHHzqfMIbEQoOM0YhE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@nomannayeem/from-messy-to-magic-a-beginner-to-expert-guide-on-data-cleaning-and-preprocessing-with-python-044ed8a3eb1f)"
      ],
      "metadata": {
        "id": "G2fcPG88A2X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating the Dataset"
      ],
      "metadata": {
        "id": "fwboSDjG4o0f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpZSP0604jtA",
        "outputId": "df8437e5-4b76-4796-8f8a-dffeca76df11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lengths: [102, 102, 102, 102]\n",
            "Synthetic dataset created:\n",
            "  customer_id  purchase_value region purchase_date\n",
            "0       CUST1      199.779257  North    2023-01-01\n",
            "1       CUST2      476.342867  South    2023-01-02\n",
            "2       CUST3      371.357092   East    2023-01-03\n",
            "3       CUST4      307.356072   West    2023-01-04\n",
            "4       CUST5       94.888947  North    2023-01-05\n",
            "5       CUST6       94.877370  South    2023-01-06\n",
            "6       CUST7       47.880134   East    2023-01-07\n",
            "7       CUST8      435.764550   West    2023-01-08\n",
            "8       CUST9      308.535206  North    2023-01-09\n",
            "9      CUST10      359.874837  South    2023-01-10\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Step 1: Í∏∞Î≥∏ Îç∞Ïù¥ÌÑ∞ 95Í∞ú ÏÉùÏÑ±\n",
        "customer_ids = [f'CUST{i}' for i in range(1, 96)]\n",
        "purchase_values = np.random.uniform(20, 500, 95).tolist()\n",
        "regions = ['North', 'South', 'East', 'West'] * 23 + ['North', 'South', 'East']\n",
        "purchase_dates = pd.date_range(start='2023-01-01', periods=95, freq='D').tolist()\n",
        "\n",
        "# Step 2: Ïù¥ÏÉÅÍ∞í Î∞è Í≤∞Ï∏°Ïπò Ìè¨Ìï® 8Í∞ú Ï∂îÍ∞Ä\n",
        "customer_ids += ['CUST96', 'CUST97', 'CUST98', 'CUST99', 'CUST100', 'CUST5', 'CUST10']\n",
        "purchase_values += [10000, None, None, None, 250.0, 45.0, 30.0]\n",
        "regions += ['north', 'EAST', None, 'West', 'South', 'North', 'West']\n",
        "purchase_dates += [None, None, None, pd.Timestamp('2023-04-10'), pd.Timestamp('2023-04-11'),\n",
        "                   pd.Timestamp('2023-01-05'), pd.Timestamp('2023-01-10')]\n",
        "\n",
        "# üëâ Í∏∏Ïù¥ ÌôïÏù∏\n",
        "print(\"Lengths:\", list(map(len, [customer_ids, purchase_values, regions, purchase_dates])))\n",
        "\n",
        "# Step 3: DataFrame ÏÉùÏÑ±\n",
        "data = {\n",
        "    'customer_id': customer_ids,\n",
        "    'purchase_value': purchase_values,\n",
        "    'region': regions,\n",
        "    'purchase_date': purchase_dates\n",
        "}\n",
        "\n",
        "messy_data = pd.DataFrame(data)\n",
        "\n",
        "# Ï†ÄÏû• Î∞è Ï∂úÎ†•\n",
        "messy_data.to_csv(\"messy_retail_data.csv\", index=False)\n",
        "print(\"Synthetic dataset created:\")\n",
        "print(messy_data.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Load the Data"
      ],
      "metadata": {
        "id": "IVZKYHev4rZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the messy dataset\n",
        "df = pd.read_csv(\"messy_retail_data.csv\")\n",
        "print(\"Original Data:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0wtuP1K4m3e",
        "outputId": "db3ddb1a-95c6-42ea-f2e5-4549f934a9fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "  customer_id  purchase_value region purchase_date\n",
            "0       CUST1      199.779257  North    2023-01-01\n",
            "1       CUST2      476.342867  South    2023-01-02\n",
            "2       CUST3      371.357092   East    2023-01-03\n",
            "3       CUST4      307.356072   West    2023-01-04\n",
            "4       CUST5       94.888947  North    2023-01-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Handle Missing Values"
      ],
      "metadata": {
        "id": "v0TjUGnzAX_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing purchase values with the median\n",
        "df['purchase_value'] = df['purchase_value'].fillna(df['purchase_value'].median())\n",
        "\n",
        "# Fill missing region and purchase date\n",
        "df['region'] = df['region'].fillna('Unknown')\n",
        "df['purchase_date'] = df['purchase_date'].fillna('2023-01-01')\n",
        "\n",
        "print(\"After Handling Missing Values:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3_XMHXiAWsu",
        "outputId": "3ebecc3a-d33a-4807-c068-6beee76d25b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Handling Missing Values:\n",
            "  customer_id  purchase_value region purchase_date\n",
            "0       CUST1      199.779257  North    2023-01-01\n",
            "1       CUST2      476.342867  South    2023-01-02\n",
            "2       CUST3      371.357092   East    2023-01-03\n",
            "3       CUST4      307.356072   West    2023-01-04\n",
            "4       CUST5       94.888947  North    2023-01-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Remove Duplicates"
      ],
      "metadata": {
        "id": "h2SdA3fEAbB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicate rows\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "print(\"After Removing Duplicates:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duSQhGEHAZus",
        "outputId": "cda42275-e561-43f8-e3a9-c7697f3e601a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Removing Duplicates:\n",
            "  customer_id  purchase_value region purchase_date\n",
            "0       CUST1      199.779257  North    2023-01-01\n",
            "1       CUST2      476.342867  South    2023-01-02\n",
            "2       CUST3      371.357092   East    2023-01-03\n",
            "3       CUST4      307.356072   West    2023-01-04\n",
            "4       CUST5       94.888947  North    2023-01-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Standardize Text Data"
      ],
      "metadata": {
        "id": "jf402PQhAedf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize region column\n",
        "df['region'] = df['region'].str.lower()\n",
        "\n",
        "print(\"After Standardizing Text Data:\")\n",
        "print(df['region'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZCU1ik8Ac_J",
        "outputId": "9fb7ec7c-88a5-4408-fed8-d49c655a8d82"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Standardizing Text Data:\n",
            "['north' 'south' 'east' 'west' 'unknown']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Handle Outliers"
      ],
      "metadata": {
        "id": "At_4hUWWAhGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cap outliers\n",
        "upper_limit = df['purchase_value'].quantile(0.95)\n",
        "df['purchase_value'] = df['purchase_value'].clip(upper=upper_limit)\n",
        "\n",
        "print(\"After Handling Outliers:\")\n",
        "print(df['purchase_value'].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP4sysLaAfpo",
        "outputId": "240ecd94-9955-483c-d144-0b08a375e6d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Handling Outliers:\n",
            "count    102.000000\n",
            "mean     247.154974\n",
            "std      142.851603\n",
            "min       22.650616\n",
            "25%      114.399712\n",
            "50%      246.663164\n",
            "75%      371.070560\n",
            "max      476.298977\n",
            "Name: purchase_value, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Scale Numerical Features"
      ],
      "metadata": {
        "id": "cEleUNolAkAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df['purchase_value_scaled'] = scaler.fit_transform(df[['purchase_value']])\n",
        "\n",
        "print(\"After Scaling:\")\n",
        "print(df[['purchase_value', 'purchase_value_scaled']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teTcqy1CAiuG",
        "outputId": "238e9dd3-baf7-467f-a32f-46330c1527f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Scaling:\n",
            "   purchase_value  purchase_value_scaled\n",
            "0      199.779257               0.390454\n",
            "1      476.298977               1.000000\n",
            "2      371.357092               0.768671\n",
            "3      307.356072               0.627591\n",
            "4       94.888947               0.159239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Save the Cleaned Data"
      ],
      "metadata": {
        "id": "pRbiwoREAmlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the cleaned dataset\n",
        "df.to_csv(\"cleaned_retail_data.csv\", index=False)\n",
        "print(\"Cleaned dataset saved to 'cleaned_retail_data.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUFhsEKgAlan",
        "outputId": "263b74f0-6bc4-4642-f288-4d5dc0dca522"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned dataset saved to 'cleaned_retail_data.csv'\n"
          ]
        }
      ]
    }
  ]
}
