{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF8wNtIi5bLmaVxcBfMVzN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd_6QiPIECHS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define custom objective function\n",
        "def custom_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    y_true: True labels (DMatrix format)\n",
        "    y_pred: Predicted probabilities\n",
        "    \"\"\"\n",
        "    # Reshape the predictions to (n_samples, n_classes)\n",
        "    y_pred = y_pred.reshape(-1, len(np.unique(y_true.get_label())))\n",
        "\n",
        "    # Get true labels as integers\n",
        "    y_true = y_true.get_label().astype(int)\n",
        "\n",
        "    # Calculate softmax probabilities\n",
        "    softmax_preds = np.exp(y_pred - np.max(y_pred, axis=1, keepdims=True))\n",
        "    softmax_preds /= softmax_preds.sum(axis=1, keepdims=True)\n",
        "\n",
        "    # Compute gradients and hessians (second derivatives)\n",
        "    grad = softmax_preds\n",
        "    grad[np.arange(len(y_true)), y_true] -= 1\n",
        "\n",
        "    # Define higher penalties for misclassifications between A and B\n",
        "    penalty_matrix = np.ones((len(np.unique(y_true)), len(np.unique(y_true))))\n",
        "    penalty = 2.0  # Set a penalty multiplier\n",
        "    important_classes = ['A', 'B']\n",
        "\n",
        "    for i in important_classes:\n",
        "        for j in important_classes:\n",
        "            if i != j:\n",
        "                penalty_matrix[i, j] = penalty\n",
        "\n",
        "    # Apply penalties to the gradients\n",
        "    for idx, (true_label, pred) in enumerate(zip(y_true, grad)):\n",
        "        grad[idx] = penalty_matrix[true_label] * pred\n",
        "\n",
        "    # Hessian is typically grad * (1 - softmax) for multi-class classification\n",
        "    hess = grad * (1 - softmax_preds)\n",
        "\n",
        "    return grad.flatten(), hess.flatten()\n",
        "\n",
        "# Define a custom evaluation metric for monitoring performance\n",
        "def custom_f1_metric(y_true, y_pred):\n",
        "    y_pred = np.argmax(y_pred.reshape(-1, len(np.unique(y_true))), axis=1)\n",
        "    y_true = y_true.get_label().astype(int)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    return 'custom_f1', f1\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "X = df_final.drop(['target'], axis=1)\n",
        "y = df_final['target'].astype(int)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train the model with the custom loss function\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "params = {\n",
        "    'objective': 'multi:softmax',  # This is required for multi-class problems\n",
        "    'num_class': len(np.unique(y)),  # Number of classes\n",
        "    'eval_metric': 'mlogloss',  # Monitor log-loss\n",
        "    'learning_rate': 0.1,\n",
        "    'max_depth': 6,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Train with the custom loss function and custom evaluation metric\n",
        "model = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=100,\n",
        "    obj=custom_loss,\n",
        "    feval=custom_f1_metric,\n",
        "    evals=[(dtest, 'test')],\n",
        "    early_stopping_rounds=10\n",
        ")\n",
        "\n",
        "# Make predictions and evaluate\n",
        "y_pred = model.predict(dtest)\n",
        "y_pred = np.round(y_pred)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Custom XGBoost Model Performance:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    }
  ]
}