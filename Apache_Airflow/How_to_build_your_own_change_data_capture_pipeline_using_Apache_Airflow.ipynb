{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuo+JZt4rjmV22o6eWDmHs"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/mitb-for-all/how-to-build-your-own-change-data-capture-pipeline-using-apache-airflow-e485fbef82c7)"
      ],
      "metadata": {
        "id": "FLo-p68KvkfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "docker run -p 6333:6333 -p 6334:6334 \\\n",
        "    -v \"$(pwd)/qdrant_storage:/qdrant/storage:z\" \\\n",
        "    qdrant/qdrant\n",
        "```"
      ],
      "metadata": {
        "id": "Ndpg6fx3v5je"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2V6liSVvdd_"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import (\n",
        "    Settings,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "from llama_index.embeddings.ollama import OllamaEmbedding\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
        "\n",
        "# Define embedding model\n",
        "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
        "\n",
        "# Define vector database\n",
        "client = QdrantClient(url=\"http://localhost:6333\",)\n",
        "aclient= AsyncQdrantClient(url=\"http://localhost:6333\")\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=\"Airflow_Experiment\",\n",
        ")\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Load documents\n",
        "documents = SimpleDirectoryReader(\"../../docs\").load_data(show_progress=True)\n",
        "\n",
        "# Convert documents into chunks, convert chunks into vector embeddings, and\n",
        "# store vector embeddings into the vector database\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents=documents,\n",
        "    storage_context=storage_context,\n",
        "    embed_model=Settings.embed_model,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow.decorators import dag, task\n",
        "from airflow.providers.standard.sensors.filesystem import FileSensor\n",
        "from airflow.providers.standard.operators.empty import EmptyOperator\n",
        "from airflow.models.baseoperator import chain\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import List, Literal, Dict, Any\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define global variables\n",
        "COLLECTION_NAME = \"Airflow_Experiment\"\n",
        "EMBEDDING_MODEL_NAME = \"nomic-embed-text\"\n",
        "EMBEDDING_DIMENSION = 768\n",
        "QDRANT_URL = \"http://localhost:6333\"\n",
        "\n",
        "# ─── TASKS ─────────────────────────────────────────────────────────────────\n",
        "@task\n",
        "def serialize_documents(paths: List[str]) -> List[Dict[str, Any]]:\n",
        "    from llama_index.core import SimpleDirectoryReader\n",
        "    from llama_index.core.schema import Document\n",
        "    if not paths:\n",
        "        return []\n",
        "    docs = SimpleDirectoryReader(input_files=paths).load_data()\n",
        "    serialized = []\n",
        "    for doc in docs:\n",
        "        serialized.append(doc.model_dump())\n",
        "    return serialized\n",
        "@task\n",
        "def ingest_serialized_documents(serialized_docs: List[Dict[str, Any]]) -> None:\n",
        "    from llama_index.core.schema import Document\n",
        "    from llama_index.core import StorageContext, VectorStoreIndex\n",
        "    from llama_index.embeddings.ollama import OllamaEmbedding\n",
        "    from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "    from qdrant_client import QdrantClient, AsyncQdrantClient\n",
        "\n",
        "    if not serialized_docs:\n",
        "        return\n",
        "    # reconstruct Document objects\n",
        "    documents = [Document(**d) for d in serialized_docs]\n",
        "    # setup Qdrant vector store\n",
        "    client = QdrantClient(url=QDRANT_URL)\n",
        "    aclient = AsyncQdrantClient(url=QDRANT_URL)\n",
        "    embed_model = OllamaEmbedding(\n",
        "        model_name=EMBEDDING_MODEL_NAME,\n",
        "        # base_url=\"http://ollama:11434\",  # Default is localhost:11434\n",
        "    )\n",
        "    vs = QdrantVectorStore(\n",
        "        client=client,\n",
        "        aclient=aclient,\n",
        "        collection_name=COLLECTION_NAME,\n",
        "    )\n",
        "    storage_ctx = StorageContext.from_defaults(vector_store=vs)\n",
        "    VectorStoreIndex.from_documents(\n",
        "        documents,\n",
        "        storage_context=storage_ctx,\n",
        "        embed_model = embed_model,\n",
        "    )\n",
        "@task.branch\n",
        "def check_or_create_collection(name: str) -> Literal[\"create_collection\", \"skip_create_collection\"]:\n",
        "    from qdrant_client import QdrantClient\n",
        "\n",
        "    client = QdrantClient(url=QDRANT_URL)\n",
        "    cols = client.get_collections().collections\n",
        "    return \"skip_create_collection\" if any(c.name == name for c in cols) else \"create_collection\"\n",
        "\n",
        "@task\n",
        "def create_collection(name: str) -> None:\n",
        "    from qdrant_client import QdrantClient\n",
        "    from qdrant_client.http.models import VectorParams\n",
        "\n",
        "    client = QdrantClient(url=QDRANT_URL)\n",
        "    client.create_collection(collection_name=name, vectors_config=VectorParams(size=EMBEDDING_DIMENSION, distance=\"Cosine\"))\n",
        "\n",
        "@task\n",
        "def load_last_read(csv_path: str) -> List[Dict[str, Any]]:\n",
        "    if os.path.exists(csv_path):\n",
        "        df = pd.read_csv(csv_path, parse_dates=[\"last_read_date\"])\n",
        "        return df.to_dict(orient=\"records\")\n",
        "    return []\n",
        "\n",
        "@task()\n",
        "def find_new_files(folder: str, last_read_records: List[Dict[str, Any]]) -> List[str]:\n",
        "    from datetime import datetime as _dt\n",
        "    last_read = pd.DataFrame(last_read_records) if last_read_records else pd.DataFrame(columns=[\"file_path\", \"last_read_date\"])\n",
        "    all_pdfs = [os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(\".pdf\")]\n",
        "    new_files = []\n",
        "    for path in all_pdfs:\n",
        "        mtime = _dt.fromtimestamp(os.path.getmtime(path))\n",
        "        prev = last_read.loc[last_read.file_path == path, \"last_read_date\"]\n",
        "        if prev.empty or mtime > prev.iloc[0]:\n",
        "            new_files.append(path)\n",
        "    return sorted(new_files)\n",
        "\n",
        "@task\n",
        "def update_last_read(paths: List[str], csv_path: str) -> None:\n",
        "    df = pd.read_csv(csv_path, parse_dates=[\"last_read_date\"]) if os.path.exists(csv_path) else pd.DataFrame(columns=[\"file_path\", \"last_read_date\"])\n",
        "    from datetime import datetime as _dt\n",
        "    for path in paths:\n",
        "        mtime = _dt.fromtimestamp(os.path.getmtime(path))\n",
        "        if path in df.file_path.values:\n",
        "            df.loc[df.file_path == path, \"last_read_date\"] = mtime\n",
        "        else:\n",
        "            df = pd.concat([df, pd.DataFrame([{\"file_path\": path, \"last_read_date\": mtime}])])\n",
        "    df.to_csv(csv_path, index=False)"
      ],
      "metadata": {
        "id": "P-RnIYBNv-jL"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}
