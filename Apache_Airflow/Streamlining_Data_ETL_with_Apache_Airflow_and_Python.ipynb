{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMm5ty2OWh8EKyVx+zwdf08"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@bragadeeshs/streamlining-data-etl-with-apache-airflow-and-python-c46006463d5e)"
      ],
      "metadata": {
        "id": "QpH9O76aJaXt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z2X4-Z7SIuV-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file path\n",
        "data_file = 'sales_data.csv'\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame\n",
        "data = pd.read_csv(data_file)\n",
        "\n",
        "# Display the first few rows of the data\n",
        "print(data.head())\n",
        "\n",
        "# Group the data by 'Category' and calculate the sum of 'Revenue'\n",
        "category_revenue = data.groupby('Category')['Revenue'].sum().reset_index()\n",
        "\n",
        "# Display the transformed data\n",
        "print(category_revenue)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the default_args dictionary\n",
        "default_args = {\n",
        "    'owner': 'your_name',\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'retries': 1,\n",
        "}\n",
        "\n",
        "# Create a DAG instance\n",
        "dag = DAG(\n",
        "    'retail_sales_etl',\n",
        "    default_args=default_args,\n",
        "    schedule_interval='@daily',  # Run the DAG daily\n",
        "    catchup=False,\n",
        ")\n",
        "\n",
        "# Define Python functions for extraction and transformation\n",
        "def extract_data():\n",
        "    # Your data extraction logic here\n",
        "    pass\n",
        "\n",
        "def transform_data():\n",
        "    # Your data transformation logic here\n",
        "    pass\n",
        "\n",
        "# Define Airflow tasks\n",
        "extract_task = PythonOperator(\n",
        "    task_id='extract_data',\n",
        "    python_callable=extract_data,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "transform_task = PythonOperator(\n",
        "    task_id='transform_data',\n",
        "    python_callable=transform_data,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Define task dependencies\n",
        "extract_task >> transform_task"
      ],
      "metadata": {
        "id": "lSigytYzJlEw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mysql-connector-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSHYuv_5JtGY",
        "outputId": "b8f3593c-0ad2-43d4-a4ac-2b4ab7325a99"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mysql-connector-python\n",
            "  Downloading mysql_connector_python-8.2.0-cp310-cp310-manylinux_2_17_x86_64.whl (31.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.6/31.6 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<=4.21.12,>=4.21.1 (from mysql-connector-python)\n",
            "  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.8/409.8 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, mysql-connector-python\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.21.12 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mysql-connector-python-8.2.0 protobuf-4.21.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mysql.connector\n",
        "\n",
        "# MySQL database credentials\n",
        "db_config = {\n",
        "    'host': 'your_host',\n",
        "    'user': 'your_user',\n",
        "    'password': 'your_password',\n",
        "    'database': 'your_database',\n",
        "}\n",
        "\n",
        "# Create a connection to MySQL\n",
        "conn = mysql.connector.connect(**db_config)\n",
        "\n",
        "# Create a cursor object to interact with the database\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Close the cursor and connection when done\n",
        "cursor.close()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "pyqm-OZaJuSk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'your_table' with your actual table name\n",
        "table_name = 'your_table'\n",
        "\n",
        "# SQL statement to insert data\n",
        "insert_query = f\"INSERT INTO {table_name} (Category, Revenue) VALUES (%s, %s)\"\n",
        "\n",
        "# Prepare data for insertion (category_revenue is the DataFrame from the transformation step)\n",
        "data_to_insert = [(row['Category'], row['Revenue']) for _, row in category_revenue.iterrows()]\n",
        "\n",
        "# Execute the insert query\n",
        "cursor.executemany(insert_query, data_to_insert)\n",
        "\n",
        "# Commit the changes to the database\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "_FzCARYOJvJl"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}
