{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYnFdF7DlaPLIc32oLuDXt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@SaiParvathaneni/apache-airflow-for-beginners-build-your-first-dag-542affef6192)"
      ],
      "metadata": {
        "id": "xQIDSBZfD4Fd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "munh0j74DxE4"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import csv\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "\n",
        "# Function to extract data from the CSV file\n",
        "def extract(directory, filename):\n",
        "    # Read the CSV file and store its data\n",
        "    with open(os.path.join(directory, filename), 'r') as file:\n",
        "        data = list(csv.reader(file))\n",
        "    print(\"Data in extract:\", data)\n",
        "    return data\n",
        "\n",
        "# Function to transform the extracted data\n",
        "def transform(data):\n",
        "    print(\"Data in transform:\", data)\n",
        "    # Add a new column to the data\n",
        "    transformed_data = [row + [int(row[1]) * 2] for row in data]\n",
        "    return transformed_data\n",
        "\n",
        "# Function to load the transformed data into a new CSV file\n",
        "def load(transformed_data, output_directory, output_filename):\n",
        "    with open(os.path.join(output_directory, output_filename), 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerows(transformed_data)\n",
        "\n",
        "\n",
        "# Define default arguments for the DAG\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'retries': 2,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False\n",
        "}\n",
        "\n",
        "# Create the DAG\n",
        "dag = DAG(\n",
        "    'my_csv_pipeline',\n",
        "    default_args=default_args,\n",
        "    description='A CSV processing pipeline',\n",
        "    schedule='0 * * * *',\n",
        "    catchup=False\n",
        ")\n",
        "\n",
        "# Define the extract_task operator\n",
        "extract_task = PythonOperator(\n",
        "    task_id='extract_task',\n",
        "    python_callable=extract,\n",
        "    op_args=[r'/path/to/airflow/dags/files', 'forex_currencies.csv'],\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Define the transform_task operator\n",
        "transform_task = PythonOperator(\n",
        "    task_id='transform_task',\n",
        "    python_callable=transform,\n",
        "    op_args=[extract_task.output],\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Define the load_task operator\n",
        "load_task = PythonOperator(\n",
        "    task_id='load_task',\n",
        "    python_callable=load,\n",
        "    op_args=[transform_task.output, r'/path/to/airflow/dags/files', 'transformed_forex_currencies.csv'],\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Set up task dependencies\n",
        "extract_task >> transform_task >> load_task"
      ]
    }
  ]
}
