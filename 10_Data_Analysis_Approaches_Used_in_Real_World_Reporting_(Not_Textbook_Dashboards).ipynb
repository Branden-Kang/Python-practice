{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5G9VgAjIaXHlh4CkJXjtf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@Rohan_Dutt/10-data-analysis-approaches-used-in-real-world-reporting-not-textbook-dashboards-c3dba4bbfb09)"
      ],
      "metadata": {
        "id": "UvZINRLWjtlm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Storytelling to Drive Action\n",
        "Insights donâ€™t create impact decisions do. Data storytelling turns analysis into a clear narrative that motivates action."
      ],
      "metadata": {
        "id": "aRG3rkQpe6TG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Machine Learning for Pattern Detection"
      ],
      "metadata": {
        "id": "fn1zKsXpfAVt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NEmxT45oe2cS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"user_behavior.csv\")\n",
        "features = [\n",
        "    \"total_sessions\",\n",
        "    \"avg_session_duration\",\n",
        "    \"pages_per_session\",\n",
        "    \"purchases_last_30d\"\n",
        "]\n",
        "X = df[features]\n",
        "# Normalize features (critical)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "# Train k-means\n",
        "kmeans = KMeans(\n",
        "    n_clusters=4,\n",
        "    random_state=42,\n",
        "    n_init=10\n",
        ")\n",
        "df[\"cluster_id\"] = kmeans.fit_predict(X_scaled)\n",
        "# Inspect cluster profiles\n",
        "cluster_profiles = (\n",
        "    df.groupby(\"cluster_id\")[features]\n",
        "      .mean()\n",
        "      .round(2)\n",
        ")\n",
        "cluster_profiles"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choosing k (Quick Reality Check)\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "silhouette = silhouette_score(X_scaled, df[\"cluster_id\"])\n",
        "silhouette"
      ],
      "metadata": {
        "id": "gZCruNHmfF78"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Rule of thumb: Silhouette > 0.4 = meaningful separation\n",
        "```"
      ],
      "metadata": {
        "id": "c2_olPCOfMxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Geospatial Analysis for Location-Based Insights"
      ],
      "metadata": {
        "id": "4oqXsvtPfVqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SQL based Geospatial (Customer Clusters & Coverage)\n",
        "```sql\n",
        "WITH customers AS (\n",
        "  SELECT\n",
        "    customer_id,\n",
        "    spend,\n",
        "    ST_SetSRID(ST_Point(lon, lat), 4326) AS geom\n",
        "  FROM customer_locations\n",
        "),\n",
        "stores AS (\n",
        "  SELECT\n",
        "    store_id,\n",
        "    ST_SetSRID(ST_Point(lon, lat), 4326) AS geom\n",
        "  FROM store_locations\n",
        "),\n",
        "customer_density AS (\n",
        "  SELECT\n",
        "    ST_SnapToGrid(geom, 0.05) AS grid_cell,   -- ~5km grid\n",
        "    COUNT(*) AS customers,\n",
        "    SUM(spend) AS total_spend\n",
        "  FROM customers\n",
        "  GROUP BY 1\n",
        "),\n",
        "underserved AS (\n",
        "  SELECT\n",
        "    d.grid_cell,\n",
        "    d.customers,\n",
        "    d.total_spend,\n",
        "    MIN(ST_Distance(d.grid_cell, s.geom)) / 1000 AS km_to_nearest_store\n",
        "  FROM customer_density d\n",
        "  LEFT JOIN stores s\n",
        "    ON ST_DWithin(d.grid_cell, s.geom, 50000) -- 50km search radius\n",
        "  GROUP BY d.grid_cell, d.customers, d.total_spend\n",
        ")\n",
        "SELECT *\n",
        "FROM underserved\n",
        "WHERE km_to_nearest_store > 20\n",
        "ORDER BY total_spend DESC;\n",
        "```"
      ],
      "metadata": {
        "id": "7LVcNBfqfXaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python based Geospatial (Customer Clusters & Coverage)"
      ],
      "metadata": {
        "id": "il5s9JRgfbMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "customers_df = pd.read_csv(\"customer_locations.csv\")  # customer_id, lat, lon, spend\n",
        "stores_df = pd.read_csv(\"store_locations.csv\")        # store_id, lat, lon\n",
        "\n",
        "# Create GeoDataFrames (WGS84)\n",
        "customers = gpd.GeoDataFrame(\n",
        "    customers_df,\n",
        "    geometry=gpd.points_from_xy(customers_df.lon, customers_df.lat),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "stores = gpd.GeoDataFrame(\n",
        "    stores_df,\n",
        "    geometry=gpd.points_from_xy(stores_df.lon, stores_df.lat),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "# Project to meters for distance calculations\n",
        "customers = customers.to_crs(epsg=3857)\n",
        "stores = stores.to_crs(epsg=3857)\n",
        "\n",
        "# Grid-based clustering (~5km)\n",
        "grid_size = 5000  # meters\n",
        "\n",
        "customers[\"grid_x\"] = (customers.geometry.x // grid_size) * grid_size\n",
        "customers[\"grid_y\"] = (customers.geometry.y // grid_size) * grid_size\n",
        "\n",
        "customers[\"grid_cell\"] = customers.apply(\n",
        "    lambda r: Point(r[\"grid_x\"], r[\"grid_y\"]), axis=1\n",
        ")\n",
        "\n",
        "# Aggregate customer density & spend\n",
        "density = (\n",
        "    customers.groupby(\"grid_cell\")\n",
        "    .agg(\n",
        "        customers=(\"customer_id\", \"count\"),\n",
        "        total_spend=(\"spend\", \"sum\")\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "density = gpd.GeoDataFrame(density, geometry=\"grid_cell\", crs=customers.crs)\n",
        "\n",
        "# # Distance to nearest store\n",
        "# density[\"km_to_nearest_store\"] = (\n",
        "#     density.geometry.apply(\n",
        "#         lambda g:\n",
        "#     )\n",
        "# )"
      ],
      "metadata": {
        "id": "UPGh5rYefL5J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Network Analysis to Map Relationships"
      ],
      "metadata": {
        "id": "AunBEWN6ftGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQL based Network Analysis (Influence & Connectors)\n",
        "```sql\n",
        "WITH edges AS (\n",
        "  SELECT\n",
        "    follower_id AS src,\n",
        "    followee_id AS dst\n",
        "  FROM user_follows\n",
        "),\n",
        "degree AS (\n",
        "  SELECT\n",
        "    user_id,\n",
        "    SUM(in_degree) AS in_degree,\n",
        "    SUM(out_degree) AS out_degree\n",
        "  FROM (\n",
        "    SELECT dst AS user_id, COUNT(*) AS in_degree, 0 AS out_degree\n",
        "    FROM edges\n",
        "    GROUP BY dst\n",
        "    UNION ALL\n",
        "    SELECT src AS user_id, 0 AS in_degree, COUNT(*) AS out_degree\n",
        "    FROM edges\n",
        "    GROUP BY src\n",
        "  ) d\n",
        "  GROUP BY user_id\n",
        "),\n",
        "two_hop_paths AS (\n",
        "  SELECT\n",
        "    e1.src AS from_user,\n",
        "    e2.dst AS to_user\n",
        "  FROM edges e1\n",
        "  JOIN edges e2\n",
        "    ON e1.dst = e2.src\n",
        "  WHERE e1.src <> e2.dst\n",
        "),\n",
        "betweenness_proxy AS (\n",
        "  SELECT\n",
        "    from_user AS user_id,\n",
        "    COUNT(DISTINCT to_user) AS bridge_count\n",
        "  FROM two_hop_paths\n",
        "  GROUP BY from_user\n",
        ")\n",
        "SELECT\n",
        "  d.user_id,\n",
        "  d.in_degree,\n",
        "  d.out_degree,\n",
        "  COALESCE(b.bridge_count, 0) AS bridge_score\n",
        "FROM degree d\n",
        "LEFT JOIN betweenness_proxy b\n",
        "  ON d.user_id = b.user_id\n",
        "ORDER BY bridge_score DESC, in_degree DESC;\n",
        "```"
      ],
      "metadata": {
        "id": "GpQo7ydlf1Y2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python based Network Analysis (Influence & Connectors)\n"
      ],
      "metadata": {
        "id": "ndIMKAK1fwzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "# Load edge list\n",
        "edges = pd.read_csv(\"user_follows.csv\")  # follower_id, followee_id\n",
        "\n",
        "# Build directed graph\n",
        "G = nx.from_pandas_edgelist(\n",
        "    edges,\n",
        "    source=\"follower_id\",\n",
        "    target=\"followee_id\",\n",
        "    create_using=nx.DiGraph()\n",
        ")\n",
        "\n",
        "# Degree metrics\n",
        "in_degree = dict(G.in_degree())\n",
        "out_degree = dict(G.out_degree())\n",
        "\n",
        "# Betweenness centrality (true bridge metric)\n",
        "betweenness = nx.betweenness_centrality(G, normalized=True)\n",
        "\n",
        "# Combine results\n",
        "influence = (\n",
        "    pd.DataFrame({\n",
        "        \"user_id\": list(G.nodes()),\n",
        "        \"in_degree\": [in_degree.get(u, 0) for u in G.nodes()],\n",
        "        \"out_degree\": [out_degree.get(u, 0) for u in G.nodes()],\n",
        "        \"betweenness\": [betweenness.get(u, 0) for u in G.nodes()]\n",
        "    })\n",
        "    .sort_values(\n",
        "        [\"betweenness\", \"in_degree\"],\n",
        "        ascending=False\n",
        "    )\n",
        ")\n",
        "\n",
        "influence.head(10)"
      ],
      "metadata": {
        "id": "9RHVTDbhffae"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. A/B Testing to Validate Assumptions"
      ],
      "metadata": {
        "id": "jw7qSIMSf703"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQL based A/B Testing (Conversion Rate + Significance)\n",
        "```sql\n",
        "WITH experiment_data AS (\n",
        "  SELECT\n",
        "    variant,                           -- 'A' or 'B'\n",
        "    COUNT(*) AS users,\n",
        "    SUM(CASE WHEN converted = 1 THEN 1 ELSE 0 END) AS conversions\n",
        "  FROM ab_events\n",
        "  WHERE experiment_name = 'landing_page_v2'\n",
        "  GROUP BY variant\n",
        "),\n",
        "rates AS (\n",
        "  SELECT\n",
        "    variant,\n",
        "    users,\n",
        "    conversions,\n",
        "    conversions * 1.0 / users AS conversion_rate\n",
        "  FROM experiment_data\n",
        "),\n",
        "stats AS (\n",
        "  SELECT\n",
        "    MAX(CASE WHEN variant = 'A' THEN conversion_rate END) AS cr_a,\n",
        "    MAX(CASE WHEN variant = 'B' THEN conversion_rate END) AS cr_b,\n",
        "    MAX(CASE WHEN variant = 'A' THEN users END) AS n_a,\n",
        "    MAX(CASE WHEN variant = 'B' THEN users END) AS n_b\n",
        "  FROM rates\n",
        ")\n",
        "SELECT\n",
        "  cr_a,\n",
        "  cr_b,\n",
        "  ROUND(cr_b - cr_a, 4) AS lift,\n",
        "  ROUND(\n",
        "    (cr_b - cr_a) /\n",
        "    SQRT(\n",
        "      (cr_a * (1 - cr_a) / n_a) +\n",
        "      (cr_b * (1 - cr_b) / n_b)\n",
        "    ),\n",
        "    3\n",
        "  ) AS z_score\n",
        "FROM stats;\n",
        "```"
      ],
      "metadata": {
        "id": "jNkpMZ64f9Nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python based A/B Testing (Conversion Rate + Significance)\n"
      ],
      "metadata": {
        "id": "C1T_YvpEgApL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"ab_events.csv\")\n",
        "\n",
        "# Filter experiment\n",
        "df = df[df[\"experiment_name\"] == \"landing_page_v2\"]\n",
        "\n",
        "# Aggregate metrics\n",
        "summary = (\n",
        "    df.groupby(\"variant\")\n",
        "      .agg(\n",
        "          users=(\"user_id\", \"count\"),\n",
        "          conversions=(\"converted\", \"sum\")\n",
        "      )\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# Extract A/B values\n",
        "a = summary.loc[summary[\"variant\"] == \"A\"].iloc[0]\n",
        "b = summary.loc[summary[\"variant\"] == \"B\"].iloc[0]\n",
        "\n",
        "cr_a = a[\"conversions\"] / a[\"users\"]\n",
        "cr_b = b[\"conversions\"] / b[\"users\"]\n",
        "\n",
        "lift = cr_b - cr_a\n",
        "\n",
        "# Z-test for proportions\n",
        "se = np.sqrt(\n",
        "    (cr_a * (1 - cr_a) / a[\"users\"]) +\n",
        "    (cr_b * (1 - cr_b) / b[\"users\"])\n",
        ")\n",
        "\n",
        "z_score = lift / se\n",
        "p_value = 2 * (1 - norm.cdf(abs(z_score)))\n",
        "\n",
        "# Results\n",
        "results = {\n",
        "    \"cr_a\": round(cr_a, 4),\n",
        "    \"cr_b\": round(cr_b, 4),\n",
        "    \"lift\": round(lift, 4),\n",
        "    \"z_score\": round(z_score, 3),\n",
        "    \"p_value\": round(p_value, 4)\n",
        "}\n",
        "\n",
        "results"
      ],
      "metadata": {
        "id": "IKSzqhL8fy1T"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Time Series Forecasting for Strategic Decisions\n",
        "```sql\n",
        "WITH monthly_traffic AS (\n",
        "  SELECT\n",
        "    DATE_TRUNC('month', visit_date) AS month,\n",
        "    COUNT(*) AS visits\n",
        "  FROM web_visits\n",
        "  GROUP BY 1\n",
        "),\n",
        "features AS (\n",
        "  SELECT\n",
        "    month,\n",
        "    visits,\n",
        "    ROW_NUMBER() OVER (ORDER BY month) AS t,\n",
        "    AVG(visits) OVER (\n",
        "      ORDER BY month\n",
        "      ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING\n",
        "    ) AS rolling_avg_3m,\n",
        "    LAG(visits, 12) OVER (ORDER BY month) AS visits_last_year\n",
        "  FROM monthly_traffic\n",
        "),\n",
        "forecast AS (\n",
        "  SELECT\n",
        "    month,\n",
        "    visits,\n",
        "    ROUND(\n",
        "      0.6 * rolling_avg_3m +\n",
        "      0.4 * visits_last_year\n",
        "    ) AS forecast_visits\n",
        "  FROM features\n",
        ")\n",
        "SELECT *\n",
        "FROM forecast\n",
        "ORDER BY month;\n",
        "```\n"
      ],
      "metadata": {
        "id": "MgWl1g3ygoFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python based Time Series Forecasting (Seasonality-Aware)"
      ],
      "metadata": {
        "id": "9ptyaRjJhnA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"web_visits.csv\", parse_dates=[\"visit_date\"])\n",
        "\n",
        "# Monthly aggregation\n",
        "monthly = (\n",
        "    df.assign(month=df[\"visit_date\"].dt.to_period(\"M\").dt.to_timestamp())\n",
        "      .groupby(\"month\")\n",
        "      .size()\n",
        "      .rename(\"visits\")\n",
        "      .reset_index()\n",
        "      .sort_values(\"month\")\n",
        ")\n",
        "\n",
        "# Feature engineering (same as SQL)\n",
        "monthly[\"t\"] = range(1, len(monthly) + 1)\n",
        "\n",
        "monthly[\"rolling_avg_3m\"] = (\n",
        "    monthly[\"visits\"]\n",
        "    .rolling(window=3)\n",
        "    .mean()\n",
        "    .shift(1)\n",
        ")\n",
        "\n",
        "monthly[\"visits_last_year\"] = monthly[\"visits\"].shift(12)\n",
        "\n",
        "# Forecast logic\n",
        "monthly[\"forecast_visits\"] = (\n",
        "    0.6 * monthly[\"rolling_avg_3m\"] +\n",
        "    0.4 * monthly[\"visits_last_year\"]\n",
        ").round()\n",
        "\n",
        "monthly"
      ],
      "metadata": {
        "id": "XjY7mtpCgGV3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Sentiment Analysis for Unstructured Data\n"
      ],
      "metadata": {
        "id": "SInYmpgahui7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python based Sentiment Analysis (vaderSentiment)"
      ],
      "metadata": {
        "id": "ql0lHOXdhxlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"support_emails.csv\", parse_dates=[\"created_at\"])\n",
        "\n",
        "# Initialize sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Compute sentiment score (-1 to +1)\n",
        "df[\"sentiment_score\"] = df[\"email_text\"].apply(\n",
        "    lambda x: analyzer.polarity_scores(str(x))[\"compound\"]\n",
        ")\n",
        "\n",
        "# Normalize text\n",
        "df[\"text\"] = df[\"email_text\"].str.lower()\n",
        "\n",
        "# Issue tagging (rule-based, transparent)\n",
        "def tag_issue(text):\n",
        "    if \"clean\" in text:\n",
        "        return \"cleanliness\"\n",
        "    elif \"delay\" in text or \"late\" in text:\n",
        "        return \"delivery\"\n",
        "    elif \"refund\" in text or \"charge\" in text:\n",
        "        return \"billing\"\n",
        "    elif \"support\" in text or \"agent\" in text:\n",
        "        return \"support\"\n",
        "    else:\n",
        "        return \"other\"\n",
        "\n",
        "df[\"issue_tag\"] = df[\"text\"].apply(tag_issue)\n",
        "\n",
        "# Aggregate insights\n",
        "summary = (\n",
        "    df.groupby(\"issue_tag\")\n",
        "      .agg(\n",
        "          email_count=(\"email_id\", \"count\"),\n",
        "          avg_sentiment=(\"sentiment_score\", \"mean\"),\n",
        "          strongly_negative_count=(\n",
        "              \"sentiment_score\", lambda x: (x < -0.3).sum()\n",
        "          )\n",
        "      )\n",
        "      .reset_index()\n",
        "      .sort_values(\"strongly_negative_count\", ascending=False)\n",
        ")\n",
        "\n",
        "summary"
      ],
      "metadata": {
        "id": "g1DIJaCshrrG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQL based Sentiment Analysis (Keyword + Scoring)\n",
        "```sql\n",
        "WITH enriched AS (\n",
        "  SELECT\n",
        "    email_id,\n",
        "    customer_id,\n",
        "    created_at,\n",
        "    sentiment_score,           -- range: -1 to +1\n",
        "    LOWER(email_text) AS text\n",
        "  FROM support_emails\n",
        "),\n",
        "tagged AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    CASE\n",
        "      WHEN text LIKE '%clean%' THEN 'cleanliness'\n",
        "      WHEN text LIKE '%delay%' OR text LIKE '%late%' THEN 'delivery'\n",
        "      WHEN text LIKE '%refund%' OR text LIKE '%charge%' THEN 'billing'\n",
        "      WHEN text LIKE '%support%' OR text LIKE '%agent%' THEN 'support'\n",
        "      ELSE 'other'\n",
        "    END AS issue_tag\n",
        "  FROM enriched\n",
        ")\n",
        "SELECT\n",
        "  issue_tag,\n",
        "  COUNT(*)                              AS email_count,\n",
        "  ROUND(AVG(sentiment_score), 3)        AS avg_sentiment,\n",
        "  SUM(CASE WHEN sentiment_score < -0.3 THEN 1 ELSE 0 END)\n",
        "                                        AS strongly_negative_count\n",
        "FROM tagged\n",
        "GROUP BY issue_tag\n",
        "ORDER BY strongly_negative_count DESC;\n",
        "```"
      ],
      "metadata": {
        "id": "fgD541EPh2RR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Regression Analysis to Isolate Key Drivers"
      ],
      "metadata": {
        "id": "K-MtrADJjG-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python based Regression (Linear Regression via Aggregation)"
      ],
      "metadata": {
        "id": "vm1U8HeIjJjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"sales_data.csv\", parse_dates=[\"sale_date\"])\n",
        "\n",
        "# Monthly feature engineering (same as SQL)\n",
        "features = (\n",
        "    df.assign(month=df[\"sale_date\"].dt.to_period(\"M\").dt.to_timestamp())\n",
        "      .groupby(\"month\")\n",
        "      .agg(\n",
        "          ad_spend=(\"ad_spend\", \"sum\"),\n",
        "          discounts=(\"discount_amount\", \"sum\"),\n",
        "          holiday_ratio=(\"is_holiday\", \"mean\"),\n",
        "          sales=(\"revenue\", \"sum\")\n",
        "      )\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# Z-score normalization (standardized betas)\n",
        "for col in [\"ad_spend\", \"discounts\", \"holiday_ratio\"]:\n",
        "    features[f\"{col}_z\"] = (\n",
        "        (features[col] - features[col].mean()) /\n",
        "        features[col].std()\n",
        "    )\n",
        "\n",
        "# Regression setup\n",
        "X = features[[\"ad_spend_z\", \"discounts_z\", \"holiday_ratio_z\"]]\n",
        "X = sm.add_constant(X)\n",
        "y = features[\"sales\"]\n",
        "\n",
        "# Fit OLS model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "T5b5ibBjh0M_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQL based Regression (Linear Regression via Aggregation)\n",
        "```sql\n",
        "WITH features AS (\n",
        "  SELECT\n",
        "    DATE_TRUNC('month', sale_date) AS month,\n",
        "    SUM(ad_spend)                  AS ad_spend,\n",
        "    SUM(discount_amount)           AS discounts,\n",
        "    AVG(is_holiday::int)           AS holiday_ratio,\n",
        "    SUM(revenue)                   AS sales\n",
        "  FROM sales_data\n",
        "  GROUP BY 1\n",
        "),\n",
        "normalized AS (\n",
        "  SELECT\n",
        "    month,\n",
        "    (ad_spend - AVG(ad_spend) OVER ()) / STDDEV(ad_spend) OVER () AS ad_spend_z,\n",
        "    (discounts - AVG(discounts) OVER ()) / STDDEV(discounts) OVER () AS discounts_z,\n",
        "    (holiday_ratio - AVG(holiday_ratio) OVER ()) / STDDEV(holiday_ratio) OVER () AS holiday_z,\n",
        "    sales\n",
        "  FROM features\n",
        ")\n",
        "SELECT\n",
        "  REGR_SLOPE(sales, ad_spend_z)     AS beta_ad_spend,\n",
        "  REGR_SLOPE(sales, discounts_z)    AS beta_discounts,\n",
        "  REGR_SLOPE(sales, holiday_z)      AS beta_holidays,\n",
        "  REGR_INTERCEPT(sales, ad_spend_z) AS intercept,\n",
        "  REGR_R2(sales, ad_spend_z)        AS r_squared\n",
        "FROM normalized;\n",
        "```"
      ],
      "metadata": {
        "id": "28lxjgzJjSRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Cohort Analysis for Hidden Trends"
      ],
      "metadata": {
        "id": "E-mJyFtljZlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQL Cohort Analysis (Retention)\n",
        "```\n",
        "WITH user_cohorts AS (\n",
        "  SELECT\n",
        "    user_id,\n",
        "    DATE_TRUNC('month', signup_date) AS cohort_month\n",
        "  FROM users\n",
        "),\n",
        "activity AS (\n",
        "  SELECT\n",
        "    u.user_id,\n",
        "    u.cohort_month,\n",
        "    DATE_TRUNC('month', a.activity_date) AS activity_month\n",
        "  FROM user_cohorts u\n",
        "  JOIN user_activity a\n",
        "    ON u.user_id = a.user_id\n",
        "),\n",
        "cohort_metrics AS (\n",
        "  SELECT\n",
        "    cohort_month,\n",
        "    activity_month,\n",
        "    DATE_PART('month', activity_month) -\n",
        "    DATE_PART('month', cohort_month) +\n",
        "    12 * (DATE_PART('year', activity_month) -\n",
        "          DATE_PART('year', cohort_month)) AS cohort_age,\n",
        "    COUNT(DISTINCT user_id) AS active_users\n",
        "  FROM activity\n",
        "  GROUP BY cohort_month, activity_month\n",
        "),\n",
        "cohort_sizes AS (\n",
        "  SELECT\n",
        "    cohort_month,\n",
        "    COUNT(DISTINCT user_id) AS cohort_size\n",
        "  FROM user_cohorts\n",
        "  GROUP BY cohort_month\n",
        ")\n",
        "SELECT\n",
        "  m.cohort_month,\n",
        "  m.cohort_age,\n",
        "  m.active_users,\n",
        "  ROUND(m.active_users * 1.0 / s.cohort_size, 3) AS retention_rate\n",
        "FROM cohort_metrics m\n",
        "JOIN cohort_sizes s\n",
        "  ON m.cohort_month = s.cohort_month\n",
        "ORDER BY cohort_month, cohort_age;\n",
        "```"
      ],
      "metadata": {
        "id": "Z9n8AMgIjbpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python Cohort Analysis (Retention)"
      ],
      "metadata": {
        "id": "OnyS7-iSjfXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "users = pd.read_csv(\"users.csv\", parse_dates=[\"signup_date\"])\n",
        "activity = pd.read_csv(\"user_activity.csv\", parse_dates=[\"activity_date\"])\n",
        "\n",
        "# Define cohort month\n",
        "users[\"cohort_month\"] = users[\"signup_date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
        "\n",
        "# Join activity to cohorts\n",
        "df = activity.merge(\n",
        "    users[[\"user_id\", \"cohort_month\"]],\n",
        "    on=\"user_id\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "# Activity month\n",
        "df[\"activity_month\"] = df[\"activity_date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
        "\n",
        "# Cohort age in months\n",
        "df[\"cohort_age\"] = (\n",
        "    (df[\"activity_month\"].dt.year - df[\"cohort_month\"].dt.year) * 12 +\n",
        "    (df[\"activity_month\"].dt.month - df[\"cohort_month\"].dt.month)\n",
        ")\n",
        "\n",
        "# Active users per cohort per month\n",
        "cohort_metrics = (\n",
        "    df.groupby([\"cohort_month\", \"cohort_age\"])[\"user_id\"]\n",
        "      .nunique()\n",
        "      .rename(\"active_users\")\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# Cohort sizes\n",
        "cohort_sizes = (\n",
        "    users.groupby(\"cohort_month\")[\"user_id\"]\n",
        "         .nunique()\n",
        "         .rename(\"cohort_size\")\n",
        "         .reset_index()\n",
        ")\n",
        "\n",
        "# Retention calculation\n",
        "retention = cohort_metrics.merge(\n",
        "    cohort_sizes, on=\"cohort_month\", how=\"left\"\n",
        ")\n",
        "\n",
        "retention[\"retention_rate\"] = (\n",
        "    retention[\"active_users\"] / retention[\"cohort_size\"]\n",
        ").round(3)\n",
        "\n",
        "retention.sort_values([\"cohort_month\", \"cohort_age\"])"
      ],
      "metadata": {
        "id": "OEwMxzN9jLnZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Exploratory Data Analysis (EDA) Before Anything Else"
      ],
      "metadata": {
        "id": "ZRrQieukjkKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQL based EDA\n",
        "```\n",
        "WITH base_stats AS (\n",
        "  SELECT\n",
        "    district,\n",
        "    COUNT(*)                      AS total_records,\n",
        "    COUNT(crime_id)               AS non_null_ids,\n",
        "    COUNT(*) - COUNT(crime_id)    AS missing_ids,\n",
        "    AVG(crime_count)              AS avg_crime,\n",
        "    PERCENTILE_CONT(0.5)\n",
        "      WITHIN GROUP (ORDER BY crime_count) AS median_crime,\n",
        "    STDDEV(crime_count)           AS stddev_crime,\n",
        "    MIN(crime_count)              AS min_crime,\n",
        "    MAX(crime_count)              AS max_crime\n",
        "  FROM crime_data\n",
        "  GROUP BY district\n",
        "),\n",
        "outliers AS (\n",
        "  SELECT\n",
        "    district,\n",
        "    crime_date,\n",
        "    crime_count,\n",
        "    AVG(crime_count) OVER (PARTITION BY district) +\n",
        "    3 * STDDEV(crime_count) OVER (PARTITION BY district) AS outlier_threshold\n",
        "  FROM crime_data\n",
        ")\n",
        "SELECT\n",
        "  b.*,\n",
        "  COUNT(o.crime_date) AS extreme_outlier_days\n",
        "FROM base_stats b\n",
        "LEFT JOIN outliers o\n",
        "  ON b.district = o.district\n",
        " AND o.crime_count > o.outlier_threshold\n",
        "GROUP BY\n",
        "  b.district, b.total_records, b.non_null_ids, b.missing_ids,\n",
        "  b.avg_crime, b.median_crime, b.stddev_crime, b.min_crime, b.max_crime\n",
        "ORDER BY extreme_outlier_days DESC;\n",
        "```"
      ],
      "metadata": {
        "id": "NujLRqFHjlar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python based EDA\n"
      ],
      "metadata": {
        "id": "ZBrlMhKgjoV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"crime_data.csv\")\n",
        "\n",
        "# Basic sanity checks\n",
        "eda_summary = (\n",
        "    df.groupby(\"district\")\n",
        "      .agg(\n",
        "          total_records=(\"crime_id\", \"size\"),\n",
        "          missing_ids=(\"crime_id\", lambda x: x.isna().sum()),\n",
        "          avg_crime=(\"crime_count\", \"mean\"),\n",
        "          median_crime=(\"crime_count\", \"median\"),\n",
        "          stddev_crime=(\"crime_count\", \"std\"),\n",
        "          min_crime=(\"crime_count\", \"min\"),\n",
        "          max_crime=(\"crime_count\", \"max\")\n",
        "      )\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# Outlier detection: 3-sigma rule per district\n",
        "df[\"outlier_threshold\"] = (\n",
        "    df.groupby(\"district\")[\"crime_count\"]\n",
        "      .transform(lambda x: x.mean() + 3 * x.std())\n",
        ")\n",
        "\n",
        "df[\"is_outlier\"] = df[\"crime_count\"] > df[\"outlier_threshold\"]\n",
        "\n",
        "outlier_counts = (\n",
        "    df[df[\"is_outlier\"]]\n",
        "    .groupby(\"district\")\n",
        "    .size()\n",
        "    .rename(\"extreme_outlier_days\")\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Combine results\n",
        "final_eda = eda_summary.merge(\n",
        "    outlier_counts, on=\"district\", how=\"left\"\n",
        ").fillna({\"extreme_outlier_days\": 0})\n",
        "\n",
        "final_eda.sort_values(\"extreme_outlier_days\", ascending=False)"
      ],
      "metadata": {
        "id": "R1H2-g1Yjhfj"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}