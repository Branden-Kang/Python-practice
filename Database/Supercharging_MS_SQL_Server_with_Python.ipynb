{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaVv0Dt/hc2AI6Z9Tc+RVr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://towardsdatascience.com/supercharging-ms-sql-server-with-python-e3335d11fa17)"
      ],
      "metadata": {
        "id": "_iYoxqmoURFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyodbc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYx_6IRHUqK0",
        "outputId": "aa52c9b0-bdec-4dd5-fb23-8fc2a55648b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyodbc\n",
            "  Downloading pyodbc-4.0.39-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (340 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.6/340.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyodbc\n",
            "Successfully installed pyodbc-4.0.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L9_ccSMiUOKn"
      },
      "outputs": [],
      "source": [
        "import pyodbc\n",
        "from datetime import datetime\n",
        "\n",
        "class Sql:\n",
        "    def __init__(self, database, server=\"XXVIR00012,55000\"):\n",
        "\n",
        "        # here we are telling python what to connect to (our SQL Server)\n",
        "        self.cnxn = pyodbc.connect(\"Driver={SQL Server Native Client 11.0};\"\n",
        "                                   \"Server=\"+server+\";\"\n",
        "                                   \"Database=\"+database+\";\"\n",
        "                                   \"Trusted_Connection=yes;\")\n",
        "\n",
        "        # initialise query attribute\n",
        "        self.query = \"-- {}\\n\\n-- Made in Python\".format(datetime.now()\n",
        "                                                         .strftime(\"%d/%m/%Y\"))\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, r'C:\\\\User\\medium\\pysqlplus\\lib')\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "sql = Sql('database123')  # initialise the Sql object\n",
        "\n",
        "directory = r'C:\\\\User\\medium\\data\\\\'  # this is where our generic data is stored\n",
        "\n",
        "file_list = os.listdir(directory)  # get a list of all files\n",
        "\n",
        "for file in file_list:  # loop to import files to sql\n",
        "    df = pd.read_csv(directory+file)  # read file to dataframe\n",
        "    sql.push_dataframe(df, file[:-4])\n",
        "    \n",
        "# now we convert our file_list names into the table names we have imported to SQL\n",
        "table_names = [x[:-4] for x in file_list]\n",
        "\n",
        "sql.union(table_names, 'generic_jan')  # union our files into one new table called 'generic_jan'\n",
        "\n",
        "sql.drop(table_names)  # drop our original tables as we now have full table\n",
        "\n",
        "# get list of categories in colX, eg ['hr', 'finance', 'tech', 'c_suite']\n",
        "sets = list(sql.manual(\"SELECT colX AS 'category' FROM generic_jan GROUP BY colX\", response=True)['category'])\n",
        "\n",
        "for category in sets:\n",
        "    sql.manual(\"SELECT * INTO generic_jan_\"+category+\" FROM generic_jan WHERE colX = '\"+category+\"'\")\n",
        "\n",
        "def push_dataframe(self, data, table=\"raw_data\", batchsize=500):\n",
        "    # create execution cursor\n",
        "    cursor = self.cnxn.cursor()\n",
        "    # activate fast execute\n",
        "    cursor.fast_executemany = True\n",
        "\n",
        "    # create create table statement\n",
        "    query = \"CREATE TABLE [\" + table + \"] (\\n\"\n",
        "\n",
        "    # iterate through each column to be included in create table statement\n",
        "    for i in range(len(list(data))):\n",
        "        query += \"\\t[{}] varchar(255)\".format(list(data)[i])  # add column (everything is varchar for now)\n",
        "        # append correct connection/end statement code\n",
        "        if i != len(list(data))-1:\n",
        "            query += \",\\n\"\n",
        "        else:\n",
        "            query += \"\\n);\"\n",
        "\n",
        "    cursor.execute(query)  # execute the create table statement\n",
        "    self.cnxn.commit()  # commit changes\n",
        "\n",
        "    # append query to our SQL code logger\n",
        "    self.query += (\"\\n\\n-- create table\\n\" + query)\n",
        "\n",
        "    # insert the data in batches\n",
        "    query = (\"INSERT INTO [{}] ({})\\n\".format(table,\n",
        "                                              '['+'], ['  # get columns\n",
        "                                              .join(list(data)) + ']') +\n",
        "             \"VALUES\\n(?{})\".format(\", ?\"*(len(list(data))-1)))\n",
        "\n",
        "    # insert data into target table in batches of 'batchsize'\n",
        "    for i in range(0, len(data), batchsize):\n",
        "        if i+batchsize > len(data):\n",
        "            batch = data[i: len(data)].values.tolist()\n",
        "        else:\n",
        "            batch = data[i: i+batchsize].values.tolist()\n",
        "        # execute batch insert\n",
        "        cursor.executemany(query, batch)\n",
        "        # commit insert to SQL Server\n",
        "        self.cnxn.commit() \n",
        "\n",
        "def manual(self, query, response=False):\n",
        "    cursor = self.cnxn.cursor()  # create execution cursor\n",
        "\n",
        "    if response:\n",
        "        return read_sql(query, self.cnxn)  # get sql query output to dataframe\n",
        "    try:\n",
        "        cursor.execute(query)  # execute\n",
        "    except pyodbc.ProgrammingError as error:\n",
        "        print(\"Warning:\\n{}\".format(error))  # print error as a warning\n",
        "\n",
        "    self.cnxn.commit()  # commit query to SQL Server\n",
        "    return \"Query complete.\"\n",
        "\n",
        "def union(self, table_list, name=\"union\", join=\"UNION\"):\n",
        "    \n",
        "    # initialise the query\n",
        "    query = \"SELECT * INTO [\"+name+\"] FROM (\\n\"\n",
        "\n",
        "    # build the SQL query\n",
        "    query += f'\\n{join}\\n'.join(\n",
        "                        [f'SELECT [{x}].* FROM [{x}]' for x in table_list]\n",
        "                        )\n",
        "\n",
        "    query += \") x\"  # add end of query\n",
        "    self.manual(query, fast=True)  # fast execute\n",
        "\n",
        "def drop(self, tables):\n",
        "\n",
        "    # check if single or list\n",
        "    if isinstance(tables, str):\n",
        "        # if single string, convert to single item in list for for-loop\n",
        "        tables = [tables]\n",
        "\n",
        "    for table in tables:\n",
        "        # check for pre-existing table and delete if present\n",
        "        query = (\"IF OBJECT_ID ('[\"+table+\"]', 'U') IS NOT NULL \"\n",
        "                 \"DROP TABLE [\"+table+\"]\")\n",
        "        self.manual(query)  # execute"
      ]
    }
  ]
}
