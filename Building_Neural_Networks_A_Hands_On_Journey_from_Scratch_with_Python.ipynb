{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM53kwm05HcE7nDD6invquH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@thisislong/building-a-neural-network-from-scratch-with-backpropagation-a789bec70b29)"
      ],
      "metadata": {
        "id": "4XIJvpFYASO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building A Neural Network Without Using External Libraries"
      ],
      "metadata": {
        "id": "StGg1LtHAjY8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gc0Eb7yA_nkr"
      },
      "outputs": [],
      "source": [
        "from math import exp\n",
        "\n",
        "def sigmoid(x: float) -> float:\n",
        "    return 1.0 / (1.0 + exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_derivative(z: float) -> float:\n",
        "    return z * (1.0 - z)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class Neuron:\n",
        "    weights: List[float]\n",
        "    bias: float\n",
        "    delta: Optional[float] = 0.0\n",
        "    output: Optional[float] = 0.0\n",
        "\n",
        "    def _set_output(self, output: float) -> None:\n",
        "        self.output = output\n",
        "\n",
        "    def set_delta(self, error: float) -> None:\n",
        "        self.delta = error * sigmoid_derivative(self.output)\n",
        "\n",
        "    def weighted_sum(self, inputs: List[float]) -> float:\n",
        "        \"\"\"\n",
        "        Usually results in a big number, but we tend to use a value [0, 1] for activation\n",
        "        Hence, after calculating this, we use the sigmoid function to normalize the result\n",
        "        \"\"\"\n",
        "        ws = self.bias\n",
        "        for i in range(len(self.weights)):\n",
        "            ws += self.weights[i] * inputs[i]\n",
        "        return ws\n",
        "\n",
        "    def activate(self, inputs: List[float]) -> float:\n",
        "        \"\"\"\n",
        "        Calculates the output of the neuron using a non-linear activation function\n",
        "        In this case we use the sigmoid function\n",
        "        \"\"\"\n",
        "        output = sigmoid(self.weighted_sum(inputs))\n",
        "        self._set_output(output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "tOAL8QqlAkxm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class Layer:\n",
        "    neurons: List[Neuron]\n",
        "\n",
        "    @property\n",
        "    def all_outputs(self) -> List[float]:\n",
        "        return [neuron.output for neuron in self.neurons]\n",
        "\n",
        "    def activate_neurons(self, inputs: List[float]) -> List[float]:\n",
        "        return [neuron.activate(inputs) for neuron in self.neurons]\n",
        "\n",
        "    def total_delta(self, previous_layer_neuron_idx: int) -> float:\n",
        "        return sum(\n",
        "            neuron.weights[previous_layer_neuron_idx] * neuron.delta\n",
        "            for neuron in self.neurons\n",
        "        )"
      ],
      "metadata": {
        "id": "17jxbXIPAqNt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Optional\n",
        "import random\n",
        "\n",
        "@dataclass\n",
        "class Network:\n",
        "    hidden_layers: List[Layer]\n",
        "    output_layer: Layer\n",
        "    learning_rate: float\n",
        "\n",
        "    @property\n",
        "    def layers(self) -> List[Layer]:\n",
        "        return self.hidden_layers + [self.output_layer]\n",
        "\n",
        "    def feed_forward(self, inputs: List[float]) -> List[float]:\n",
        "        for layer in self.hidden_layers:\n",
        "            # update inputs as outputs of previous layers as we go\n",
        "            inputs = layer.activate_neurons(inputs)\n",
        "        return self.output_layer.activate_neurons(inputs)\n",
        "\n",
        "    def derivative_error_to_output(\n",
        "        self, actual: List[float], expected: List[float]\n",
        "    ) -> List[float]:\n",
        "        \"\"\"\n",
        "        Derivative of error function with respect to the output\n",
        "        \"\"\"\n",
        "        return [actual[i] - expected[i] for i in range(len(actual))]\n",
        "\n",
        "    def back_propagate(self, inputs: List[float], errors: List[float]) -> None:\n",
        "        \"\"\"\n",
        "        Compute the gradient and then update the weights\n",
        "        \"\"\"\n",
        "\n",
        "        # Delta of output layer = derivative of the error functions times the derivative of output layer activation function\n",
        "        # We calculate deltas of output layer first\n",
        "        # So when we get to hidden layers, the output deltas are ready to be used in calculation (work backwards)\n",
        "        for index, neuron in enumerate(self.output_layer.neurons):\n",
        "            neuron.set_delta(errors[index])\n",
        "\n",
        "        # Calculate deltas of hidden layer\n",
        "        for layer_idx in reversed(range(len(self.hidden_layers))):\n",
        "            layer = self.hidden_layers[layer_idx]\n",
        "            next_layer = (\n",
        "                self.output_layer\n",
        "                if layer_idx == len(self.hidden_layers) - 1\n",
        "                else self.hidden_layers[layer_idx + 1]\n",
        "            )\n",
        "            for neuron_idx, neuron in enumerate(layer.neurons):\n",
        "                error_from_next_layer = next_layer.total_delta(neuron_idx)\n",
        "                neuron.set_delta(error_from_next_layer)\n",
        "\n",
        "        # Only update the weights after you've calculated the deltas\n",
        "        # If you update the weights as you move through the network, it will affect the deltas of other layers\n",
        "        self.update_weights_for_all_layers(inputs)\n",
        "\n",
        "    def update_weights_for_all_layers(self, inputs: List[float]):\n",
        "        \"\"\"\n",
        "        Update weights for all layers\n",
        "        \"\"\"\n",
        "        # Update weights for hidden layers\n",
        "        for layer_idx in range(len(self.hidden_layers)):\n",
        "            layer = self.hidden_layers[layer_idx]\n",
        "            previous_layer_outputs: List[float] = (\n",
        "                inputs\n",
        "                if layer_idx == 0\n",
        "                else self.hidden_layers[layer_idx - 1].all_outputs\n",
        "            )\n",
        "            for neuron in layer.neurons:\n",
        "                self.update_weights_in_a_layer(previous_layer_outputs, neuron)\n",
        "\n",
        "        # Update weights for output layer\n",
        "        for index, neuron in enumerate(self.output_layer.neurons):\n",
        "            self.update_weights_in_a_layer(self.hidden_layers[-1].all_outputs, neuron)\n",
        "\n",
        "    def update_weights_in_a_layer(\n",
        "        self, previous_layer_outputs: List[float], neuron: Neuron\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Update weights in all neurons in a layer\n",
        "        \"\"\"\n",
        "        for idx in range(len(previous_layer_outputs)):\n",
        "            neuron.weights[idx] -= (\n",
        "                self.learning_rate * neuron.delta * previous_layer_outputs[idx]\n",
        "            )\n",
        "            neuron.bias -= self.learning_rate * neuron.delta\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        num_epoch: int,\n",
        "        num_outputs: int,\n",
        "        training_set: List[List[float]],\n",
        "        training_output: List[float],\n",
        "    ) -> None:\n",
        "        for epoch in range(num_epoch):\n",
        "            sum_error = 0.0\n",
        "            for idx, row in enumerate(training_set):\n",
        "                expected = [0 for _ in range(num_outputs)]\n",
        "                expected[training_output[idx]] = 1  # one-hot encoding\n",
        "                actual = self.feed_forward(row)\n",
        "                errors = self.derivative_error_to_output(actual, expected)\n",
        "                self.back_propagate(row, errors)\n",
        "                sum_error += self.mse(actual, training_output)\n",
        "            print(f\"Mean squared error: {sum_error}\")\n",
        "            print(f\"epoch={epoch}\")\n",
        "\n",
        "    def predict(self, inputs: List[float]) -> int:\n",
        "        outputs = self.feed_forward(inputs)\n",
        "        return outputs.index(max(outputs))\n",
        "\n",
        "    def mse(self, actual: List[float], expected: List[float]) -> float:\n",
        "        \"\"\"\n",
        "        Mean Squared Error formula\n",
        "        \"\"\"\n",
        "        return sum((actual[i] - expected[i]) ** 2 for i in range(len(actual))) / len(\n",
        "            actual\n",
        "        )"
      ],
      "metadata": {
        "id": "opl2COOnAsdW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_make_prediction_with_network():\n",
        "    # Test making predictions with the network\n",
        "    # Mock data is from https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
        "    dataset = [\n",
        "        [2.7810836, 2.550537003],\n",
        "        [1.465489372, 2.362125076],\n",
        "        [3.396561688, 4.400293529],\n",
        "        [1.38807019, 1.850220317],\n",
        "        [3.06407232, 3.005305973],\n",
        "        [7.627531214, 2.759262235],\n",
        "        [5.332441248, 2.088626775],\n",
        "        [6.922596716, 1.77106367],\n",
        "        [8.675418651, -0.242068655],\n",
        "        [7.673756466, 3.508563011],\n",
        "    ]\n",
        "    expected = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
        "    n_inputs = len(dataset[0])\n",
        "    n_outputs = len(set(expected))\n",
        "    hidden_layers = [\n",
        "        Layer(\n",
        "            neurons=[\n",
        "                Neuron(weights=[random.random() for _ in range(n_inputs)], bias=random.random()),\n",
        "                Neuron(weights=[random.random() for _ in range(n_inputs)], bias=random.random()),\n",
        "            ],\n",
        "        )\n",
        "    ]\n",
        "    output_layer = Layer(\n",
        "        neurons=[\n",
        "            Neuron(weights=[random.random() for _ in range(n_outputs)], bias=random.random()),\n",
        "            Neuron(weights=[random.random() for _ in range(n_outputs)], bias=random.random()),\n",
        "        ],\n",
        "    )\n",
        "    network = Network(\n",
        "        hidden_layers=hidden_layers, output_layer=output_layer, learning_rate=0.5\n",
        "    )\n",
        "    network.train(40, n_outputs, dataset, expected)\n",
        "    print(f\"Hidden layer: {network.layers[0].neurons}\")\n",
        "    print(f\"Output layer: {network.layers[1].neurons}\")\n",
        "\n",
        "    # This is just for demonstration only\n",
        "    for i in range(len(dataset)):\n",
        "        prediction = network.predict(dataset[i])\n",
        "        print(\"Expected=%d, Got=%d\" % (expected[i], prediction))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_make_prediction_with_network()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdmjFuLGA5JT",
        "outputId": "20ed322b-19c1-4f42-cdea-95c261273898"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 4.246857416591776\n",
            "epoch=0\n",
            "Mean squared error: 3.157097886930553\n",
            "epoch=1\n",
            "Mean squared error: 2.776520268796843\n",
            "epoch=2\n",
            "Mean squared error: 2.6494176465949275\n",
            "epoch=3\n",
            "Mean squared error: 2.6063303771640127\n",
            "epoch=4\n",
            "Mean squared error: 2.591541010399624\n",
            "epoch=5\n",
            "Mean squared error: 2.586370107079758\n",
            "epoch=6\n",
            "Mean squared error: 2.584464752967969\n",
            "epoch=7\n",
            "Mean squared error: 2.583648578883447\n",
            "epoch=8\n",
            "Mean squared error: 2.583168598425941\n",
            "epoch=9\n",
            "Mean squared error: 2.582755079784383\n",
            "epoch=10\n",
            "Mean squared error: 2.58229775935919\n",
            "epoch=11\n",
            "Mean squared error: 2.581731080819543\n",
            "epoch=12\n",
            "Mean squared error: 2.5809875148105283\n",
            "epoch=13\n",
            "Mean squared error: 2.579966734358143\n",
            "epoch=14\n",
            "Mean squared error: 2.578493041250705\n",
            "epoch=15\n",
            "Mean squared error: 2.576226357294675\n",
            "epoch=16\n",
            "Mean squared error: 2.572451023802584\n",
            "epoch=17\n",
            "Mean squared error: 2.5656427016275236\n",
            "epoch=18\n",
            "Mean squared error: 2.5539378778035595\n",
            "epoch=19\n",
            "Mean squared error: 2.5436556542262614\n",
            "epoch=20\n",
            "Mean squared error: 2.5505678821172206\n",
            "epoch=21\n",
            "Mean squared error: 2.5709655641012183\n",
            "epoch=22\n",
            "Mean squared error: 2.5953290792366417\n",
            "epoch=23\n",
            "Mean squared error: 2.621166447001852\n",
            "epoch=24\n",
            "Mean squared error: 2.649929623500983\n",
            "epoch=25\n",
            "Mean squared error: 2.6833284642033646\n",
            "epoch=26\n",
            "Mean squared error: 2.7217562293195527\n",
            "epoch=27\n",
            "Mean squared error: 2.7646502802995316\n",
            "epoch=28\n",
            "Mean squared error: 2.8111124136129835\n",
            "epoch=29\n",
            "Mean squared error: 2.8601295728990555\n",
            "epoch=30\n",
            "Mean squared error: 2.9106662572039355\n",
            "epoch=31\n",
            "Mean squared error: 2.961771157857126\n",
            "epoch=32\n",
            "Mean squared error: 3.012648355096211\n",
            "epoch=33\n",
            "Mean squared error: 3.062675763621446\n",
            "epoch=34\n",
            "Mean squared error: 3.111395278429797\n",
            "epoch=35\n",
            "Mean squared error: 3.158492192409702\n",
            "epoch=36\n",
            "Mean squared error: 3.203770250864439\n",
            "epoch=37\n",
            "Mean squared error: 3.247125907278902\n",
            "epoch=38\n",
            "Mean squared error: 3.2885246614038386\n",
            "epoch=39\n",
            "Hidden layer: [Neuron(weights=[0.9304806206630007, 0.24147794386229496], bias=1.0695702878445577, delta=1.790674651354764e-06, output=0.999883459840953), Neuron(weights=[1.5355103925433777, -2.0973453267913675], bias=-0.972861950766297, delta=-0.006161283983772025, output=0.9618051151093413)]\n",
            "Output layer: [Neuron(weights=[0.17550496273765498, -2.775607033050673], bias=1.0232726795964522, delta=0.030946650079655986, output=0.196217402646413), Neuron(weights=[-0.3287200413122424, 2.9934679646739095], bias=-1.0063736173630664, delta=-0.027609825222056628, output=0.8160625770647171)]\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building A Neural Network With Numpy"
      ],
      "metadata": {
        "id": "JXF6t0CHBh_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_derivative(z: float) -> float:\n",
        "    return z * (1.0 - z)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Layer:\n",
        "    weights: np.array\n",
        "    bias: np.array\n",
        "    outputs: np.array\n",
        "    deltas: np.array\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Network:\n",
        "    layers: List[Layer]\n",
        "    learning_rate: Optional[int] = 0.5\n",
        "\n",
        "    @property\n",
        "    def length(self) -> int:\n",
        "        return len(self.layers)\n",
        "\n",
        "    @property\n",
        "    def outputs(self) -> np.array:\n",
        "        return self.layers[-1].outputs\n",
        "\n",
        "    @staticmethod\n",
        "    def create(\n",
        "        layers: List[int],\n",
        "    ) -> Network:\n",
        "        \"\"\"\n",
        "        Create a network with random weights and biases given a list of layers\n",
        "        The \"layers\" is a list of the number of neurons in each layer\n",
        "        \"\"\"\n",
        "        layers = [\n",
        "            Layer(\n",
        "                # layers[i] is the number of neurons in layer i (row), layers[i - 1] is the number of weights, matching with number of neurons in layer i - 1 (column)\n",
        "                weights=np.random.rand(layers[i], layers[i - 1]),\n",
        "                bias=np.random.rand(layers[i]),\n",
        "                outputs=np.zeros(layers[i]),\n",
        "                deltas=np.zeros(layers[i]),\n",
        "            )\n",
        "            for i in range(1, len(layers))\n",
        "        ]\n",
        "\n",
        "        return Network(layers=layers)\n",
        "\n",
        "    def feed_forward(self, inputs: np.array) -> np.array:\n",
        "        for layer in self.layers:\n",
        "            # layer.outputs is a (3,1) - dimension we expect\n",
        "            # layer.weights is a (3,2), inputs is a (2,1) - multiply to get (3,1)\n",
        "            layer.outputs = sigmoid(layer.weights @ inputs + layer.bias)  # == np.matmul, https://stackoverflow.com/a/34142617\n",
        "            inputs = layer.outputs\n",
        "        return self.layers[-1].outputs\n",
        "\n",
        "    def back_propagate(self, inputs: np.array, expected: np.array) -> None:\n",
        "        for idx in reversed(range(self.length)):\n",
        "            layer = self.layers[idx]\n",
        "            if idx == len(self.layers) - 1:  # if last layer (output layer)\n",
        "                layer.deltas = (layer.outputs - expected) * sigmoid_derivative(\n",
        "                    layer.outputs\n",
        "                )\n",
        "            else:\n",
        "                next_layer = self.layers[idx + 1]\n",
        "                # layer.deltas is a (3,1) - the dimension we expect\n",
        "                # next_layer.weights is a (2,3), next_layer.deltas is a (2,1)\n",
        "                # need to transpose next_layer.weights to get (3,2) then multiply by next_layer.deltas (2,1) to get (3,1)\n",
        "                layer.deltas = (\n",
        "                    next_layer.weights.T @ next_layer.deltas\n",
        "                    * sigmoid_derivative(layer.outputs)\n",
        "                ) * sigmoid_derivative(layer.outputs)\n",
        "\n",
        "        self.update_weights(inputs)\n",
        "\n",
        "    def update_weights(self, inputs: np.array) -> None:\n",
        "        for idx in range(self.length):\n",
        "            layer = self.layers[idx]\n",
        "            previous_layer_outputs = self.layers[idx - 1].outputs if idx > 0 else inputs\n",
        "            # deltas (3,) -> deltas[np.newaxis] (1, 3) -> .T (3, 1)\n",
        "            # previous_layer_outputs (2,) -> previous_layer_outputs[np.newaxis] (1, 2)\n",
        "            # (3,1) @ (1,2) = (3,2) for weights\n",
        "            layer.weights -= (\n",
        "                layer.deltas[np.newaxis].T\n",
        "                @ previous_layer_outputs[np.newaxis]\n",
        "                * self.learning_rate\n",
        "            )\n",
        "            layer.bias -= layer.deltas * self.learning_rate\n",
        "\n",
        "    def train(self, inputs: np.array, expected: np.array, epochs: int) -> None:\n",
        "        for epoch in range(epochs):\n",
        "            sum_error = 0.0\n",
        "            for idx, row in enumerate(inputs):\n",
        "                actual = self.feed_forward(row)\n",
        "                self.back_propagate(row, expected[idx])\n",
        "                sum_error += self.mse(actual, expected[idx])\n",
        "            print(f\"Mean squared error: {sum_error}\")\n",
        "            print(f\"epoch={epoch}\")\n",
        "\n",
        "    def mse(self, actual: np.array, expected: np.array) -> float:\n",
        "        return np.power(actual - expected, 2).mean()\n",
        "\n",
        "    def predict(self, inputs: np.array) -> int:\n",
        "        outputs = self.feed_forward(inputs)\n",
        "        return np.where(outputs == outputs.max())[0][0]"
      ],
      "metadata": {
        "id": "M_E4mJatA75-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_make_prediction_with_network():\n",
        "    # Test making predictions with the network\n",
        "    # Mock data is from https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
        "    dataset = np.array(\n",
        "        [\n",
        "            [2.7810836, 2.550537003],\n",
        "            [1.465489372, 2.362125076],\n",
        "            [3.396561688, 4.400293529],\n",
        "            [1.38807019, 1.850220317],\n",
        "            [3.06407232, 3.005305973],\n",
        "            [7.627531214, 2.759262235],\n",
        "            [5.332441248, 2.088626775],\n",
        "            [6.922596716, 1.77106367],\n",
        "            [8.675418651, -0.242068655],\n",
        "            [7.673756466, 3.508563011],\n",
        "        ]\n",
        "    )\n",
        "    expected = np.array(\n",
        "        [\n",
        "            [1, 0],\n",
        "            [1, 0],\n",
        "            [1, 0],\n",
        "            [1, 0],\n",
        "            [1, 0],\n",
        "            [0, 1],\n",
        "            [0, 1],\n",
        "            [0, 1],\n",
        "            [0, 1],\n",
        "            [0, 1],\n",
        "        ]\n",
        "    )\n",
        "    # 2 input neurons, 3 hidden neurons, 2 output neurons\n",
        "    network = Network.create([len(dataset[0]), 3, len(expected[0])])\n",
        "    network.train(dataset, expected, 40)\n",
        "    for i in range(len(dataset)):\n",
        "        prediction = network.predict(dataset[i])\n",
        "        print(\n",
        "            f\"{i} - Expected={np.where(expected[i] == expected[i].max())[0][0]}, Got={prediction}\"\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_make_prediction_with_network()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY6B1MH1BlrO",
        "outputId": "bba64eca-ef2d-41e4-9f51-bc840c983539"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 3.578942020721095\n",
            "epoch=0\n",
            "Mean squared error: 3.0158318584631347\n",
            "epoch=1\n",
            "Mean squared error: 2.6781737181859366\n",
            "epoch=2\n",
            "Mean squared error: 2.6823987470746813\n",
            "epoch=3\n",
            "Mean squared error: 2.7161663842292865\n",
            "epoch=4\n",
            "Mean squared error: 2.7306571688562817\n",
            "epoch=5\n",
            "Mean squared error: 2.7357596877382626\n",
            "epoch=6\n",
            "Mean squared error: 2.737360976486021\n",
            "epoch=7\n",
            "Mean squared error: 2.737724005938362\n",
            "epoch=8\n",
            "Mean squared error: 2.737654816827882\n",
            "epoch=9\n",
            "Mean squared error: 2.7374345275320864\n",
            "epoch=10\n",
            "Mean squared error: 2.737160491228439\n",
            "epoch=11\n",
            "Mean squared error: 2.7368663096174157\n",
            "epoch=12\n",
            "Mean squared error: 2.736563535492673\n",
            "epoch=13\n",
            "Mean squared error: 2.7362561034099953\n",
            "epoch=14\n",
            "Mean squared error: 2.7359453157234777\n",
            "epoch=15\n",
            "Mean squared error: 2.7356315642600824\n",
            "epoch=16\n",
            "Mean squared error: 2.7353149245792587\n",
            "epoch=17\n",
            "Mean squared error: 2.734995360985862\n",
            "epoch=18\n",
            "Mean squared error: 2.734672797176156\n",
            "epoch=19\n",
            "Mean squared error: 2.7343471405034716\n",
            "epoch=20\n",
            "Mean squared error: 2.7340182902320884\n",
            "epoch=21\n",
            "Mean squared error: 2.7336861402592953\n",
            "epoch=22\n",
            "Mean squared error: 2.73335057992138\n",
            "epoch=23\n",
            "Mean squared error: 2.733011494130483\n",
            "epoch=24\n",
            "Mean squared error: 2.7326687632718936\n",
            "epoch=25\n",
            "Mean squared error: 2.7323222630093182\n",
            "epoch=26\n",
            "Mean squared error: 2.731971864048259\n",
            "epoch=27\n",
            "Mean squared error: 2.7316174318740867\n",
            "epoch=28\n",
            "Mean squared error: 2.731258826469616\n",
            "epoch=29\n",
            "Mean squared error: 2.7308959020129926\n",
            "epoch=30\n",
            "Mean squared error: 2.730528506555141\n",
            "epoch=31\n",
            "Mean squared error: 2.730156481675432\n",
            "epoch=32\n",
            "Mean squared error: 2.7297796621139514\n",
            "epoch=33\n",
            "Mean squared error: 2.72939787537848\n",
            "epoch=34\n",
            "Mean squared error: 2.729010941324148\n",
            "epoch=35\n",
            "Mean squared error: 2.7286186717034875\n",
            "epoch=36\n",
            "Mean squared error: 2.728220869684414\n",
            "epoch=37\n",
            "Mean squared error: 2.727817329333408\n",
            "epoch=38\n",
            "Mean squared error: 2.7274078350608932\n",
            "epoch=39\n",
            "0 - Expected=0, Got=1\n",
            "1 - Expected=0, Got=1\n",
            "2 - Expected=0, Got=1\n",
            "3 - Expected=0, Got=1\n",
            "4 - Expected=0, Got=1\n",
            "5 - Expected=1, Got=1\n",
            "6 - Expected=1, Got=1\n",
            "7 - Expected=1, Got=1\n",
            "8 - Expected=1, Got=1\n",
            "9 - Expected=1, Got=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building A Neural Network With Tensorflow"
      ],
      "metadata": {
        "id": "l4LqLzgdBv0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "def build_model() -> tf.keras.Sequential:\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "            layers.Dense(units=3, activation=\"sigmoid\", input_shape=(2,)),\n",
        "            layers.Dense(units=2),\n",
        "        ]\n",
        "    )\n",
        "    model.summary()\n",
        "    loss_fn = keras.losses.MeanSquaredError()\n",
        "    model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = np.array(\n",
        "        [\n",
        "            [2.7810836, 2.550537003],\n",
        "            [1.465489372, 2.362125076],\n",
        "            [3.396561688, 4.400293529],\n",
        "            [1.38807019, 1.850220317],\n",
        "            [3.06407232, 3.005305973],\n",
        "            [7.627531214, 2.759262235],\n",
        "            [5.332441248, 2.088626775],\n",
        "            [6.922596716, 1.77106367],\n",
        "            [8.675418651, -0.242068655],\n",
        "            [7.673756466, 3.508563011],\n",
        "        ]\n",
        "    )\n",
        "    expected = np.array(\n",
        "        [\n",
        "            [1, 0],\n",
        "            [1, 0],\n",
        "            [1, 0],\n",
        "            [1, 0],\n",
        "            [1, 0],\n",
        "            [0, 1],\n",
        "            [0, 1],\n",
        "            [0, 1],\n",
        "            [0, 1],\n",
        "            [0, 1],\n",
        "        ]\n",
        "    )\n",
        "    model = build_model()\n",
        "    # Convert the data to TensorFlow format\n",
        "    dataset_tf = tf.constant(dataset, dtype=tf.float32)\n",
        "    expected_tf = tf.constant(expected, dtype=tf.float32)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(dataset_tf, expected_tf, epochs=200)\n",
        "    predictions = model.predict(dataset_tf)\n",
        "    print(predictions)\n",
        "\n",
        "    # Convert continuous predictions to class labels (0 or 1)\n",
        "    class_predictions = np.argmax(predictions, axis=1)\n",
        "    print(class_predictions)\n",
        "\n",
        "    # Print the comparison\n",
        "    for i, (expected_row, prediction) in enumerate(zip(expected, class_predictions)):\n",
        "        print(f\"{i} - Expected={expected_row.argmax()}, Got={prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E0_PXR1Btlp",
        "outputId": "7b6ddc15-2982-4bb6-b32d-827e3eaf3fe7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 3)                 9         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 8         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17 (68.00 Byte)\n",
            "Trainable params: 17 (68.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "1/1 [==============================] - 1s 791ms/step - loss: 0.7237 - accuracy: 0.5000\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7175 - accuracy: 0.5000\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7112 - accuracy: 0.5000\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7051 - accuracy: 0.5000\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6990 - accuracy: 0.5000\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6929 - accuracy: 0.5000\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6869 - accuracy: 0.5000\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6810 - accuracy: 0.5000\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6751 - accuracy: 0.5000\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6692 - accuracy: 0.5000\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6634 - accuracy: 0.5000\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6577 - accuracy: 0.5000\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6520 - accuracy: 0.5000\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6464 - accuracy: 0.5000\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6409 - accuracy: 0.5000\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6354 - accuracy: 0.5000\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6299 - accuracy: 0.5000\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6245 - accuracy: 0.5000\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6192 - accuracy: 0.5000\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6139 - accuracy: 0.5000\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6087 - accuracy: 0.5000\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6036 - accuracy: 0.5000\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5985 - accuracy: 0.5000\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5935 - accuracy: 0.5000\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5885 - accuracy: 0.5000\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5836 - accuracy: 0.5000\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5787 - accuracy: 0.5000\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5739 - accuracy: 0.5000\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5692 - accuracy: 0.5000\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.5645 - accuracy: 0.5000\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5599 - accuracy: 0.5000\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5553 - accuracy: 0.5000\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5508 - accuracy: 0.5000\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.5463 - accuracy: 0.5000\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5419 - accuracy: 0.5000\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.5376 - accuracy: 0.5000\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5333 - accuracy: 0.5000\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5290 - accuracy: 0.5000\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5248 - accuracy: 0.5000\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5207 - accuracy: 0.5000\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.5166 - accuracy: 0.5000\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.5126 - accuracy: 0.5000\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5086 - accuracy: 0.5000\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5046 - accuracy: 0.5000\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5007 - accuracy: 0.5000\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4969 - accuracy: 0.5000\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4931 - accuracy: 0.5000\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4894 - accuracy: 0.5000\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4857 - accuracy: 0.5000\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4820 - accuracy: 0.5000\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4784 - accuracy: 0.5000\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4748 - accuracy: 0.5000\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4713 - accuracy: 0.5000\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4678 - accuracy: 0.5000\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4644 - accuracy: 0.5000\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4610 - accuracy: 0.5000\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4576 - accuracy: 0.5000\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4543 - accuracy: 0.5000\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4511 - accuracy: 0.5000\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4478 - accuracy: 0.5000\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4446 - accuracy: 0.5000\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4415 - accuracy: 0.5000\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4383 - accuracy: 0.5000\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4352 - accuracy: 0.5000\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4322 - accuracy: 0.5000\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4292 - accuracy: 0.5000\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4262 - accuracy: 0.5000\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4232 - accuracy: 0.5000\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4203 - accuracy: 0.5000\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4174 - accuracy: 0.5000\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4146 - accuracy: 0.5000\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4117 - accuracy: 0.5000\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4090 - accuracy: 0.5000\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4062 - accuracy: 0.5000\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4035 - accuracy: 0.5000\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4008 - accuracy: 0.5000\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3981 - accuracy: 0.5000\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3955 - accuracy: 0.5000\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3929 - accuracy: 0.5000\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3903 - accuracy: 0.5000\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3877 - accuracy: 0.5000\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3852 - accuracy: 0.5000\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3827 - accuracy: 0.5000\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3802 - accuracy: 0.5000\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3778 - accuracy: 0.5000\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3754 - accuracy: 0.5000\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3730 - accuracy: 0.5000\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3706 - accuracy: 0.5000\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3683 - accuracy: 0.5000\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3659 - accuracy: 0.5000\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3636 - accuracy: 0.5000\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3614 - accuracy: 0.5000\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3591 - accuracy: 0.5000\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3569 - accuracy: 0.5000\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3547 - accuracy: 0.5000\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3525 - accuracy: 0.5000\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3504 - accuracy: 0.5000\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3483 - accuracy: 0.5000\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3462 - accuracy: 0.5000\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3441 - accuracy: 0.5000\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3420 - accuracy: 0.5000\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3400 - accuracy: 0.5000\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3380 - accuracy: 0.5000\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3360 - accuracy: 0.5000\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3340 - accuracy: 0.5000\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3320 - accuracy: 0.5000\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3301 - accuracy: 0.5000\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3282 - accuracy: 0.5000\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3263 - accuracy: 0.5000\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3244 - accuracy: 0.5000\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3226 - accuracy: 0.5000\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3208 - accuracy: 0.5000\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3190 - accuracy: 0.5000\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3172 - accuracy: 0.5000\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3154 - accuracy: 0.5000\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3136 - accuracy: 0.5000\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3119 - accuracy: 0.5000\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3102 - accuracy: 0.5000\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3085 - accuracy: 0.5000\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3068 - accuracy: 0.5000\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3052 - accuracy: 0.5000\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3035 - accuracy: 0.5000\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3019 - accuracy: 0.5000\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3003 - accuracy: 0.5000\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2987 - accuracy: 0.5000\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2971 - accuracy: 0.5000\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2956 - accuracy: 0.5000\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2940 - accuracy: 0.5000\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2925 - accuracy: 0.5000\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2910 - accuracy: 0.5000\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2895 - accuracy: 0.5000\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2881 - accuracy: 0.5000\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2866 - accuracy: 0.5000\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2852 - accuracy: 0.5000\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2837 - accuracy: 0.5000\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2823 - accuracy: 0.5000\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2810 - accuracy: 0.5000\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2796 - accuracy: 0.5000\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2782 - accuracy: 0.5000\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2769 - accuracy: 0.5000\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2755 - accuracy: 0.5000\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2742 - accuracy: 0.5000\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2729 - accuracy: 0.5000\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2716 - accuracy: 0.5000\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2704 - accuracy: 0.5000\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2691 - accuracy: 0.5000\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2679 - accuracy: 0.5000\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2666 - accuracy: 0.5000\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2654 - accuracy: 0.5000\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2642 - accuracy: 0.5000\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2630 - accuracy: 0.5000\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2618 - accuracy: 0.5000\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2607 - accuracy: 0.5000\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2595 - accuracy: 0.5000\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2584 - accuracy: 0.5000\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2573 - accuracy: 0.5000\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2562 - accuracy: 0.5000\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2551 - accuracy: 0.5000\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2540 - accuracy: 0.5000\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2529 - accuracy: 0.5000\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2518 - accuracy: 0.5000\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2508 - accuracy: 0.5000\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2498 - accuracy: 0.5000\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2487 - accuracy: 0.5000\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2477 - accuracy: 0.5000\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2467 - accuracy: 0.5000\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2457 - accuracy: 0.5000\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2447 - accuracy: 0.5000\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2438 - accuracy: 0.5000\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2428 - accuracy: 0.5000\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2419 - accuracy: 0.5000\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2409 - accuracy: 0.5000\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2400 - accuracy: 0.5000\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2391 - accuracy: 0.5000\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2382 - accuracy: 0.5000\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2373 - accuracy: 0.5000\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2364 - accuracy: 0.5000\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2355 - accuracy: 0.5000\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2347 - accuracy: 0.5000\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2338 - accuracy: 0.5000\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2330 - accuracy: 0.5000\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2321 - accuracy: 0.5000\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2313 - accuracy: 0.5000\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2305 - accuracy: 0.5000\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2297 - accuracy: 0.5000\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2289 - accuracy: 0.5000\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2281 - accuracy: 0.5000\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2273 - accuracy: 0.5000\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2266 - accuracy: 0.5000\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2258 - accuracy: 0.5000\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2251 - accuracy: 0.5000\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2243 - accuracy: 0.5000\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2236 - accuracy: 0.5000\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2228 - accuracy: 0.5000\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2221 - accuracy: 0.5000\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2214 - accuracy: 0.5000\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2207 - accuracy: 0.5000\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2200 - accuracy: 0.5000\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2193 - accuracy: 0.5000\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2187 - accuracy: 0.5000\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "[[0.40678424 0.6696748 ]\n",
            " [0.47138238 0.6273898 ]\n",
            " [0.40048307 0.6394241 ]\n",
            " [0.47006404 0.6424904 ]\n",
            " [0.39851755 0.6650973 ]\n",
            " [0.26614067 0.76290995]\n",
            " [0.30772305 0.75190896]\n",
            " [0.2717209  0.7841773 ]\n",
            " [0.24507126 0.8626028 ]\n",
            " [0.27038753 0.7454967 ]]\n",
            "[1 1 1 1 1 1 1 1 1 1]\n",
            "0 - Expected=0, Got=1\n",
            "1 - Expected=0, Got=1\n",
            "2 - Expected=0, Got=1\n",
            "3 - Expected=0, Got=1\n",
            "4 - Expected=0, Got=1\n",
            "5 - Expected=1, Got=1\n",
            "6 - Expected=1, Got=1\n",
            "7 - Expected=1, Got=1\n",
            "8 - Expected=1, Got=1\n",
            "9 - Expected=1, Got=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X5hTVfduBzXb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}