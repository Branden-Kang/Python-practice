{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOONtATeomJXxTQuOIwfkRp"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://blog.dataengineerthings.org/duckdb-and-cloud-data-warehouses-how-to-choose-for-your-project-54c7576496ae)"
      ],
      "metadata": {
        "id": "SqscQER8ysFA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w8qJPSuWykzt"
      },
      "outputs": [],
      "source": [
        "import duckdb\n",
        "\n",
        "# 1. Define the Data Sources\n",
        "# Note: The read_csv_auto() function reads a local file directly.\n",
        "LOCAL_SALES_CSV = 'data/local_sales_2023.csv'\n",
        "\n",
        "# Note: This reads a public Parquet file directly from the internet (simulating S3/GCS).\n",
        "REMOTE_CATALOG_PARQUET = 'https://blobs.duckdb.org/train_services.parquet'\n",
        "\n",
        "# 2. Run the Polylingual SQL Query\n",
        "result = duckdb.sql(f\"\"\"\n",
        "    SELECT\n",
        "        catalog.pickup_station_name,\n",
        "        COUNT(sales.*) AS total_sales_count\n",
        "    FROM\n",
        "        read_csv_auto('{LOCAL_SALES_CSV}') AS sales\n",
        "    JOIN\n",
        "        read_parquet('{REMOTE_CATALOG_PARQUET}') AS catalog\n",
        "    ON\n",
        "        sales.item_id = catalog.destination_station_name\n",
        "    GROUP BY\n",
        "        1\n",
        "    LIMIT 5;\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n--- DuckDB Result (you can change the results to Python Dataframe) ---\")\n",
        "print(result.to_df().to_markdown(index=False))"
      ]
    }
  ]
}