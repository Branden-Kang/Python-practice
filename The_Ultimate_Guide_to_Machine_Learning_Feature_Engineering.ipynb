{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUsB9V3tSZyZ3Vla8Ubixu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@simranjeetsingh1497/the-ultimate-guide-to-machine-learning-from-eda-to-model-deployment-part-2-e56ac58785f8)"
      ],
      "metadata": {
        "id": "94tz8eYjL_VB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G6WrBhvlL5wz"
      },
      "outputs": [],
      "source": [
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "# # create a PCA object\n",
        "# pca = PCA(n_components=2)\n",
        "\n",
        "# # fit and transform the data\n",
        "# X_pca = pca.fit_transform(X)\n",
        "\n",
        "# # calculate the explained variance ratio\n",
        "# print(\"Explained variance ratio:\", pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# # create an LDA object\n",
        "# lda = LinearDiscriminantAnalysis(n_components=1)\n",
        "\n",
        "# # fit and transform the data\n",
        "# X_lda = lda.fit_transform(X, y)"
      ],
      "metadata": {
        "id": "x9R-MtHhMCqR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # calculate the correlation matrix\n",
        "# corr_matrix = df.corr()\n",
        "\n",
        "# # select highly correlated features\n",
        "# high_corr = corr_matrix[abs(corr_matrix) > 0.8]\n",
        "\n",
        "# # drop highly correlated features\n",
        "# df = df.drop(high_corr.columns, axis=1)"
      ],
      "metadata": {
        "id": "UutmTcf0MD1I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "model = LinearRegression()\n",
        "rfe = RFE(model, n_features_to_select=5)\n",
        "rfe.fit(X, y)\n",
        "\n",
        "# selected_features = data.feature_names[rfe.support_]\n",
        "# print(selected_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "oy5NyCA-MFxi",
        "outputId": "19d52445-8c89-4e41-ff06-c5d17bb81db4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RFE(estimator=LinearRegression(), n_features_to_select=5)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RFE(estimator=LinearRegression(), n_features_to_select=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RFE</label><div class=\"sk-toggleable__content\"><pre>RFE(estimator=LinearRegression(), n_features_to_select=5)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# # Load the data\n",
        "# X, y = load_data()\n",
        "\n",
        "# # Create a random forest regressor\n",
        "# rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# # Fit the model\n",
        "# rf.fit(X, y)\n",
        "\n",
        "# # Get feature importances\n",
        "# importances = rf.feature_importances_\n",
        "\n",
        "# # Print feature importances\n",
        "# for feature, importance in zip(X.columns, importances):\n",
        "#     print(feature, importance)"
      ],
      "metadata": {
        "id": "Z_64PDGDMJ-e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# load the iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# create an SVM classifier\n",
        "svm = SVC(kernel='linear')\n",
        "\n",
        "# create a feature selector using RFE with SVM\n",
        "selector = RFE(svm, n_features_to_select=2)\n",
        "\n",
        "# fit the selector to the data\n",
        "selector.fit(X, y)\n",
        "\n",
        "# print the selected features\n",
        "print(selector.support_)\n",
        "print(selector.ranking_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNCOYtC5MMmj",
        "outputId": "0b72f451-1f76-426e-e684-7c82a44da623"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False False  True  True]\n",
            "[3 2 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.feature_selection import SequentialFeatureSelector\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# # Load the dataset\n",
        "# X, y = load_dataset()\n",
        "\n",
        "# # Initialize the feature selector\n",
        "# selector = SequentialFeatureSelector(LinearRegression(), n_features_to_select=5, direction='forward')\n",
        "\n",
        "# # Fit the feature selector\n",
        "# selector.fit(X, y)\n",
        "\n",
        "# # Print the selected features\n",
        "# print(selector.support_)"
      ],
      "metadata": {
        "id": "0EiYbRpKMfCR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the dataset\n",
        "X, y = load_dataset()\n",
        "\n",
        "# Initialize the feature selector\n",
        "selector = SequentialFeatureSelector(LinearRegression(), n_features_to_select=5, direction='backward')\n",
        "\n",
        "# Fit the feature selector\n",
        "selector.fit(X, y)\n",
        "\n",
        "# Print the selected features\n",
        "print(selector.support_)"
      ],
      "metadata": {
        "id": "LuySNomVMgQ2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the dataset\n",
        "X, y = load_dataset()\n",
        "\n",
        "# Initialize variables\n",
        "best_score = -float('inf')\n",
        "best_features = None\n",
        "\n",
        "# Loop over all possible subsets of features\n",
        "for k in range(1, len(X.columns) + 1):\n",
        "    for subset in combinations(X.columns, k):\n",
        "        # Train a linear regression model\n",
        "        X_subset = X[list(subset)]\n",
        "        model = LinearRegression().fit(X_subset, y)\n",
        "        # Compute the R2 score\n",
        "        score = r2_score(y, model.predict(X_subset))\n",
        "        # Update the best subset of features\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_features = subset\n",
        "\n",
        "# Print the best subset of features\n",
        "print(best_features)"
      ],
      "metadata": {
        "id": "T62tuYVuMknh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Fit the Lasso model\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X, y)\n",
        "\n",
        "# Get the coefficients\n",
        "coefficients = lasso.coef_"
      ],
      "metadata": {
        "id": "b8hDv0-xMn4N"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Fit the Ridge model\n",
        "ridge = Ridge(alpha=0.1)\n",
        "ridge.fit(X, y)\n",
        "\n",
        "# Get the coefficients\n",
        "coefficients = ridge.coef_"
      ],
      "metadata": {
        "id": "pMdDI-A-Ms7s"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# create a sample dataframe\n",
        "df = pd.DataFrame({\n",
        "   'color': ['red', 'blue', 'green', 'red', 'yellow', 'blue']\n",
        "})\n",
        "\n",
        "# apply one-hot encoding\n",
        "one_hot_encoded = pd.get_dummies(df['color'])\n",
        "print(one_hot_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNK88X_ONBhV",
        "outputId": "98cb69e6-f92e-478e-9fa4-73349cabdfe1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   blue  green  red  yellow\n",
            "0     0      0    1       0\n",
            "1     1      0    0       0\n",
            "2     0      1    0       0\n",
            "3     0      0    1       0\n",
            "4     0      0    0       1\n",
            "5     1      0    0       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# create a sample dataframe\n",
        "df = pd.DataFrame({\n",
        "   'color': ['red', 'blue', 'green', 'red', 'yellow', 'blue']\n",
        "})\n",
        "\n",
        "# apply label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "df['color_encoded'] = label_encoder.fit_transform(df['color'])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jRJEpJ9NEmJ",
        "outputId": "9c29ced2-6a60-478c-e255-e0cfc9c085ce"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    color  color_encoded\n",
            "0     red              2\n",
            "1    blue              0\n",
            "2   green              1\n",
            "3     red              2\n",
            "4  yellow              3\n",
            "5    blue              0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozRQm8yzNIJ1",
        "outputId": "1b4e2295-56d3-422f-bc97-1262730a41f0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.0-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (1.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (0.5.3)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (1.22.4)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (0.13.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.5->category_encoders) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.9/dist-packages (from statsmodels>=0.9.0->category_encoders) (23.0)\n",
            "Installing collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import category_encoders as ce\n",
        "\n",
        "# # create a sample dataframe\n",
        "# df = pd.DataFrame({\n",
        "#    'size': ['S', 'M', 'L', 'XL', 'M', 'S']\n",
        "# })\n",
        "\n",
        "# # apply ordinal encoding\n",
        "# ordinal_encoder = ce.OrdinalEncoder(cols=['size'], order=['S', 'M', 'L', 'XL'])\n",
        "# df = ordinal_encoder.fit_transform(df)\n",
        "# print(df)"
      ],
      "metadata": {
        "id": "ZIN-CKzPNFzb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "DUSU22aSNG2n"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "MuNFfLTyNLR9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Create a RobustScaler object\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "GtwTz9xiNOcY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Create a Normalizer object\n",
        "scaler = Normalizer()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "KfFhC1hFNPyH"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# create a sample data frame\n",
        "data = pd.DataFrame({'age': [25, 30, 35],\n",
        "                     'income': [50000, 60000, 70000]})\n",
        "\n",
        "# create a new interaction feature\n",
        "data['age_income'] = data['age'] * data['income']\n",
        "\n",
        "# display the updated data frame\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldR_6O20NRgA",
        "outputId": "3cb5958c-aeed-4ecd-e5f6-4f32c8bdf0cb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age  income  age_income\n",
            "0   25   50000     1250000\n",
            "1   30   60000     1800000\n",
            "2   35   70000     2450000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import numpy as np\n",
        "\n",
        "# create a sample data set\n",
        "X = np.array([[1, 2],\n",
        "              [3, 4]])\n",
        "\n",
        "# create polynomial features up to degree 2\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# display the updated feature matrix\n",
        "print(X_poly)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJu6inUMNSqO",
        "outputId": "74316102-878a-4621-94c3-321046af9a4e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.  1.  2.  1.  2.  4.]\n",
            " [ 1.  3.  4.  9. 12. 16.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# create a sample data frame\n",
        "data = pd.DataFrame({'age': [20, 25, 30, 35, 40, 45, 50, 55]})\n",
        "\n",
        "# create bins for different age groups\n",
        "bins = [0, 18, 25, 35, 50, float('inf')]\n",
        "labels = ['0-18', '18-25', '25-35', '35-50', '50+']\n",
        "data['age_group'] = pd.cut(data['age'], bins=bins, labels=labels)\n",
        "\n",
        "# display the updated data frame\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP24xaqmNToR",
        "outputId": "ac23c671-aca2-407f-b48f-3937dcc03f8d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age age_group\n",
            "0   20     18-25\n",
            "1   25     18-25\n",
            "2   30     25-35\n",
            "3   35     25-35\n",
            "4   40     35-50\n",
            "5   45     35-50\n",
            "6   50     35-50\n",
            "7   55       50+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.utils import resample\n",
        "\n",
        "# # Upsample minority class\n",
        "# X_upsampled, y_upsampled = resample(X_minority, y_minority, replace=True, n_samples=len(X_majority), random_state=42)"
      ],
      "metadata": {
        "id": "7AJvhdm1NUsT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.utils import resample\n",
        "\n",
        "# # Downsample majority class\n",
        "# X_downsampled, y_downsampled = resample(X_majority, y_majority, replace=False, n_samples=len(X_minority), random_state=42)"
      ],
      "metadata": {
        "id": "IFQEQWimNV3i"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# # Use SMOTE to upsample minority class\n",
        "# sm = SMOTE(random_state=42)\n",
        "# X_resampled, y_resampled = sm.fit_resample(X, y)"
      ],
      "metadata": {
        "id": "XdXWUSfPNYQK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Generate some skewed data\n",
        "data = np.random.gamma(1, 10, 1000)\n",
        "\n",
        "# Calculate skewness and kurtosis\n",
        "skewness = stats.skew(data)\n",
        "kurtosis = stats.kurtosis(data)\n",
        "\n",
        "print(\"Skewness:\", skewness)\n",
        "print(\"Kurtosis:\", kurtosis)\n",
        "\n",
        "# Log transformation\n",
        "log_data = np.log(data)\n",
        "log_skewness = stats.skew(log_data)\n",
        "log_kurtosis = stats.kurtosis(log_data)\n",
        "\n",
        "print(\"Log Skewness:\", log_skewness)\n",
        "print(\"Log Kurtosis:\", log_kurtosis)\n",
        "\n",
        "# Square root transformation\n",
        "sqrt_data = np.sqrt(data)\n",
        "sqrt_skewness = stats.skew(sqrt_data)\n",
        "sqrt_kurtosis = stats.kurtosis(sqrt_data)\n",
        "\n",
        "print(\"Sqrt Skewness:\", sqrt_skewness)\n",
        "print(\"Sqrt Kurtosis:\", sqrt_kurtosis)\n",
        "\n",
        "# Box-Cox transformation\n",
        "box_cox_data, _ = stats.boxcox(data)\n",
        "box_cox_skewness = stats.skew(box_cox_data)\n",
        "box_cox_kurtosis = stats.kurtosis(box_cox_data)\n",
        "\n",
        "print(\"Box-Cox Skewness:\", box_cox_skewness)\n",
        "print(\"Box-Cox Kurtosis:\", box_cox_kurtosis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s4wnv-VNZ0M",
        "outputId": "4e0aae16-8f94-45d8-fcb9-8e6337368b4c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skewness: 1.989347671174275\n",
            "Kurtosis: 5.666687493498708\n",
            "Log Skewness: -1.0357708633174667\n",
            "Log Kurtosis: 1.5345947433540923\n",
            "Sqrt Skewness: 0.6247037340957508\n",
            "Sqrt Kurtosis: 0.2619484560084735\n",
            "Box-Cox Skewness: -0.03923807785481781\n",
            "Box-Cox Kurtosis: -0.26949896454093647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Generate some data with high kurtosis\n",
        "data = np.random.normal(0, 5, 1000)**3\n",
        "\n",
        "# Calculate skewness and kurtosis\n",
        "skewness = stats.skew(data)\n",
        "kurtosis = stats.kurtosis(data)\n",
        "\n",
        "print(\"Skewness:\", skewness)\n",
        "print(\"Kurtosis:\", kurtosis)\n",
        "\n",
        "# Log transformation\n",
        "log_data = np.log(data)\n",
        "log_skewness = stats.skew(log_data)\n",
        "log_kurtosis = stats.kurtosis(log_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4boFka9yNfh4",
        "outputId": "0a8a3a04-e54f-4542-9d50-d809e5439220"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skewness: -0.9386033484128025\n",
            "Kurtosis: 48.20623396227437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-f3a85f4b1985>:16: RuntimeWarning: invalid value encountered in log\n",
            "  log_data = np.log(data)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Import libraries\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from statsmodels.tsa.arima_model import ARIMA\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import LSTM, Dense\n",
        "\n",
        "# # Load time-series data\n",
        "# data = pd.read_csv('time_series_data.csv')\n",
        "\n",
        "# # Preprocess data\n",
        "# data.fillna(method='ffill', inplace=True)\n",
        "# data = data[(data['date'] > '2020-01-01') & (data['date'] < '2021-12-31')]\n",
        "# data.set_index('date', inplace=True)\n",
        "# scaler = StandardScaler()\n",
        "# data = scaler.fit_transform(data)\n",
        "\n",
        "# # Extract features\n",
        "# rolling_mean = data.rolling(window=7).mean()\n",
        "# fft = np.fft.fft(data)\n",
        "# wavelet = pywt.dwt(data, 'db1')\n",
        "\n",
        "# # Train ARIMA model\n",
        "# model = ARIMA(data, order=(1, 1, 1))\n",
        "# model_fit = model.fit(disp=0)\n",
        "# predictions = model_fit.predict(start='2022-01-01', end='2022-12-31')\n",
        "\n",
        "# # Train LSTM model\n",
        "# X_train, y_train = [], []\n",
        "# for i in range(7, len(data)):\n",
        "#     X_train.append(data[i-7:i, 0])\n",
        "#     y_train.append(data[i, 0])\n",
        "# X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "# X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "\n",
        "# # Define the LSTM model\n",
        "# model = Sequential()\n",
        "# model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(LSTM(units=50, return_sequences=True))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(LSTM(units=50))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Dense(units=1))\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# # Fit the model to the training data\n",
        "# model.fit(X_train, y_train, epochs=100, batch_size=32)"
      ],
      "metadata": {
        "id": "c6ESp4fjNhw8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"This is a sample sentence. It contains some words.\"\n",
        "words = word_tokenize(text)\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(words)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pFL9ua3NmAg",
        "outputId": "3cd7b379-b3a1-4269-c887-e750ee8e7ce1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', '.', 'It', 'contains', 'some', 'words', '.']\n",
            "['This is a sample sentence.', 'It contains some words.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.casefold() not in stop_words]\n",
        "\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84aVPFH8NoMR",
        "outputId": "0de1f43a-6bc8-4f37-c3d3-b99b320b95b9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'sentence', '.', 'contains', 'words', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "print(stemmed_words)\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CD2zbwA1Nx2_",
        "outputId": "4bfd7146-3c24-413f-d8e3-18ed4a21adde"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sampl', 'sentenc', '.', 'contain', 'word', '.']\n",
            "['sample', 'sentence', '.', 'contains', 'word', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\b(can\\'t|won\\'t|shouldn\\'t)\\b', 'not', text)\n",
        "    text = re.sub(r'\\b(i\\'m|you\\'re|he\\'s|she\\'s|it\\'s|we\\'re|they\\'re)\\b', 'be', text)\n",
        "    return text\n",
        "\n",
        "text = \"I can't believe it's not butter!\"\n",
        "normalized_text = normalize_text(text)\n",
        "\n",
        "print(normalized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlbRLkxON2Dd",
        "outputId": "048885ba-cec4-401b-ac8a-7e5552686a87"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i cant believe its not butter\n"
          ]
        }
      ]
    }
  ]
}