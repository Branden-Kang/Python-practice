{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpTxLWAdaaqCEKZ4+JCLax"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@felixpratama242/etl-using-python-postgresql-and-docker-8724d1efbc97)"
      ],
      "metadata": {
        "id": "XvbGvTNaqwjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import pandas as pd\n",
        "\n",
        "#read dataset:\n",
        "def extract_data(file):\n",
        "    data = pd.read_csv(file)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "QPChqZvlqwrT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rLA72htqqpak"
      },
      "outputs": [],
      "source": [
        "# from extract import extract_data\n",
        "import pandas as pd\n",
        "\n",
        "def transform_data(data):\n",
        "    # remove unnecessary column\n",
        "    data.drop(data.iloc[:, 2:44], inplace=True, axis=1)\n",
        "\n",
        "    # remove missing values:\n",
        "    data_clean = data.dropna()\n",
        "\n",
        "    #round consumption values to have only 2 number after comma\n",
        "    data_column_name = data_clean.columns[-1]\n",
        "    consumption_data = round(data_clean[data_column_name], 2)\n",
        "\n",
        "    #create new dictionary for new last column\n",
        "    new_consumption_data = {\n",
        "        data_column_name : consumption_data\n",
        "    }\n",
        "\n",
        "    #change the new dictionary into new dataframe\n",
        "    new_consumption_dataframe = pd.DataFrame(new_consumption_data)\n",
        "    #drop the old last column in dataframe\n",
        "    data_clean= data_clean.drop([data_column_name], axis = 1)\n",
        "\n",
        "    #join new last column that has been transformed into dataframe\n",
        "    data_new = data_clean.join(new_consumption_dataframe)\n",
        "\n",
        "    return data_new"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from extract import extract_data\n",
        "# from transform import transform_data\n",
        "import psycopg2\n",
        "import argparse\n",
        "\n",
        "def load_data(file_path, database, host, user, password, port):\n",
        "\n",
        "    connection = psycopg2.connect(database=database,\n",
        "                        host=host,\n",
        "                        user=user,\n",
        "                        password=password,\n",
        "                        port=port)\n",
        "\n",
        "    cursor = connection.cursor()\n",
        "\n",
        "    print(\"loading data...\")\n",
        "    data = extract_data(file_path)\n",
        "\n",
        "    print(\"transforming data...\")\n",
        "    data_transform = transform_data(data)\n",
        "\n",
        "    column_name = data_transform.columns[-1]\n",
        "\n",
        "    #create table\n",
        "    query_create_table = f\"CREATE TABLE IF NOT EXISTS {column_name}(\\\n",
        "    ID SERIAL PRIMARY KEY,\\\n",
        "    continent varchar(50) NOT NULL,\\\n",
        "    country varchar(50) NOT NULL,\\\n",
        "    {column_name} decimal\\\n",
        "    );\"\n",
        "\n",
        "    cursor.execute(query_create_table)\n",
        "\n",
        "    #start loading data\n",
        "    print('loading data...')\n",
        "    for index, row in data_transform.iterrows():\n",
        "        query_insert_value = f\"INSERT INTO {column_name} (continent, country, {column_name}) VALUES ('{row[0]}', \\\n",
        "            '{row[1]}', {row[2]})\"\n",
        "\n",
        "        cursor.execute(query_insert_value)\n",
        "    connection.commit()\n",
        "\n",
        "    cursor.close()\n",
        "    connection.close()\n",
        "\n",
        "    print(\"etl success...\\n\")\n",
        "\n",
        "    return \"all processes completed\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Initialize parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Adding optional argument\n",
        "    parser.add_argument(\"-f\", \"--file\", help = \"file path of your dataset\")\n",
        "    parser.add_argument(\"-db\", \"--database\", help = \"database name\")\n",
        "    parser.add_argument(\"-hs\", \"--host\", help = \"your postgresql host\")\n",
        "    parser.add_argument(\"-u\", \"--user\", help = \"postgresql username\")\n",
        "    parser.add_argument(\"-pass\", \"--password\", help = \"postgresql password\")\n",
        "    parser.add_argument(\"-p\", \"--port\", help = \"postgresql port\")\n",
        "\n",
        "    # Read arguments from command line\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    load_data(args.file, args.database, args.host, args.user, args.password, args.port)"
      ],
      "metadata": {
        "id": "u1maLSMsq7U1"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}