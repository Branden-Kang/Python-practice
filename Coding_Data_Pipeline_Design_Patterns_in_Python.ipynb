{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQnooJ1uFZeFj26fPu7LDR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://amsayed.medium.com/coding-data-pipeline-design-patterns-in-python-44a705f0af9e)"
      ],
      "metadata": {
        "id": "sOpVgmiFeGoE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yi1L1Wp2b_Va"
      },
      "outputs": [],
      "source": [
        "class DataPipelineFacade:\n",
        "    def __init__(self, db_client, api_client, file_system):\n",
        "        self.db_client = db_client\n",
        "        self.api_client = api_client\n",
        "        self.file_system = file_system\n",
        "\n",
        "    def extract_transform_load(self, source_type, source_config, transformations):\n",
        "        if source_type == 'database':\n",
        "            data = self.db_client.extract_data(**source_config)\n",
        "        elif source_type == 'api':\n",
        "            data = self.api_client.get_data(**source_config)\n",
        "        elif source_type == 'file':\n",
        "            data = self.file_system.load_csv(**source_config)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid source type\")\n",
        "\n",
        "        transformed_data = self.apply_transformations(data, transformations)\n",
        "        self.db_client.load_data(transformed_data)\n",
        "\n",
        "    def apply_transformations(self, data, transformations):\n",
        "        # ... Logic to apply transformations\n",
        "        return transformed_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class XMLAdapter:\n",
        "    def __init__(self, xml_data):\n",
        "        self.xml_data = xml_data\n",
        "        # Use library like xmltodict to parse if needed\n",
        "\n",
        "    def get_data(self):\n",
        "        # Adapt XML structure to the format your pipeline expects\n",
        "        ...\n",
        "        return adapted_data"
      ],
      "metadata": {
        "id": "ZW_Gtxu0eLXU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logging_decorator(func):\n",
        "    def inner(*args, **kwargs):\n",
        "        print(f\"Calling function: {func.__name__}\")\n",
        "        result = func(*args, **kwargs)\n",
        "        print(f\"Function completed.\")\n",
        "        return result\n",
        "    return inner\n",
        "\n",
        "@logging_decorator\n",
        "def processing_step(data):\n",
        "    # ...processing logic\n",
        "    return processed_data"
      ],
      "metadata": {
        "id": "21LeY90GeNUY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logging_decorator(func):\n",
        "    def inner(*args, **kwargs):\n",
        "        print(f\"Calling function: {func.__name__}\")\n",
        "        result = func(*args, **kwargs)\n",
        "        print(f\"Function completed.\")\n",
        "        return result\n",
        "    return inner\n",
        "\n",
        "@logging_decorator\n",
        "def processing_step(data):\n",
        "    # ...processing logic\n",
        "    return processed_data"
      ],
      "metadata": {
        "id": "Srmq0YlreO7M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CSVFileIterator:\n",
        "    def __init__(self, filename):\n",
        "        self.file = open(filename)\n",
        "        self.reader = csv.reader(self.file)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        row = next(self.reader)\n",
        "        if not row:\n",
        "            raise StopIteration\n",
        "        return row\n",
        "\n",
        "# Usage\n",
        "for row in CSVFileIterator('large_data.csv'):\n",
        "    # Process each row individually"
      ],
      "metadata": {
        "id": "G_PyL1EaeQoq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProcessingStep:\n",
        "    def __init__(self, successor=None):\n",
        "        self.successor = successor\n",
        "\n",
        "    def handle(self, data):\n",
        "        if self.can_handle(data):\n",
        "            return self.process(data)\n",
        "        elif self.successor:\n",
        "            return self.successor.handle(data)\n",
        "        else:\n",
        "            raise Exception(\"No suitable handler found\")\n",
        "\n",
        "    def can_handle(self, data):\n",
        "        #  Logic to determine if this step can process data\n",
        "        ...\n",
        "\n",
        "    def process(self, data):\n",
        "        #  Actual processing logic\n",
        "        ..."
      ],
      "metadata": {
        "id": "noBZshD-eR1X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataCleaningStrategy:\n",
        "    def clean_data(self, data):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "class BasicCleaning(DataCleaningStrategy):\n",
        "    def clean_data(self, data):\n",
        "        # ... Basic cleaning logic\n",
        "        return data\n",
        "\n",
        "class AdvancedCleaning(DataCleaningStrategy):\n",
        "    def clean_data(self, data):\n",
        "        # ...  Complex cleaning logic\n",
        "        return data\n",
        "\n",
        "class DataPipeline:\n",
        "    def __init__(self, cleaning_strategy):\n",
        "        self.cleaning_strategy = cleaning_strategy\n",
        "\n",
        "    def process_data(self, data):\n",
        "        cleaned_data = self.cleaning_strategy.clean_data(data)\n",
        "        # ... Further processing"
      ],
      "metadata": {
        "id": "rBgVUILQeTau"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}