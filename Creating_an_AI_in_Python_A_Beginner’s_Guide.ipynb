{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhqfZj7lDhPt9usTeZ0Kck"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@etirismagazine/creating-an-ai-in-python-a-beginners-guide-9038a84e1fca)"
      ],
      "metadata": {
        "id": "wLhApFgDFWdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Natural Language Processing (NLP):"
      ],
      "metadata": {
        "id": "RP55Lkl-GQ8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HGFVZgIFU_j",
        "outputId": "7b927153-9fc1-49a0-89b6-605fdba2d640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sia = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "PrlbTbJJGTy-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentences = [\n",
        "    \"I love this movie, it's amazing!\",\n",
        "    \"This movie is terrible, I hated it.\",\n",
        "    \"The acting was great, but the plot was confusing.\",\n",
        "    \"I didn't really like this movie, it was just okay.\",\n",
        "    \"The special effects were impressive, but the story was weak.\"\n",
        "]\n",
        "\n",
        "for sentence in example_sentences:\n",
        "    sentiment = sia.polarity_scores(sentence)\n",
        "    print(sentence)\n",
        "    print(sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXgAxiqgGUbN",
        "outputId": "6d4ecad4-e760-4142-a24f-919a9c7b5599"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love this movie, it's amazing!\n",
            "{'neg': 0.0, 'neu': 0.266, 'pos': 0.734, 'compound': 0.8516}\n",
            "This movie is terrible, I hated it.\n",
            "{'neg': 0.646, 'neu': 0.354, 'pos': 0.0, 'compound': -0.8074}\n",
            "The acting was great, but the plot was confusing.\n",
            "{'neg': 0.197, 'neu': 0.588, 'pos': 0.214, 'compound': 0.0516}\n",
            "I didn't really like this movie, it was just okay.\n",
            "{'neg': 0.207, 'neu': 0.624, 'pos': 0.169, 'compound': -0.1095}\n",
            "The special effects were impressive, but the story was weak.\n",
            "{'neg': 0.259, 'neu': 0.471, 'pos': 0.269, 'compound': -0.2144}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#     2. Recommendation Systems:"
      ],
      "metadata": {
        "id": "Tg81uJ3DGXNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the movie data\n",
        "movies = pd.read_csv('movies.csv')\n",
        "\n",
        "# Load the user ratings data\n",
        "ratings = pd.read_csv('ratings.csv')\n",
        "\n",
        "# Merge the movie and ratings data\n",
        "movie_ratings = pd.merge(movies, ratings, on='movieId')\n",
        "\n",
        "# Calculate the average rating for each movie\n",
        "movie_ratings_grouped = movie_ratings.groupby('title')['rating'].mean().reset_index()\n",
        "\n",
        "# Create a matrix of user ratings for each movie\n",
        "movie_ratings_matrix = movie_ratings.pivot_table(index='userId', columns='title', values='rating')\n",
        "\n",
        "# Create a matrix of user ratings for each movie\n",
        "movie_ratings_matrix = movie_ratings.pivot_table(index='userId', columns='title', values='rating')\n",
        "\n",
        "# Find movies similar to a given movie\n",
        "similar_movies = movie_ratings_matrix.corrwith(movie_ratings_matrix['Toy Story (1995)'])\n",
        "similar_movies = similar_movies.dropna()\n",
        "similar_movies_df = pd.DataFrame(similar_movies, columns=['correlation'])\n",
        "similar_movies_df = similar_movies_df.sort_values('correlation', ascending=False)\n",
        "\n",
        "# Suggest movies based on user preferences\n",
        "user_ratings = movie_ratings_matrix.loc[1].dropna()\n",
        "user_ratings_df = pd.DataFrame(user_ratings, columns=['rating'])\n",
        "user_ratings_df = user_ratings_df.join(similar_movies_df)\n",
        "suggested_movies = user_ratings_df.sort_values('correlation', ascending=False).head(10)"
      ],
      "metadata": {
        "id": "04-AxxgcGYFC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Image Recognition:"
      ],
      "metadata": {
        "id": "Pu7f_6eNGrVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "# Load the image data\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Preprocess the image data\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Define the CNN model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(train_images, train_labels, epochs=10, \n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "qf0QTI2tGVfI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#     4. Fraud Detection:"
      ],
      "metadata": {
        "id": "1jDvCYqoG0JL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the transaction data\n",
        "data = pd.read_csv('transactions.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "data = pd.get_dummies(data, columns=['location'])\n",
        "X = data.drop('is_fraud', axis=1)\n",
        "y = data['is_fraud']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Define the machine learning model\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "upGAwipDGzC1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Healthcare:"
      ],
      "metadata": {
        "id": "9H3sPGQXG-_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load the image data\n",
        "train_dir = 'train_images/'\n",
        "train_df = pd.read_csv('train_labels.csv')\n",
        "\n",
        "# Preprocess the image data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    directory=train_dir,\n",
        "    x_col='id_code',\n",
        "    y_col='diagnosis',\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    directory=train_dir,\n",
        "    x_col='id_code',\n",
        "    y_col='diagnosis',\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Define the AI model\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(256,256,3)),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile and fit the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
        "\n",
        "# Evaluate the model\n",
        "test_dir = 'test_images/'\n",
        "test_df = pd.read_csv('test_labels.csv')\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    test_df,\n",
        "    directory=test_dir,\n",
        "    x_col='id_code',\n",
        "    y_col=None,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode=None,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "predictions = model.predict(test_generator)"
      ],
      "metadata": {
        "id": "lUmdxSXfG8GL"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}