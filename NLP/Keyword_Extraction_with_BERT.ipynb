{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keyword Extraction with BERT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNFaNOs04kec0IQ8rRC8IlY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nh42TJDfhRI"
      },
      "source": [
        "[Reference](https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V30Ofx1FX9EV"
      },
      "source": [
        "# 1. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CWznOSGfEA0"
      },
      "source": [
        "doc = \"\"\"\n",
        "         Supervised learning is the machine learning task of \n",
        "         learning a function that maps an input to an output based \n",
        "         on example input-output pairs.[1] It infers a function \n",
        "         from labeled training data consisting of a set of \n",
        "         training examples.[2] In supervised learning, each \n",
        "         example is a pair consisting of an input object \n",
        "         (typically a vector) and a desired output value (also \n",
        "         called the supervisory signal). A supervised learning \n",
        "         algorithm analyzes the training data and produces an \n",
        "         inferred function, which can be used for mapping new \n",
        "         examples. An optimal scenario will allow for the algorithm \n",
        "         to correctly determine the class labels for unseen \n",
        "         instances. This requires the learning algorithm to  \n",
        "         generalize from the training data to unseen situations \n",
        "         in a 'reasonable' way (see inductive bias).\n",
        "      \"\"\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxvR_hoqX_u1"
      },
      "source": [
        "# 2. Candidate Keywords/Keyphrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z4se_3yWLBV"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "n_gram_range = (3, 3)\n",
        "stop_words = \"english\"\n",
        "\n",
        "# Extract candidate words/phrases\n",
        "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
        "candidates = count.get_feature_names()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uaSYpYKWLrO",
        "outputId": "ff8ce4ec-9c28-4962-d071-25f20329014b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "candidates"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['algorithm analyzes training',\n",
              " 'algorithm correctly determine',\n",
              " 'algorithm generalize training',\n",
              " 'allow algorithm correctly',\n",
              " 'analyzes training data',\n",
              " 'based example input',\n",
              " 'called supervisory signal',\n",
              " 'class labels unseen',\n",
              " 'consisting input object',\n",
              " 'consisting set training',\n",
              " 'correctly determine class',\n",
              " 'data consisting set',\n",
              " 'data produces inferred',\n",
              " 'data unseen situations',\n",
              " 'desired output value',\n",
              " 'determine class labels',\n",
              " 'example input output',\n",
              " 'example pair consisting',\n",
              " 'examples optimal scenario',\n",
              " 'examples supervised learning',\n",
              " 'function labeled training',\n",
              " 'function maps input',\n",
              " 'function used mapping',\n",
              " 'generalize training data',\n",
              " 'inferred function used',\n",
              " 'infers function labeled',\n",
              " 'input object typically',\n",
              " 'input output based',\n",
              " 'input output pairs',\n",
              " 'instances requires learning',\n",
              " 'labeled training data',\n",
              " 'labels unseen instances',\n",
              " 'learning algorithm analyzes',\n",
              " 'learning algorithm generalize',\n",
              " 'learning example pair',\n",
              " 'learning function maps',\n",
              " 'learning machine learning',\n",
              " 'learning task learning',\n",
              " 'machine learning task',\n",
              " 'mapping new examples',\n",
              " 'maps input output',\n",
              " 'new examples optimal',\n",
              " 'object typically vector',\n",
              " 'optimal scenario allow',\n",
              " 'output based example',\n",
              " 'output pairs infers',\n",
              " 'output value called',\n",
              " 'pair consisting input',\n",
              " 'pairs infers function',\n",
              " 'produces inferred function',\n",
              " 'reasonable way inductive',\n",
              " 'requires learning algorithm',\n",
              " 'scenario allow algorithm',\n",
              " 'set training examples',\n",
              " 'signal supervised learning',\n",
              " 'situations reasonable way',\n",
              " 'supervised learning algorithm',\n",
              " 'supervised learning example',\n",
              " 'supervised learning machine',\n",
              " 'supervisory signal supervised',\n",
              " 'task learning function',\n",
              " 'training data consisting',\n",
              " 'training data produces',\n",
              " 'training data unseen',\n",
              " 'training examples supervised',\n",
              " 'typically vector desired',\n",
              " 'unseen instances requires',\n",
              " 'unseen situations reasonable',\n",
              " 'used mapping new',\n",
              " 'value called supervisory',\n",
              " 'vector desired output',\n",
              " 'way inductive bias']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJER9A-eYJGi"
      },
      "source": [
        "# 3. Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eVZlTvzWM-2"
      },
      "source": [
        "# !pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "doc_embedding = model.encode([doc])\n",
        "candidate_embeddings = model.encode(candidates)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WjNMc_2YZwH"
      },
      "source": [
        "# 4. Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfsj8OC8YOpN"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "top_n = 5\n",
        "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMY8mgHld-WP",
        "outputId": "62164c7c-8457-4e39-fce9-38578c28de69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "keywords"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['learning algorithm generalize',\n",
              " 'algorithm analyzes training',\n",
              " 'learning algorithm analyzes',\n",
              " 'learning machine learning',\n",
              " 'algorithm generalize training']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie5yU9NmeF81"
      },
      "source": [
        "# 5. Diversification\n",
        "\n",
        "- Max Sum Similarity\n",
        "- Maximal Marginal Relevance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCaPpAE_eA6f"
      },
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "def max_sum_sim(doc_embedding, word_embeddings, words, top_n, nr_candidates):\n",
        "    # Calculate distances and extract keywords\n",
        "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
        "                                            candidate_embeddings)\n",
        "\n",
        "    # Get top_n words as candidates based on cosine similarity\n",
        "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
        "    words_vals = [candidates[index] for index in words_idx]\n",
        "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
        "\n",
        "    # Calculate the combination of words that are the least similar to each other\n",
        "    min_sim = np.inf\n",
        "    candidate = None\n",
        "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
        "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
        "        if sim < min_sim:\n",
        "            candidate = combination\n",
        "            min_sim = sim\n",
        "\n",
        "    return [words_vals[idx] for idx in candidate]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFhQrWkgej2c",
        "outputId": "ee54c55c-9963-4f8b-d217-f4686c9922d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates = 10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['signal supervised learning',\n",
              " 'requires learning algorithm',\n",
              " 'learning function maps',\n",
              " 'algorithm analyzes training',\n",
              " 'learning machine learning']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRFovKs8er5V"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def mmr(doc_embedding, word_embeddings, words, top_n, diversity):\n",
        "\n",
        "    # Extract similarity within words, and between words and the document\n",
        "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
        "    word_similarity = cosine_similarity(word_embeddings)\n",
        "\n",
        "    # Initialize candidates and already choose best keyword/keyphras\n",
        "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
        "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
        "\n",
        "    for _ in range(top_n - 1):\n",
        "        # Extract similarities within candidates and\n",
        "        # between candidates and selected keywords/phrases\n",
        "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
        "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
        "\n",
        "        # Calculate MMR\n",
        "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
        "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
        "\n",
        "        # Update keywords & candidates\n",
        "        keywords_idx.append(mmr_idx)\n",
        "        candidates_idx.remove(mmr_idx)\n",
        "\n",
        "    return [words[idx] for idx in keywords_idx]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltBIVLb9fA_j",
        "outputId": "8e6b0aa1-12dc-4cd8-9605-1435c53541ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity = 0.2)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['algorithm generalize training',\n",
              " 'learning machine learning',\n",
              " 'learning algorithm analyzes',\n",
              " 'supervised learning algorithm',\n",
              " 'algorithm analyzes training']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}