{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Build a URL Text Summarizer with NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNHfi6uhrpflJfE+fli3Uj5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0nAX8ZlSYYA"
      },
      "source": [
        "[Reference](https://medium.com/better-programming/how-to-build-a-url-text-summarizer-with-simple-natural-language-processing-ac1a9cb742de)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mznvu0ZSS9Fe",
        "outputId": "480ea9a0-85a9-4e20-92fc-6e8a418d8d50"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import sys\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csnBBk6MTHTa"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "from string import punctuation\n",
        "from heapq import nlargest\n",
        "\n",
        "class FrequencySummarizer:\n",
        "  def __init__(self, min_cut=0.1, max_cut=0.9):\n",
        "    \"\"\"\n",
        "     Initialize the text summarizer.\n",
        "     Words that have a frequency term lower than min_cut\n",
        "     or higher than max_cut will be ignored.\n",
        "    \"\"\"\n",
        "    self._min_cut = min_cut\n",
        "    self._max_cut = max_cut\n",
        "    self._stopwords = set(stopwords.words('english') + list(punctuation))\n",
        "\n",
        "  def _compute_frequencies(self, word_sent):\n",
        "    \"\"\"\n",
        "      Compute the frequency of each of word.\n",
        "      Input:\n",
        "       word_sent, a list of sentences already tokenized.\n",
        "      Output:\n",
        "       freq, a dictionary where freq[w] is the frequency of w.\n",
        "    \"\"\"\n",
        "    freq = defaultdict(int)\n",
        "    for s in word_sent:\n",
        "      for word in s:\n",
        "        if word not in self._stopwords:\n",
        "          freq[word] += 1\n",
        "    # frequencies normalization and filtering\n",
        "    m = float(max(freq.values()))\n",
        "    for w in list(freq):\n",
        "      freq[w] = freq[w]/m\n",
        "      if freq[w] >= self._max_cut or freq[w] <= self._min_cut:\n",
        "        del freq[w]\n",
        "    return freq\n",
        "\n",
        "  def summarize(self, text, n):\n",
        "    \"\"\"\n",
        "      Return a list of n sentences\n",
        "      which represent the summary of text.\n",
        "    \"\"\"\n",
        "    sents = sent_tokenize(text)\n",
        "    assert n <= len(sents)\n",
        "    word_sent = [word_tokenize(s.lower()) for s in sents]\n",
        "    self._freq = self._compute_frequencies(word_sent)\n",
        "    ranking = defaultdict(int)\n",
        "    for i,sent in enumerate(word_sent):\n",
        "      for w in sent:\n",
        "        if w in self._freq:\n",
        "          ranking[i] += self._freq[w]\n",
        "    sents_idx = self._rank(ranking, n)\n",
        "    return [sents[j] for j in sents_idx]\n",
        "\n",
        "  def _rank(self, ranking, n):\n",
        "    \"\"\" return the first n sentences with highest ranking \"\"\"\n",
        "    return nlargest(n, ranking, key=ranking.get)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGq9n_Y-SUK9"
      },
      "source": [
        "def getTextFromURL(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "    text = ' '.join(map(lambda p: p.text, soup.find_all('p')))\n",
        "    return text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUzWK4vMSxTB"
      },
      "source": [
        "def summarizeURL(url, total_pars):\n",
        "\turl_text = getTextFromURL(url).replace(u\"Â\", u\"\").replace(u\"â\", u\"\")\n",
        "\n",
        "\tfs = FrequencySummarizer()\n",
        "\tfinal_summary = fs.summarize(url_text.replace(\"\\n\",\" \"), total_pars)\n",
        "\treturn \" \".join(final_summary)\n",
        "\n",
        "url = str(input(\"Enter an article URL\\n\")) if len(sys.argv) < 1 else sys.argv[1]\n",
        "final_summary = summarizeURL(url, 5)\n",
        "print(final_summary)"
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}