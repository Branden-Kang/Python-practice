{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjEJ9q8SdLZWSJWEoXemL5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@patricklowe33/etl-using-docker-python-postgres-airflow-ed3e9508bd2e)"
      ],
      "metadata": {
        "id": "DHqnZIucJO_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 — Prerequisites\n",
        "- Docker\n",
        "- Python\n",
        "- PostgreSQL\n",
        "- Apache Airflow\n",
        "- Airlabs"
      ],
      "metadata": {
        "id": "7BYMajjcJfOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 — Setup\n",
        "## Dockerfile\n",
        "```\n",
        "FROM python:3.6.1-alpine\n",
        "\n",
        "RUN apk update \\\n",
        "  && apk add \\\n",
        "    build-base \\\n",
        "    postgresql \\\n",
        "    postgresql-dev \\\n",
        "    libpq\n",
        "\n",
        "RUN mkdir /app\n",
        "WORKDIR /app\n",
        "COPY ./requirements.txt .\n",
        "RUN pip install -r requirements.txt\n",
        "\n",
        "ENV PYTHONUNBUFFERED 1\n",
        "\n",
        "COPY . .\n",
        "```"
      ],
      "metadata": {
        "id": "OtdxmqPSJmH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## requirements.txt\n",
        "```\n",
        "psycopg2==2.7.2\n",
        "```"
      ],
      "metadata": {
        "id": "3ycxKiVHJz2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compose.yml\n",
        "```\n",
        "version: '3.4'\n",
        "\n",
        "x-common:\n",
        "  &common\n",
        "  image: apache/airflow:2.3.0\n",
        "  user: \"${AIRFLOW_UID}:0\"\n",
        "  env_file:\n",
        "    - .env\n",
        "  volumes:\n",
        "    - ./dags:/opt/airflow/dags\n",
        "    - ./logs:/opt/airflow/logs\n",
        "    - ./plugins:/opt/airflow/plugins\n",
        "    - /var/run/docker.sock:/var/run/docker.sock\n",
        "\n",
        "x-depends-on:\n",
        "  &depends-on\n",
        "  depends_on:\n",
        "    postgres:\n",
        "      condition: service_healthy\n",
        "    airflow-init:\n",
        "      condition: service_completed_successfully\n",
        "\n",
        "services:\n",
        "  postgres:\n",
        "    image: postgres:13\n",
        "    container_name: postgres\n",
        "    healthcheck:\n",
        "      test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]\n",
        "      interval: 5s\n",
        "      retries: 5\n",
        "    env_file:\n",
        "      - .env\n",
        "    environment:\n",
        "      POSTGRES_PORT: ${POSTGRES_PORT:-5432}\n",
        "# UPDATE your host here, for local machines (not docker) \"localhost\" worked for me\n",
        "      POSTGRES_HOST: ${POSTGRES_HOST:-postgres}\n",
        "    volumes:\n",
        "      - local_postgres_data:/var/lib/postgresql/data\n",
        "    ports:\n",
        "      - 5432:5432\n",
        "\n",
        "  scheduler:\n",
        "    <<: [*common, *depends-on]\n",
        "    container_name: airflow-scheduler\n",
        "    command: scheduler\n",
        "    restart: on-failure\n",
        "    ports:\n",
        "      - \"8793:8793\"\n",
        "\n",
        "  webserver:\n",
        "    <<: [*common, *depends-on]\n",
        "    container_name: airflow-webserver\n",
        "    restart: always\n",
        "    command: webserver\n",
        "    ports:\n",
        "      - \"8080:8080\"\n",
        "    healthcheck:\n",
        "      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]\n",
        "      interval: 30s\n",
        "      timeout: 30s\n",
        "      retries: 5\n",
        "  \n",
        "  airflow-init:\n",
        "    <<: *common\n",
        "    container_name: airflow-init\n",
        "    entrypoint: /bin/bash\n",
        "    command:\n",
        "      - -c\n",
        "      - |\n",
        "        mkdir -p /sources/logs /sources/dags /sources/plugins\n",
        "        chown -R \"${AIRFLOW_UID}:0\" /sources/{logs,dags,plugins}\n",
        "        exec /entrypoint airflow version\n",
        "volumes:\n",
        "    local_postgres_data:\n",
        "```\n"
      ],
      "metadata": {
        "id": "_PetZ_NpJ3Sz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .env\n",
        "```\n",
        "# Database\n",
        "POSTGRES_USER=postgres\n",
        "POSTGRES_PASSWORD=YOUR_PASSWORD\n",
        "POSTGRES_DB=ryanair_API\n",
        "\n",
        "# Backend DB - update your username and password\n",
        "AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://YOUR_USERNAME:YOUR_PASSWORD@postgres/YOUR_DATABASE\n",
        "AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS=False\n",
        "\n",
        "# Airflow Init\n",
        "_AIRFLOW_DB_UPGRADE=True\n",
        "_AIRFLOW_WWW_USER_CREATE=True\n",
        "_AIRFLOW_WWW_USER_USERNAME=postgres\n",
        "_AIRFLOW_WWW_USER_PASSWORD=YOUR_PASSWORD\n",
        "```"
      ],
      "metadata": {
        "id": "vYBEtYs0KBrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 —DAG Script for Airflow"
      ],
      "metadata": {
        "id": "lR4s_X3NKFU6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zAZzh1t6FQ2_"
      },
      "outputs": [],
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.dummy_operator import DummyOperator\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import datetime,timedelta\n",
        "\n",
        "default_args = {\n",
        "   'owner': 'airflow',\n",
        "   'depends_on_past': False,\n",
        "   'retries': 0\n",
        "}\n",
        "\n",
        "dag=DAG(\n",
        "    dag_id='ryanair_DAG',\n",
        "    default_args=default_args,\n",
        "    start_date=datetime(2024,1,9),\n",
        "    catchup=False,\n",
        "    schedule_interval='*/3 * * * *', #every 3 minutes\n",
        "    )\n",
        "\n",
        "t1 = BashOperator(\n",
        "    task_id = 'Bash_task',\n",
        "    bash_command = 'python $AIRFLOW_HOME/dags/scripts/RyanAir_ETL.py',\n",
        "    dag = dag\n",
        "    )\n",
        "\n",
        "t1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 — Python Script"
      ],
      "metadata": {
        "id": "Jm3LXCv9KL4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import psycopg2\n",
        "import psycopg2.extras as extras\n",
        "\n",
        "\n",
        "def extract(api_key):\n",
        "    # ## Extract\n",
        "    # Gather data from airlabs API about RyanAir flights\n",
        "    # https://airlabs.co/account\n",
        "    fields = \"&_fields=flight_iata,dep_iata,dep_time_utc,dep_estimated_utc,dep_actual_utc,arr_iata,arr_time_utc,arr_estimated_utc,status,duration,delayed,dep_delayed,arr_delayed\"\n",
        "    method = 'ping'\n",
        "    params = {'api_key': api_key}\n",
        "    # Flights based on Airline, FR is IATA code for RyanAir\n",
        "    schedules_api = 'https://airlabs.co/api/v9/schedules?airline_iata=FR'\n",
        "    print(\"Extracting...\")\n",
        "    schedule_data = pd.json_normalize(requests.get(schedules_api+fields+method, params).json(), record_path=['response'])\n",
        "    return schedule_data\n",
        "\n",
        "# ## Transform\n",
        "# Clean data for uploading\n",
        "import os\n",
        "#print(\"Current Directory:\" + str(CUR_DIR) + \"/dags/scripts/airport_codes.csv\")\n",
        "def convert_iata(df):\n",
        "    CUR_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "    airport_codes = pd.read_csv(str(CUR_DIR)+'/airport_codes.csv')\n",
        "    df = df.merge(\n",
        "        airport_codes[['dep_iata','airport_name']],\n",
        "        on='dep_iata',\n",
        "        how=\"left\")\n",
        "    df.rename(\n",
        "        columns={\"airport_name\": \"departure_airport\"},\n",
        "        inplace=True)\n",
        "    df = df.drop(columns=['dep_iata'])\n",
        "    df = df.merge(\n",
        "        airport_codes[['arr_iata','airport_name']],\n",
        "        on='arr_iata',\n",
        "        how=\"left\")\n",
        "    df.rename(\n",
        "        columns={\"airport_name\": \"arrival_airport\"},\n",
        "        inplace=True)\n",
        "    df = df.drop( columns=['arr_iata'])\n",
        "    return df\n",
        "\n",
        "def convert_timestamp(df):\n",
        "    cols = ['dep_time_utc','dep_estimated_utc','dep_actual_utc','arr_time_utc','arr_estimated_utc']\n",
        "    df[cols] = df[cols].apply(pd.to_datetime)\n",
        "    # departure date/time in GMT\n",
        "    df['dep_date'], df['dep_time'] = df['dep_time_utc'].dt.normalize(), df['dep_time_utc'].dt.time\n",
        "    # updated departure date/time in GMT\n",
        "    df['dep_date_upd'], df['dep_time_upd'] = df['dep_estimated_utc'].dt.normalize(), df['dep_estimated_utc'].dt.time\n",
        "    df['dep_date_upd'].fillna(df['dep_date'], inplace=True)\n",
        "    df['dep_time_upd'].fillna(df['dep_time'], inplace=True)\n",
        "\n",
        "    # actual departure date/time in GMT\n",
        "    df['dep_date_act'], df['dep_time_act'] = df['dep_actual_utc'].dt.normalize(), df['dep_actual_utc'].dt.time\n",
        "    df['dep_date_act'].fillna(df['dep_date'], inplace=True)\n",
        "    df['dep_time_act'].fillna(df['dep_time'], inplace=True)\n",
        "    # arrival date/time in GMT\n",
        "    df['arr_date'], df['arr_time'] = df['arr_time_utc'].dt.normalize(), df['arr_time_utc'].dt.time\n",
        "    # updated arrival date/time in GMT\n",
        "    df['arr_date_upd'], df['arr_time_upd'] = df['arr_estimated_utc'].dt.normalize(), df['arr_estimated_utc'].dt.time\n",
        "    df['arr_date_upd'].fillna(df['arr_date'], inplace=True)\n",
        "    df['arr_time_upd'].fillna(df['arr_time'], inplace=True)\n",
        "    df.drop( columns=cols, inplace=True)\n",
        "    return df\n",
        "\n",
        "def prep_load(df):\n",
        "    df[['delayed','dep_delayed']] = df[['delayed','dep_delayed']].fillna(0)\n",
        "    df = df[df['status']=='landed']\n",
        "    return df\n",
        "\n",
        "# ## Load\n",
        "# Load data into Postgres database\n",
        "\n",
        "def create_table(conn):\n",
        "    cur = conn.cursor()\n",
        "    try:\n",
        "        cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS public.schedule(\n",
        "        flight_id BIGSERIAL PRIMARY KEY,\n",
        "        flight_iata VARCHAR(8),\n",
        "        status VARCHAR(10),\n",
        "        departure_airport VARCHAR(255),\n",
        "        arrival_airport VARCHAR(255),\n",
        "        dep_date date,\n",
        "        dep_time time,\n",
        "        dep_time_upd time,\n",
        "        dep_time_act time,\n",
        "        arr_date date,\n",
        "        arr_time time,\n",
        "        arr_time_upd time,\n",
        "        duration real,\n",
        "        delayed real,\n",
        "        dep_delayed real,\n",
        "        arr_date_upd date,\n",
        "        dep_date_upd date,\n",
        "        dep_date_act date);\n",
        "        \"\"\")\n",
        "    except (Exception, psycopg2.DatabaseError) as error:\n",
        "        print(error)\n",
        "        conn.rollback()\n",
        "    else:\n",
        "        conn.commit()\n",
        "\n",
        "def insert_values(conn, df, table):\n",
        "    tuples = [tuple(x) for x in df.to_numpy()]\n",
        "    cols = ','.join(list(df.columns))\n",
        "    query = \"\"\"INSERT INTO %s(%s) VALUES %%s;\"\"\" % (table, cols)\n",
        "    cursor = conn.cursor()\n",
        "    try:\n",
        "        extras.execute_values(cursor, query, tuples)\n",
        "        conn.commit()\n",
        "    except (Exception, psycopg2.DatabaseError) as error:\n",
        "        print(\"Error: %s\" % error)\n",
        "        conn.rollback()\n",
        "        cursor.close()\n",
        "        return 1\n",
        "    cursor.close()\n",
        "\n",
        "def main():\n",
        "\n",
        "    api_key = \"YOUR_AIRLABS_API_KEY\"\n",
        "\n",
        "    conn = psycopg2.connect(\n",
        "        host=\"postgres\", # changed from 'localhost' so it would work with docker\n",
        "        database=\"ryanair_API\",\n",
        "        user=\"postgres\", #your postgres username\n",
        "        password=\"POSTGRES_PASSWORD\")\n",
        "\n",
        "    data = extract(api_key)\n",
        "    print(\"Transforming...\")\n",
        "    create_table(conn)\n",
        "    data = prep_load(convert_timestamp(convert_iata(data)))\n",
        "    print(\"Loading...\")\n",
        "    insert_values(conn, data, 'schedule')\n",
        "    print(\"Finished.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "X_eo4V8PKH2p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 —Running Docker and Airflow\n",
        "```\n",
        "docker compose up -d\n",
        "\n",
        "docker exec -tiu postgres postgres psql\n",
        "\n",
        "\\l\n",
        "\n",
        "\\c ryanair_API\n",
        "SELECT COUNT(*) FROM schedule;\n",
        "\n",
        "docker exec -it -u postgres postgres psql -d ryanair_API -c \"COPY (SELECT * FROM schedule) TO STDOUT WITH CSV HEADER\" > data.csv\n",
        "\n",
        "docker compose down\n",
        "```"
      ],
      "metadata": {
        "id": "npFjU0NiKShP"
      }
    }
  ]
}