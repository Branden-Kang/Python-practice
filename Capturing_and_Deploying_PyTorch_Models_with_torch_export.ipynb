{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNWo1gCg+32hWDl68KlBDvT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://chaimrand.medium.com/capturing-and-deploying-pytorch-models-with-torch-export-480f0d9ea8fd)"
      ],
      "metadata": {
        "id": "-ECLjXXux9cE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fkBSvC2MxKpd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "NUM_TOKENS = 1024\n",
        "MAX_SEQ_LEN = 256\n",
        "PAD_ID = 0\n",
        "START_ID = 1\n",
        "END_ID = 2\n",
        "\n",
        "# Set up an image-to-text model.\n",
        "def get_model():\n",
        "\n",
        "    # import transformers utilities\n",
        "    from transformers import (\n",
        "        VisionEncoderDecoderModel,\n",
        "        VisionEncoderDecoderConfig,\n",
        "        AutoConfig\n",
        "    )\n",
        "\n",
        "    config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(\n",
        "        encoder_config=AutoConfig.for_model(\"vit\"),  # vit encoder\n",
        "        decoder_config=AutoConfig.for_model(\"gpt2\")  # gpt2 decoder\n",
        "    )\n",
        "    config.decoder.vocab_size = NUM_TOKENS\n",
        "    config.decoder.use_cache = False\n",
        "    config.decoder_start_token_id = START_ID\n",
        "    config.pad_token_id = PAD_ID\n",
        "    config.eos_token_id = END_ID\n",
        "    config.max_length = MAX_SEQ_LEN\n",
        "\n",
        "    model = VisionEncoderDecoderModel(config=config)\n",
        "    model.encoder.pooler = None  # remove unused pooler\n",
        "    model.eval() # prepare the model for evaluation\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the next token\n",
        "def generate_token(decoder, encoder_hidden_states, sequence):\n",
        "    outputs = decoder(\n",
        "        sequence,\n",
        "        encoder_hidden_states\n",
        "    )\n",
        "    logits = outputs[0][:, -1, :]\n",
        "    return torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "# simple auto-regressive sequence generator\n",
        "def image_to_text_generator(encoder, decoder, image):\n",
        "    # run encoder\n",
        "    encoder_hidden_states = encoder(image)[0]\n",
        "\n",
        "    # initialize sequence\n",
        "    generated_ids = torch.ones(\n",
        "        (image.shape[0], 1),\n",
        "        dtype=torch.long,\n",
        "        device=image.device\n",
        "    ) * START_ID\n",
        "\n",
        "    for _ in range(MAX_SEQ_LEN):\n",
        "        # generate next token\n",
        "        next_token = generate_token(\n",
        "            decoder,\n",
        "            encoder_hidden_states,\n",
        "            generated_ids\n",
        "        )\n",
        "        generated_ids = torch.cat([generated_ids, next_token], dim=-1)\n",
        "        if (next_token == END_ID).all():\n",
        "            break\n",
        "\n",
        "    return generated_ids"
      ],
      "metadata": {
        "id": "9_YwPIwgyDJB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, random, torch\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EXPORT_PATH = '/tmp/export/'\n",
        "\n",
        "def test_inference(model_path=EXPORT_PATH, mode=None, compile=False):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    rnd_image = torch.randn(BATCH_SIZE, 3, 224, 224).to(device)\n",
        "    encoder, decoder = load_model(model_path, mode)\n",
        "    encoder = encoder.to(device)\n",
        "    decoder = decoder.to(device)\n",
        "\n",
        "    if compile:\n",
        "        encoder = torch.compile(encoder, mode=\"reduce-overhead\")\n",
        "        decoder = torch.compile(decoder, dynamic=True)\n",
        "        # run a few warmup rounds\n",
        "        for i in range(10):\n",
        "            image_to_text_generator(encoder, decoder, random_image)\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    # optionally enable mixed precision\n",
        "    with torch.amp.autocast(device, dtype=torch.bfloat16, enabled=True):\n",
        "        with torch.no_grad():\n",
        "            caption = image_to_text_generator(encoder, decoder, rnd_image)\n",
        "\n",
        "    total_time = time.perf_counter() - t0\n",
        "    print(f'batched inference total time: {total_time}')"
      ],
      "metadata": {
        "id": "o-_94apjyLx-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderWrapper(torch.nn.Module):\n",
        "    def __init__(self, decoder_model):\n",
        "        super().__init__()\n",
        "        self.decoder = decoder_model\n",
        "\n",
        "    def forward(self, input_ids, encoder_hidden_states):\n",
        "        return self.decoder(\n",
        "            input_ids=input_ids,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            use_cache=False,\n",
        "            output_attentions=False,\n",
        "            output_hidden_states=False,\n",
        "            return_dict=False\n",
        "        )\n",
        "\n",
        "def load_model(path=EXPORT_PATH, mode=None):\n",
        "    model = get_model()\n",
        "    encoder = model.encoder\n",
        "    decoder = model.decoder\n",
        "    return encoder, DecoderWrapper(decoder)"
      ],
      "metadata": {
        "id": "oHuAMd_NyQ_f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Capturing and Deployment Strategies"
      ],
      "metadata": {
        "id": "jcqBuAnQyTIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def capture_model(model, path=EXPORT_PATH):\n",
        "    # weights only\n",
        "    weights_path = os.path.join(EXPORT_PATH, \"weights.pth\")\n",
        "    torch.save(model.state_dict(), weights_path)\n",
        "\n",
        "def load_model(path=EXPORT_PATH, mode=None):\n",
        "    if mode == 'weights':\n",
        "        model = get_model()\n",
        "        weights_path = os.path.join(path,\"weights.pth\")\n",
        "        state_dict = torch.load(weights_path, map_location=\"cpu\")\n",
        "        model.load_state_dict(state_dict)\n",
        "        return model.encoder, DecoderWrapper(model.decoder)\n",
        "    else:\n",
        "        model = get_model()\n",
        "        return model.encoder, DecoderWrapper(model.decoder)"
      ],
      "metadata": {
        "id": "GAsLAZfgyRLq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def capture_model(model, path=EXPORT_PATH):\n",
        "    # weights only\n",
        "    weights_path = os.path.join(EXPORT_PATH, \"weights.pth\")\n",
        "    torch.save(model.state_dict(), weights_path)\n",
        "\n",
        "    encoder = model.encoder\n",
        "    decoder = DecoderWrapper(model.decoder)\n",
        "\n",
        "    # torchscript encoder using trace\n",
        "    example = torch.randn(1, 3, 224, 224)\n",
        "    encoder_jit = torch.jit.trace(encoder, example)\n",
        "    # optionally apply jit.freeze optimization\n",
        "    encoder_jit = torch.jit.freeze(encoder_jit)\n",
        "    encoder_path = os.path.join(path, \"encoder.pt\")\n",
        "    torch.jit.save(encoder_jit, encoder_path)\n",
        "\n",
        "    try:\n",
        "        # torchscript decoder using scripting\n",
        "        decoder_jit = torch.jit.script(decoder)\n",
        "        # optionally apply jit.freeze optimization\n",
        "        decoder_jit = torch.jit.freeze(decoder_jit)\n",
        "        decoder_path = os.path.join(path, \"decoder.pt\")\n",
        "        torch.jit.save(decoder_jit, decoder_path)\n",
        "    except Exception as e:\n",
        "        print(f'torch.jit.script(model.decoder) failed\\n{e}')\n",
        "\n",
        "def load_model(path=EXPORT_PATH, mode=None):\n",
        "    if mode == 'weights':\n",
        "        model = get_model()\n",
        "        weights_path = os.path.join(path,\"weights.pth\")\n",
        "        state_dict = torch.load(weights_path, map_location=\"cpu\")\n",
        "        model.load_state_dict(state_dict)\n",
        "        return model.encoder, DecoderWrapper(model.decoder)\n",
        "    elif mode == 'torchscript':\n",
        "        encoder_path = os.path.join(path, \"encoder.pt\")\n",
        "        decoder_path = os.path.join(path, \"decoder.pt\")\n",
        "        encoder = torch.jit.load(encoder_path)\n",
        "        decoder = torch.jit.load(decoder_path)\n",
        "        # optionally apply target-device optimization\n",
        "        encoder = torch.jit.optimize_for_inference(encoder)\n",
        "        decoder = torch.jit.optimize_for_inference(decoder)\n",
        "        return encoder, decoder\n",
        "    else:\n",
        "        model = get_model()\n",
        "        return model.encoder, DecoderWrapper(model.decoder)"
      ],
      "metadata": {
        "id": "I7ewBUrMyVgw"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}