{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUPE3Etw6/lO9ZXPHllYq3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://levelup.gitconnected.com/pytorch-a-comprehensive-performance-tuning-guide-a917d18bc6c2)"
      ],
      "metadata": {
        "id": "nAwt8k7hjoPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Always use Mixed Precision"
      ],
      "metadata": {
        "id": "HvCWQBdtjtox"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRPUJBxQjmUX",
        "outputId": "7c0e9706-3668-44c3-fa70-9e0a07addde7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-4b0cb0e0f923>:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.3273839950561523\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Assume model, optimizer, data_loader, loss_fn are defined\n",
        "model = torch.nn.Linear(1024, 1024).cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "# Gradient scaler is crucial for stability\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Sample Data\n",
        "input_data = torch.randn(64, 1024).cuda()\n",
        "target = torch.randn(64, 1024).cuda()\n",
        "\n",
        "# Training step with autocast\n",
        "optimizer.zero_grad()\n",
        "with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "    output = model(input_data)\n",
        "    loss = torch.mean((output - target)**2) # Example loss\n",
        "\n",
        "# Scales loss. Calls backward() on scaled loss to create scaled gradients.\n",
        "scaler.scale(loss).backward()\n",
        "\n",
        "# scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "# If gradients aren't inf/NaN, optimizer.step() is then called,\n",
        "# otherwise, optimizer.step() is skipped.\n",
        "scaler.step(optimizer)\n",
        "\n",
        "# Updates the scale for next iteration.\n",
        "scaler.update()\n",
        "\n",
        "print(f\"Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Use PyTorch 2.0 (or later) If Possible"
      ],
      "metadata": {
        "id": "T3_JmrR3jv6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define a regular Python function using PyTorch operations\n",
        "def my_complex_function(a, b):\n",
        "    x = torch.sin(a) + torch.cos(b)\n",
        "    y = torch.tanh(x * a)\n",
        "    return y / (torch.abs(b) + 1e-6)\n",
        "\n",
        "# Compile the function\n",
        "compiled_function = torch.compile(my_complex_function)\n",
        "\n",
        "# Use the compiled function - first run might be slower due to compilation\n",
        "input_a = torch.randn(1000, 1000).cuda() # Best results often on GPU\n",
        "input_b = torch.randn(1000, 1000).cuda()\n",
        "\n",
        "# Warm-up run (optional, but good practice for timing)\n",
        "_ = compiled_function(input_a, input_b)\n",
        "\n",
        "# Timed run\n",
        "import time\n",
        "start = time.time()\n",
        "output = compiled_function(input_a, input_b)\n",
        "end = time.time()\n",
        "print(f\"Compiled function execution time: {end - start:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxuuqCcgjuH4",
        "outputId": "72c1fd58-4199-4dcd-8dd8-ed2e9a67be06"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiled function execution time: 0.0003 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Never Forget Inference Mode"
      ],
      "metadata": {
        "id": "saGoO3L6j3vI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model = torch.nn.Linear(10, 2) # Example model\n",
        "input_tensor = torch.randn(1, 10)\n",
        "\n",
        "# Using torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    output_no_grad = model(input_tensor)\n",
        "print(f\"Output (no_grad) requires_grad: {output_no_grad.requires_grad}\") # Output: False\n",
        "\n",
        "# Using torch.inference_mode() - Recommended\n",
        "with torch.inference_mode():\n",
        "    output_inference_mode = model(input_tensor)\n",
        "print(f\"Output (inference_mode) requires_grad: {output_inference_mode.requires_grad}\") # Output: False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BwaFZFDj5N8",
        "outputId": "d682ee29-d9f6-49d0-a804-85dd59ea4be1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output (no_grad) requires_grad: False\n",
            "Output (inference_mode) requires_grad: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Use Channels-Last Memory Format for CNNs"
      ],
      "metadata": {
        "id": "n0VqKHV_j7BW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "N, C, H, W = 32, 3, 224, 224 # Example dimensions\n",
        "model = nn.Conv2d(C, 64, kernel_size=3, stride=1, padding=1).cuda()\n",
        "input_tensor = torch.randn(N, C, H, W).cuda()\n",
        "\n",
        "# Convert model and input to channels-last\n",
        "model = model.to(memory_format=torch.channels_last)\n",
        "input_tensor = input_tensor.to(memory_format=torch.channels_last)\n",
        "\n",
        "print(f\"Model parameter memory format: {model.weight.stride()}\") # Stride indicates memory layout\n",
        "print(f\"Input tensor memory format: {input_tensor.stride()}\")\n",
        "\n",
        "# Perform operations - PyTorch handles the format internally\n",
        "output = model(input_tensor)\n",
        "print(f\"Output tensor memory format: {output.stride()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG61GeKVj6YR",
        "outputId": "fdef77a6-14b8-46c6-b4df-48486d615c98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameter memory format: (27, 1, 9, 3)\n",
            "Input tensor memory format: (150528, 1, 672, 3)\n",
            "Output tensor memory format: (3211264, 1, 14336, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Perform Graph Surgery where Required"
      ],
      "metadata": {
        "id": "0fo5BG3gj9r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.fx as fx\n",
        "\n",
        "class SimpleNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(5, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = torch.relu(x)\n",
        "        return x\n",
        "\n",
        "module = SimpleNet()\n",
        "symbolic_traced : fx.GraphModule = fx.symbolic_trace(module)\n",
        "\n",
        "# Print the traced graph representation\n",
        "print(\"--- FX Graph ---\")\n",
        "print(symbolic_traced.graph)\n",
        "\n",
        "# Print the generated Python code from the graph\n",
        "print(\"\\n--- FX Code ---\")\n",
        "print(symbolic_traced.code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgU7Bfhxj8Xg",
        "outputId": "92f0b6f7-246f-45b8-fa90-f23f7b42c6c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FX Graph ---\n",
            "graph():\n",
            "    %x : [num_users=1] = placeholder[target=x]\n",
            "    %linear : [num_users=1] = call_module[target=linear](args = (%x,), kwargs = {})\n",
            "    %relu : [num_users=1] = call_function[target=torch.relu](args = (%linear,), kwargs = {})\n",
            "    return relu\n",
            "\n",
            "--- FX Code ---\n",
            "\n",
            "\n",
            "\n",
            "def forward(self, x):\n",
            "    linear = self.linear(x);  x = None\n",
            "    relu = torch.relu(linear);  linear = None\n",
            "    return relu\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Use Activation Checkpointing"
      ],
      "metadata": {
        "id": "GV47QWNKkBOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils import checkpoint as chkpt\n",
        "\n",
        "# regular invocation with default activation caching\n",
        "result = module(*args, **kwargs)  # module is part of a model\n",
        "# checkpointed invocation\n",
        "result = chkpt.checkpoint(module, *args, **kwargs)"
      ],
      "metadata": {
        "id": "SGJ2vL0tj_Yt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Diligent Optimizer Choices"
      ],
      "metadata": {
        "id": "RHuS8g-mkFMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bitsandbytes library developed by Tim Dettmers offers 8-bit versions of many algorithms found in torch.optim, often enabling efficient state tensor management between host and GPU memory as needed."
      ],
      "metadata": {
        "id": "nVobMfHzkJ8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Autotune Convolutions Using cuDNN Benchmarking"
      ],
      "metadata": {
        "id": "pnlkmSt1kMs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Enable benchmark mode (usually at the start of your script)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Define and run your CNN model as usual\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 64, 7, stride=2, padding=3),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(3, 2, 1),\n",
        "    # ... more layers\n",
        ").cuda()\n",
        "\n",
        "# Fixed input size helps benchmark mode\n",
        "input_tensor = torch.randn(64, 3, 224, 224).cuda()\n",
        "\n",
        "# The first forward pass might be slightly slower as benchmarking occurs\n",
        "print(\"Running first forward pass (benchmarking)...\")\n",
        "output = model(input_tensor)\n",
        "print(\"First pass complete.\")\n",
        "\n",
        "# Subsequent passes should use the optimized algorithms\n",
        "print(\"Running second forward pass...\")\n",
        "output = model(input_tensor)\n",
        "print(\"Second pass complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp_QKmpakC9Y",
        "outputId": "88b59553-3f3b-4780-e8d5-c40807e31a65"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running first forward pass (benchmarking)...\n",
            "First pass complete.\n",
            "Running second forward pass...\n",
            "Second pass complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Enable Asynchronous Data Loading"
      ],
      "metadata": {
        "id": "RHdBFLZ5kQTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, size=1000):\n",
        "        self.data = torch.randn(size, 128)\n",
        "        self.labels = torch.randint(0, 10, (size,))\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        # Simulate some data loading/processing\n",
        "        # time.sleep(0.001) # Uncomment to simulate I/O delay\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "dataset = MyDataset()\n",
        "\n",
        "# Optimized DataLoader configuration\n",
        "# Rule of thumb for num_workers: Start with 4 * num_gpus, benchmark, adjust.\n",
        "# Requires sufficient CPU cores and memory.\n",
        "optimized_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    num_workers=4,  # Use multiple processes for loading\n",
        "    pin_memory=True # Speeds up CPU-to-GPU transfer (if using CUDA)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q__ZFWgTkO45",
        "outputId": "87eb0c16-a385-4496-dc3c-4a97f9d047cd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Optimize Memory Usage\n",
        "To save memory by resetting gradient tensors to None instead of updating them to dense tensors filled with zeros, simply call optimizer.zero_grad(set_to_none=True) or model.zero_grad(set_to_none=True). This not only conserves memory but also enhances performance during training."
      ],
      "metadata": {
        "id": "_IqgnrLXkTmO"
      }
    }
  ]
}