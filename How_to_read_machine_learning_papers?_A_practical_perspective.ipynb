{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+fh/Lw7kn6a3efYP3bynz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://hilbert-cantor.medium.com/how-to-read-machine-learning-papers-a-practical-perspective-pt-2-5fca0485708b)"
      ],
      "metadata": {
        "id": "c2hSSbAlsoVU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2peUpHzbqIe0",
        "outputId": "3e64ba75-1aa9-4bfa-8c3d-bd66c485855d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P_n(A) = 0.75\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def empirical_probability(data, A):\n",
        "    \"\"\"\n",
        "    Calculate P_n(A) = (1/n) sum_{i=1}^n 1{x_i in A}\n",
        "\n",
        "    Args:\n",
        "        data (list or array): List of observations x1, x2, ..., xn\n",
        "        A (function): A function that takes x and returns True if x in A, otherwise False\n",
        "\n",
        "    Returns:\n",
        "        float: Empirical probability P_n(A)\n",
        "    \"\"\"\n",
        "    n = len(data)\n",
        "    count = sum(1 for x in data if A(x))\n",
        "    return count / n\n",
        "\n",
        "# Example usage:\n",
        "# Sample data\n",
        "data = [3, 5, 5, 7]\n",
        "# Define the set A: here, A = {x | x <= 5}\n",
        "def A(x):\n",
        "    return x <= 5\n",
        "# Compute P_n(A)\n",
        "p_n_A = empirical_probability(data, A)\n",
        "print(f\"P_n(A) = {p_n_A}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# 1. Initialize model q_theta with parameters theta\n",
        "model = initialize_model()\n",
        "\n",
        "# 2. Set optimization parameters (like learning rate)\n",
        "optimizer = initialize_optimizer(model.parameters())\n",
        "# 3. Loop over training iterations\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    # 4. Sample a batch of data from the empirical distribution p_n\n",
        "    batch = sample_batch_from_data(data, batch_size)\n",
        "    \n",
        "    # 5. Compute the negative log-likelihood loss:\n",
        "    #    Loss = - (1 / batch_size) * sum(log(q_theta(x))) over x in batch\n",
        "    log_probs = model.log_prob(batch)        # model must output log-probabilities\n",
        "    loss = - log_probs.mean()\n",
        "    \n",
        "    # 6. Backpropagate and update theta\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # 7. (Optional) Print or log the loss\n",
        "    print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
        "```"
      ],
      "metadata": {
        "id": "NezD70XZtQxU"
      }
    }
  ]
}