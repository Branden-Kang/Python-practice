{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPO/if72eD7wtbPXld5NWR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Standard Library\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Third Party\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import average_precision_score, log_loss, roc_auc_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Base Neural Network Model\n",
        "# -----------------------------------------------------------\n",
        "class BaseNNModel(nn.Module, ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self, **kwargs):\n",
        "        super(BaseNNModel, self).__init__()\n",
        "\n",
        "    @abstractmethod\n",
        "    def build_network(self) -> nn.Module:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        pass\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# MLP Model\n",
        "# -----------------------------------------------------------\n",
        "class MLPModel(BaseNNModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dims: list[int],\n",
        "        cat_features: list[int] | None = None,\n",
        "        cat_dims: list[int] | None = None,\n",
        "        emb_dim: int = 8,\n",
        "    ):\n",
        "        super(MLPModel, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.cat_features = cat_features or []\n",
        "        self.cat_dims = cat_dims or []\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.embeddings = None\n",
        "        self.network = self.build_network()\n",
        "\n",
        "    def build_network(self) -> nn.Sequential:\n",
        "        # categorical embedding layer\n",
        "        if len(self.cat_dims) > 0:\n",
        "            self.embeddings = nn.ModuleList(\n",
        "                [nn.Embedding(cat_dim, self.emb_dim) for cat_dim in self.cat_dims]\n",
        "            )\n",
        "\n",
        "        combined_input_dim = (\n",
        "            self.input_dim - len(self.cat_features)\n",
        "            + len(self.cat_features) * self.emb_dim\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        dims = [combined_input_dim] + self.hidden_dims\n",
        "\n",
        "        for i in range(len(dims) - 1):\n",
        "            hidden_layer = nn.Linear(dims[i], dims[i + 1])\n",
        "            nn.init.kaiming_normal_(\n",
        "                hidden_layer.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
        "            )\n",
        "\n",
        "            layers.append(hidden_layer)\n",
        "            layers.append(nn.BatchNorm1d(dims[i + 1]))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        # output layer (logit)\n",
        "        layers.append(nn.Linear(dims[-1], 1))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        continuous_features = []\n",
        "        embedded_features = []\n",
        "        cat_idx = 0\n",
        "\n",
        "        for i in range(self.input_dim):\n",
        "            if i in self.cat_features:\n",
        "                cat_values = x[:, i].long()\n",
        "                embedded = self.embeddings[cat_idx](cat_values)\n",
        "                embedded_features.append(embedded)\n",
        "                cat_idx += 1\n",
        "            else:\n",
        "                continuous_features.append(x[:, i : i + 1])\n",
        "\n",
        "        if continuous_features:\n",
        "            continuous_features = torch.cat(continuous_features, dim=1)\n",
        "        else:\n",
        "            continuous_features = None\n",
        "\n",
        "        if embedded_features:\n",
        "            embedded_features = torch.cat(embedded_features, dim=1)\n",
        "        else:\n",
        "            embedded_features = None\n",
        "\n",
        "        if continuous_features is not None and embedded_features is not None:\n",
        "            combined_features = torch.cat(\n",
        "                [continuous_features, embedded_features], dim=1\n",
        "            )\n",
        "        elif embedded_features is not None:\n",
        "            combined_features = embedded_features\n",
        "        elif continuous_features is not None:\n",
        "            combined_features = continuous_features\n",
        "        else:\n",
        "            raise ValueError(\"No features found for forward pass.\")\n",
        "\n",
        "        logits = self.network(combined_features)\n",
        "        return logits  # (B, 1)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# TabTransformer Model (단일 클래스, BaseNNModel 상속)\n",
        "# -----------------------------------------------------------\n",
        "class TabTransformerModel(BaseNNModel):\n",
        "    \"\"\"\n",
        "    DeepLearningBinaryClassifier 에서 사용할 단일 클래스 TabTransformer.\n",
        "    - input_dim: 전체 feature 개수\n",
        "    - cat_features: 범주형 feature 인덱스 리스트 (X의 column index)\n",
        "    - cat_dims: 각 범주형 feature의 cardinality (고유값 개수)\n",
        "    - 나머지 파라미터는 TabTransformer 구조 하이퍼파라미터\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        cat_features: list[int] | None = None,\n",
        "        cat_dims: list[int] | None = None,\n",
        "        d_token: int = 32,\n",
        "        n_heads: int = 4,\n",
        "        n_layers: int = 2,\n",
        "        dim_feedforward: int = 128,\n",
        "        attn_dropout: float = 0.1,\n",
        "        embedding_dropout: float = 0.1,\n",
        "        add_cls: bool = False,\n",
        "        pooling: str = \"concat\",  # \"concat\" or \"cls\"\n",
        "        cont_proj: str = \"linear\",  # \"none\" or \"linear\"\n",
        "        mlp_hidden_dims: tuple[int, ...] = (128, 64),\n",
        "        mlp_dropout: float = 0.2,\n",
        "        padding_idx: int = 0,  # 0 을 OOV/pad로 예약한 경우\n",
        "    ):\n",
        "        super(TabTransformerModel, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.cat_features = cat_features or []\n",
        "        self.cat_dims = cat_dims or []\n",
        "        self.d_token = d_token\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.dim_feedforward = dim_feedforward\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.embedding_dropout = embedding_dropout\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "        self.cont_proj = cont_proj\n",
        "        self.mlp_hidden_dims = mlp_hidden_dims\n",
        "        self.mlp_dropout = mlp_dropout\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "        assert len(self.cat_features) == len(\n",
        "            self.cat_dims\n",
        "        ), \"cat_features 개수와 cat_dims 개수가 다릅니다.\"\n",
        "\n",
        "        # 연속형 feature 개수\n",
        "        self.n_cat = len(self.cat_features)\n",
        "        self.n_cont = self.input_dim - self.n_cat\n",
        "\n",
        "        # ====== 모듈 구성 (Categorical path) ======\n",
        "        if self.n_cat == 0:\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "            self.col_embedding = None\n",
        "        else:\n",
        "            self.cat_embeddings = nn.ModuleList(\n",
        "                [\n",
        "                    nn.Embedding(\n",
        "                        num_embeddings=c + (1 if padding_idx is not None else 0),\n",
        "                        embedding_dim=d_token,\n",
        "                        padding_idx=padding_idx if padding_idx is not None else None,\n",
        "                    )\n",
        "                    for c in self.cat_dims\n",
        "                ]\n",
        "            )\n",
        "            self.col_embedding = nn.Embedding(self.n_cat, d_token)\n",
        "\n",
        "        if self.add_cls:\n",
        "            self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n",
        "            nn.init.normal_(self.cls_token, std=0.02)\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout(self.embedding_dropout)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_token,\n",
        "            nhead=self.n_heads,\n",
        "            dim_feedforward=self.dim_feedforward,\n",
        "            dropout=self.attn_dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True,\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=self.n_layers)\n",
        "\n",
        "        # cat embedding 초기화\n",
        "        for emb in self.cat_embeddings:\n",
        "            nn.init.normal_(emb.weight, std=0.02)\n",
        "        if self.col_embedding is not None:\n",
        "            nn.init.normal_(self.col_embedding.weight, std=0.02)\n",
        "\n",
        "        # ====== 연속형 path ======\n",
        "        if self.n_cont > 0:\n",
        "            self.cont_bn = nn.BatchNorm1d(self.n_cont)\n",
        "            if self.cont_proj == \"linear\":\n",
        "                self.cont_linear = nn.Linear(self.n_cont, d_token)\n",
        "                nn.init.kaiming_uniform_(\n",
        "                    self.cont_linear.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
        "                )\n",
        "                cont_out_dim = d_token\n",
        "            else:\n",
        "                self.cont_linear = nn.Identity()\n",
        "                cont_out_dim = self.n_cont\n",
        "        else:\n",
        "            self.cont_bn = None\n",
        "            self.cont_linear = None\n",
        "            cont_out_dim = 0\n",
        "\n",
        "        # ====== Head (logit 출력) ======\n",
        "        backbone_out = (\n",
        "            self.d_token if self.pooling == \"cls\" else self.n_cat * self.d_token\n",
        "        )\n",
        "        in_dim = backbone_out + cont_out_dim\n",
        "\n",
        "        layers = []\n",
        "        prev = in_dim\n",
        "        for h in self.mlp_hidden_dims:\n",
        "            lin = nn.Linear(prev, h)\n",
        "            nn.init.kaiming_uniform_(\n",
        "                lin.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
        "            )\n",
        "            layers.extend(\n",
        "                [lin, nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(self.mlp_dropout)]\n",
        "            )\n",
        "            prev = h\n",
        "        layers.append(nn.Linear(prev, 1))  # ★ Sigmoid 없음: BCEWithLogitsLoss와 맞춤\n",
        "        self.head = nn.Sequential(*layers)\n",
        "\n",
        "    # BaseNNModel 요구사항: build_network 구현 (여기서는 self 반환)\n",
        "    def build_network(self) -> nn.Module:\n",
        "        return self\n",
        "\n",
        "    def _encode_categoricals(self, x_cat: torch.LongTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x_cat: (B, n_cat) -> contextualized embedding\n",
        "        \"\"\"\n",
        "        B = x_cat.size(0)\n",
        "        if self.n_cat == 0:\n",
        "            return torch.zeros(\n",
        "                B,\n",
        "                self.d_token if self.pooling == \"cls\" else 0,\n",
        "                device=x_cat.device,\n",
        "                dtype=torch.float32,\n",
        "            )\n",
        "\n",
        "        tok_list = []\n",
        "        for j, emb in enumerate(self.cat_embeddings):\n",
        "            tok = emb(x_cat[:, j])  # (B, d)\n",
        "            if self.col_embedding is not None:\n",
        "                tok = tok + self.col_embedding.weight[j]  # (d,)\n",
        "            tok_list.append(tok.unsqueeze(1))  # (B, 1, d)\n",
        "\n",
        "        x_tok = torch.cat(tok_list, dim=1)  # (B, n_cat, d)\n",
        "\n",
        "        if self.add_cls:\n",
        "            cls = self.cls_token.expand(B, -1, -1)  # (B, 1, d)\n",
        "            x_tok = torch.cat([cls, x_tok], dim=1)  # (B, 1 + n_cat, d)\n",
        "\n",
        "        x_tok = self.embedding_dropout(x_tok)\n",
        "        z = self.transformer(x_tok)  # (B, T, d)\n",
        "\n",
        "        if self.pooling == \"cls\" and self.add_cls:\n",
        "            out = z[:, 0, :]  # (B, d)\n",
        "        elif self.pooling == \"cls\":\n",
        "            out = z.mean(dim=1)  # (B, d)\n",
        "        else:\n",
        "            if self.add_cls:\n",
        "                z = z[:, 1:, :]  # CLS 제거\n",
        "            out = z.reshape(B, -1)  # (B, n_cat*d)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, input_dim)\n",
        "        - self.cat_features 인덱스에 해당하는 column은 categorical, 나머지는 continuous로 사용.\n",
        "        - 출력: (B, 1) logits (sigmoid 전)\n",
        "        \"\"\"\n",
        "        B, D = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        # ----- categorical split -----\n",
        "        if self.n_cat > 0:\n",
        "            cat_idx_tensor = torch.tensor(self.cat_features, device=device)\n",
        "            x_cat = x[:, cat_idx_tensor].long()\n",
        "        else:\n",
        "            x_cat = torch.zeros(B, 0, dtype=torch.long, device=device)\n",
        "\n",
        "        # ----- continuous split -----\n",
        "        if self.n_cont > 0:\n",
        "            cont_idx = [i for i in range(self.input_dim) if i not in self.cat_features]\n",
        "            cont_idx_tensor = torch.tensor(cont_idx, device=device)\n",
        "            x_cont = x[:, cont_idx_tensor].float()\n",
        "        else:\n",
        "            x_cont = None\n",
        "\n",
        "        # categorical path\n",
        "        z_cat = self._encode_categoricals(x_cat)\n",
        "\n",
        "        # continuous path\n",
        "        if (x_cont is not None) and (self.n_cont > 0):\n",
        "            if x_cont.ndim == 1:\n",
        "                x_cont = x_cont.unsqueeze(1)\n",
        "            x_cont = self.cont_bn(x_cont)\n",
        "            x_cont = self.cont_linear(x_cont)\n",
        "            z = torch.cat([z_cat, x_cont], dim=1)\n",
        "        else:\n",
        "            z = z_cat\n",
        "\n",
        "        logits = self.head(z)  # (B, 1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Deep Learning Binary Classifier\n",
        "# -----------------------------------------------------------\n",
        "class DeepLearningBinaryClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_type: str = \"mlp\",\n",
        "        model_params: dict | None = None,\n",
        "    ):\n",
        "        self.model_type = model_type\n",
        "        self.model_params = model_params or {}\n",
        "        self.model = None\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def _build_model(self) -> BaseNNModel:\n",
        "        model_registry = {\n",
        "            \"mlp\": MLPModel,\n",
        "            \"tabtransformer\": TabTransformerModel,\n",
        "        }\n",
        "\n",
        "        if self.model_type not in model_registry:\n",
        "            raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
        "\n",
        "        valid_params = {\n",
        "            k: v for k, v in self.model_params.items() if k not in [\"loss_fn\", \"lr\"]\n",
        "        }\n",
        "\n",
        "        model_class = model_registry[self.model_type](**valid_params)\n",
        "        return model_class\n",
        "\n",
        "    def _get_loss_fn(self) -> nn.Module:\n",
        "        loss_name = self.model_params.get(\"loss_fn\", \"logloss\")\n",
        "        if loss_name == \"logloss\":\n",
        "            return nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss function: {loss_name}\")\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        X: np.ndarray,\n",
        "        y: np.ndarray,\n",
        "        sample_weight: np.ndarray | None = None,\n",
        "        eval_set: list[tuple[np.ndarray, np.ndarray]] | None = None,\n",
        "        eval_metric: list[str] | None = None,\n",
        "        max_epochs: int = 10,\n",
        "        patience: int | None = None,\n",
        "        batch_size: int = 128,\n",
        "        verbose: bool = True,\n",
        "    ) -> \"DeepLearningBinaryClassifier\":\n",
        "\n",
        "        lr = self.model_params.get(\"lr\", 0.001)\n",
        "        eval_metric = eval_metric or [\"logloss\"]\n",
        "\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(self.device)\n",
        "\n",
        "        if eval_set is not None:\n",
        "            x_eval_tensor = torch.tensor(\n",
        "                eval_set[0][0], dtype=torch.float32\n",
        "            ).to(self.device)\n",
        "            y_eval_true = eval_set[0][1]\n",
        "        else:\n",
        "            x_eval_tensor = None\n",
        "            y_eval_true = None\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight_tensor = torch.tensor(\n",
        "                sample_weight, dtype=torch.float32\n",
        "            ).to(self.device)\n",
        "        else:\n",
        "            sample_weight_tensor = torch.ones_like(y_tensor, dtype=torch.float32)\n",
        "\n",
        "        train_dataset = TensorDataset(X_tensor, y_tensor, sample_weight_tensor)\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True\n",
        "        )\n",
        "\n",
        "        if self.model is None:\n",
        "            self.model = self._build_model().to(self.device)\n",
        "\n",
        "        loss_fn = self._get_loss_fn()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "        patience_counter = 0\n",
        "        best_metric = float(\"inf\")\n",
        "        best_model_weights = None\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            self.model.train()\n",
        "            epoch_loss = 0.0\n",
        "            n_batches = 0\n",
        "\n",
        "            for x_batch, y_batch, weight_batch in train_dataloader:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                y_pred_logits = self.model(x_batch)\n",
        "                loss = loss_fn(y_pred_logits, y_batch)\n",
        "                weighted_loss = (loss * weight_batch).sum() / weight_batch.sum()\n",
        "\n",
        "                weighted_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += weighted_loss.item()\n",
        "                n_batches += 1\n",
        "\n",
        "            if verbose:\n",
        "                print(\n",
        "                    f\"Epoch {epoch + 1}/{max_epochs} \"\n",
        "                    f\"- [train] loss: {epoch_loss / max(1, n_batches):.6f}\"\n",
        "                )\n",
        "\n",
        "            # evaluation\n",
        "            if eval_set is not None:\n",
        "                self.model.eval()\n",
        "                with torch.no_grad():\n",
        "                    y_eval_logits = self.model(x_eval_tensor)\n",
        "                    y_eval_pred = torch.sigmoid(y_eval_logits).cpu().numpy().ravel()\n",
        "\n",
        "                eval_metrics = {}\n",
        "                for metric in eval_metric:\n",
        "                    if metric == \"logloss\":\n",
        "                        eval_metrics[\"logloss\"] = log_loss(y_eval_true, y_eval_pred)\n",
        "                    elif metric == \"average_precision\":\n",
        "                        eval_metrics[\"average_precision\"] = -average_precision_score(\n",
        "                            y_eval_true, y_eval_pred\n",
        "                        )\n",
        "                    elif metric == \"auc\":\n",
        "                        eval_metrics[\"auc\"] = -roc_auc_score(y_eval_true, y_eval_pred)\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unknown metric: {metric}\")\n",
        "\n",
        "                if verbose:\n",
        "                    metrics_str = \", \".join(\n",
        "                        [f\"{k}: {v:.4f}\" for k, v in eval_metrics.items()]\n",
        "                    )\n",
        "                    print(f\"  - [eval] {metrics_str}\")\n",
        "\n",
        "                # early stopping (기준 metric은 리스트의 첫 번째)\n",
        "                main_metric_name = eval_metric[0]\n",
        "                current_metric = eval_metrics.get(\n",
        "                    main_metric_name, eval_metrics[\"logloss\"]\n",
        "                )\n",
        "\n",
        "                if verbose:\n",
        "                    print(\n",
        "                        f\"    -- (early_stopping) current_metric: {current_metric:.6f}, \"\n",
        "                        f\"best_metric: {best_metric:.6f}\"\n",
        "                    )\n",
        "\n",
        "                if current_metric < best_metric:\n",
        "                    best_metric = current_metric\n",
        "                    patience_counter = 0\n",
        "                    best_model_weights = self.model.state_dict()\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience is not None and patience_counter >= patience:\n",
        "                        if verbose:\n",
        "                            print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                        break\n",
        "\n",
        "        if best_model_weights is not None:\n",
        "            self.model.load_state_dict(best_model_weights)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        self.model = self.model.to(self.device)\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            logits = self.model(X_tensor)\n",
        "            probs1 = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "        if probs1.shape[1] == 1:\n",
        "            probs1 = probs1.reshape(-1, 1)\n",
        "\n",
        "        probs0 = 1.0 - probs1\n",
        "        probs = np.hstack((probs0, probs1))\n",
        "        return probs.astype(\"float\")\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        probs = self.predict_proba(X)\n",
        "        return probs.argmax(axis=1)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 한국형 피처 스키마(연령/성별/OS/점수형) 데모 데이터 생성\n",
        "# -----------------------------------------------------------\n",
        "def make_kor_feature_demo_data(\n",
        "    n_samples: int = 5000,\n",
        "    seed: int = 42,\n",
        "):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = n_samples\n",
        "\n",
        "    # --- 1) 각 피처 생성 ---\n",
        "    # 연령: 18~80 사이 normal 분포\n",
        "    age = np.clip(rng.normal(loc=40, scale=12, size=n), 18, 80).astype(\"float32\")\n",
        "\n",
        "    # 성별: 3개 카테고리 (0: 미상, 1: 남, 2: 여)\n",
        "    gender = rng.randint(0, 3, size=n).astype(\"int64\")\n",
        "\n",
        "    # 앱가입건수: 포아송\n",
        "    app_join_cnt = rng.poisson(lam=2.0, size=n).astype(\"float32\")\n",
        "\n",
        "    # 앱마지막접속경과일: 지수분포 (최근일수록 작게)\n",
        "    last_login_days = rng.exponential(scale=10.0, size=n).astype(\"float32\")\n",
        "\n",
        "    # OS: 4개 카테고리 (0: Android, 1: iOS, 2: Web, 3: 기타)\n",
        "    os_cat = rng.randint(0, 4, size=n).astype(\"int64\")\n",
        "\n",
        "    # 앱가입경과일: 평균 200일 정도로\n",
        "    app_join_days = rng.exponential(scale=200.0, size=n).astype(\"float32\")\n",
        "\n",
        "    # 점수형(0~1 근처의 score 형태로)\n",
        "    biz_owner_score = (rng.binomial(1, 0.3, size=n) * rng.uniform(0.5, 1.0, size=n)).astype(\"float32\")\n",
        "    home_owner_score = (rng.binomial(1, 0.4, size=n) * rng.uniform(0.5, 1.0, size=n)).astype(\"float32\")\n",
        "    car_owner_score = (rng.binomial(1, 0.5, size=n) * rng.uniform(0.5, 1.0, size=n)).astype(\"float32\")\n",
        "    married_score   = (rng.binomial(1, 0.5, size=n) * rng.uniform(0.5, 1.0, size=n)).astype(\"float32\")\n",
        "    child_score     = (rng.binomial(1, 0.4, size=n) * rng.uniform(0.5, 1.0, size=n)).astype(\"float32\")\n",
        "\n",
        "    # --- 2) 라벨 생성용 latent score (대충 그럴듯하게) ---\n",
        "    # 범주형 가중치\n",
        "    w_gender = rng.uniform(-0.3, 0.3, size=3)   # gender 0/1/2\n",
        "    w_os     = rng.uniform(-0.2, 0.2, size=4)   # os 0~3\n",
        "\n",
        "    score = np.zeros(n, dtype=\"float32\")\n",
        "\n",
        "    # 연속형/점수형 효과\n",
        "    score += 0.05 * (age - 40) / 10.0          # 나이가 40보다 많으면 +, 적으면 -\n",
        "    score += 0.25 * app_join_cnt              # 가입건수 많을수록 +\n",
        "    score -= 0.03 * last_login_days           # 마지막 접속이 오래되면 -\n",
        "    score -= 0.002 * app_join_days            # 가입한 지 너무 오래돼도 -\n",
        "    score += 0.8 * biz_owner_score            # 사업자일수록 +\n",
        "    score += 0.6 * home_owner_score           # 자가주택 +\n",
        "    score += 0.7 * car_owner_score            # 자동차 보유 +\n",
        "    score += 0.9 * married_score              # 결혼 +\n",
        "    score += 1.0 * child_score                # 자녀 보유 +\n",
        "\n",
        "    # 범주형 효과\n",
        "    score += w_gender[gender]\n",
        "    score += w_os[os_cat]\n",
        "\n",
        "    # 노이즈 + bias\n",
        "    noise = rng.normal(scale=0.5, size=n).astype(\"float32\")\n",
        "    bias = -0.2\n",
        "    logit = score + bias + noise\n",
        "    prob = 1.0 / (1.0 + np.exp(-logit))\n",
        "    y = (prob > 0.5).astype(\"int64\")\n",
        "\n",
        "    # --- 3) 최종 X 매트릭스 (TabTransformer는 float32로 받고, 범주형은 내부에서 long으로 캐스팅) ---\n",
        "    X = np.column_stack(\n",
        "        [\n",
        "            age,                    # 0\n",
        "            gender.astype(\"float32\"),  # 1\n",
        "            app_join_cnt,           # 2\n",
        "            last_login_days,        # 3\n",
        "            os_cat.astype(\"float32\"),  # 4\n",
        "            app_join_days,          # 5\n",
        "            biz_owner_score,        # 6\n",
        "            home_owner_score,       # 7\n",
        "            car_owner_score,        # 8\n",
        "            married_score,          # 9\n",
        "            child_score,            # 10\n",
        "        ]\n",
        "    ).astype(\"float32\")\n",
        "\n",
        "    # TabTransformer용 범주형 인덱스 및 cardinality\n",
        "    cat_feature_indices = [1, 4]   # 성별, OS\n",
        "    cat_dims = [3, 4]              # gender:3, os:4\n",
        "\n",
        "    return X, y, cat_feature_indices, cat_dims\n",
        "\n",
        "\n",
        "def demo_train_tabtransformer():\n",
        "    print(\"\\n===== TabTransformer Demo (KOR Feature Schema) =====\")\n",
        "\n",
        "    X, y, cat_features, cat_dims = make_kor_feature_demo_data(\n",
        "        n_samples=6000,\n",
        "        seed=123,\n",
        "    )\n",
        "\n",
        "    print(\"X shape:\", X.shape, \" | y shape:\", y.shape)\n",
        "    print(\"Categorical feature indices:\", cat_features)\n",
        "    print(\"Categorical dims (cardinality):\", cat_dims)\n",
        "\n",
        "    # train / val / test split\n",
        "    N = X.shape[0]\n",
        "    idx = np.arange(N)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    tr_end = int(N * 0.7)\n",
        "    va_end = int(N * 0.85)\n",
        "    tr_idx, va_idx, te_idx = idx[:tr_end], idx[tr_end:va_end], idx[va_end:]\n",
        "\n",
        "    X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
        "    X_va, y_va = X[va_idx], y[va_idx]\n",
        "    X_te, y_te = X[te_idx], y[te_idx]\n",
        "\n",
        "    # 간단히 균일한 sample_weight\n",
        "    sample_weight = np.ones_like(y_tr, dtype=\"float32\")\n",
        "\n",
        "    input_dim = X.shape[1]\n",
        "\n",
        "    model_params = {\n",
        "        \"input_dim\": input_dim,\n",
        "        \"cat_features\": cat_features,  # [1,4]  성별/OS\n",
        "        \"cat_dims\": cat_dims,          # [3,4]\n",
        "        \"d_token\": 32,\n",
        "        \"n_heads\": 4,\n",
        "        \"n_layers\": 2,\n",
        "        \"dim_feedforward\": 128,\n",
        "        \"attn_dropout\": 0.1,\n",
        "        \"embedding_dropout\": 0.05,\n",
        "        \"add_cls\": False,\n",
        "        \"pooling\": \"concat\",\n",
        "        \"cont_proj\": \"linear\",\n",
        "        \"mlp_hidden_dims\": (128, 64),\n",
        "        \"mlp_dropout\": 0.2,\n",
        "        \"padding_idx\": 0,\n",
        "        \"lr\": 1e-3,\n",
        "        \"loss_fn\": \"logloss\",\n",
        "    }\n",
        "\n",
        "    clf = DeepLearningBinaryClassifier(\n",
        "        model_type=\"tabtransformer\",\n",
        "        model_params=model_params,\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_tr,\n",
        "        y_tr,\n",
        "        sample_weight=sample_weight,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric=[\"logloss\"],\n",
        "        max_epochs=15,\n",
        "        patience=3,\n",
        "        batch_size=256,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    # 평가\n",
        "    probs_te = clf.predict_proba(X_te)[:, 1]\n",
        "    preds_te = (probs_te >= 0.5).astype(\"int64\")\n",
        "\n",
        "    acc = (preds_te == y_te).mean()\n",
        "    auc = roc_auc_score(y_te, probs_te)\n",
        "    ll = log_loss(y_te, probs_te)\n",
        "\n",
        "    print(\"\\n===== Test Metrics =====\")\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"ROC-AUC  : {auc:.4f}\")\n",
        "    print(f\"Logloss  : {ll:.4f}\")\n",
        "    print(\"Sample probs (first 10):\", np.round(probs_te[:10], 4))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo_train_tabtransformer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4gRYz7YXBRy",
        "outputId": "f0b7d550-c43f-4832-961f-5cc94cb2b5e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== TabTransformer Demo (KOR Feature Schema) =====\n",
            "X shape: (6000, 11)  | y shape: (6000,)\n",
            "Categorical feature indices: [1, 4]\n",
            "Categorical dims (cardinality): [3, 4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 - [train] loss: 138.271694\n",
            "  - [eval] logloss: 0.4680\n",
            "    -- (early_stopping) current_metric: 0.467974, best_metric: inf\n",
            "Epoch 2/15 - [train] loss: 102.804705\n",
            "  - [eval] logloss: 0.3810\n",
            "    -- (early_stopping) current_metric: 0.380984, best_metric: 0.467974\n",
            "Epoch 3/15 - [train] loss: 87.605172\n",
            "  - [eval] logloss: 0.3257\n",
            "    -- (early_stopping) current_metric: 0.325656, best_metric: 0.380984\n",
            "Epoch 4/15 - [train] loss: 78.494526\n",
            "  - [eval] logloss: 0.3000\n",
            "    -- (early_stopping) current_metric: 0.299957, best_metric: 0.325656\n",
            "Epoch 5/15 - [train] loss: 74.516504\n",
            "  - [eval] logloss: 0.2966\n",
            "    -- (early_stopping) current_metric: 0.296625, best_metric: 0.299957\n",
            "Epoch 6/15 - [train] loss: 71.521301\n",
            "  - [eval] logloss: 0.2932\n",
            "    -- (early_stopping) current_metric: 0.293182, best_metric: 0.296625\n",
            "Epoch 7/15 - [train] loss: 71.191573\n",
            "  - [eval] logloss: 0.2861\n",
            "    -- (early_stopping) current_metric: 0.286077, best_metric: 0.293182\n",
            "Epoch 8/15 - [train] loss: 70.055059\n",
            "  - [eval] logloss: 0.2853\n",
            "    -- (early_stopping) current_metric: 0.285349, best_metric: 0.286077\n",
            "Epoch 9/15 - [train] loss: 69.061328\n",
            "  - [eval] logloss: 0.2838\n",
            "    -- (early_stopping) current_metric: 0.283772, best_metric: 0.285349\n",
            "Epoch 10/15 - [train] loss: 68.345420\n",
            "  - [eval] logloss: 0.2843\n",
            "    -- (early_stopping) current_metric: 0.284337, best_metric: 0.283772\n",
            "Epoch 11/15 - [train] loss: 69.313582\n",
            "  - [eval] logloss: 0.2822\n",
            "    -- (early_stopping) current_metric: 0.282214, best_metric: 0.283772\n",
            "Epoch 12/15 - [train] loss: 68.017124\n",
            "  - [eval] logloss: 0.2863\n",
            "    -- (early_stopping) current_metric: 0.286328, best_metric: 0.282214\n",
            "Epoch 13/15 - [train] loss: 66.295483\n",
            "  - [eval] logloss: 0.2860\n",
            "    -- (early_stopping) current_metric: 0.286006, best_metric: 0.282214\n",
            "Epoch 14/15 - [train] loss: 66.833822\n",
            "  - [eval] logloss: 0.2872\n",
            "    -- (early_stopping) current_metric: 0.287150, best_metric: 0.282214\n",
            "Early stopping at epoch 14\n",
            "\n",
            "===== Test Metrics =====\n",
            "Accuracy : 0.8811\n",
            "ROC-AUC  : 0.9347\n",
            "Logloss  : 0.2657\n",
            "Sample probs (first 10): [0.4607 0.9967 0.7107 0.9428 0.5666 0.9976 0.0426 0.9914 0.9426 0.9536]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "단일 파일 테스트용 코드:\n",
        "- BaseNNModel\n",
        "- MLPModel\n",
        "- TabTransformerModel (BaseNNModel 상속, 하나의 클래스에 TabTransformer 구조 포함)\n",
        "- DeepLearningBinaryClassifier (MLP / TabTransformer 공통 학습기)\n",
        "- 더미 데이터셋 생성 + 학습/평가 데모\n",
        "\"\"\"\n",
        "\n",
        "# Standard Library\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Third Party\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import average_precision_score, log_loss, roc_auc_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Base Neural Network Model\n",
        "# -----------------------------------------------------------\n",
        "class BaseNNModel(nn.Module, ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self, **kwargs):\n",
        "        super(BaseNNModel, self).__init__()\n",
        "\n",
        "    @abstractmethod\n",
        "    def build_network(self) -> nn.Module:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        pass\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# MLP Model\n",
        "# -----------------------------------------------------------\n",
        "class MLPModel(BaseNNModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dims: list[int],\n",
        "        cat_features: list[int] | None = None,\n",
        "        cat_dims: list[int] | None = None,\n",
        "        emb_dim: int = 8,\n",
        "    ):\n",
        "        super(MLPModel, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.cat_features = cat_features or []\n",
        "        self.cat_dims = cat_dims or []\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.embeddings = None\n",
        "        self.network = self.build_network()\n",
        "\n",
        "    def build_network(self) -> nn.Sequential:\n",
        "        # categorical embedding layer\n",
        "        if len(self.cat_dims) > 0:\n",
        "            self.embeddings = nn.ModuleList(\n",
        "                [nn.Embedding(cat_dim, self.emb_dim) for cat_dim in self.cat_dims]\n",
        "            )\n",
        "\n",
        "        combined_input_dim = (\n",
        "            self.input_dim - len(self.cat_features)\n",
        "            + len(self.cat_features) * self.emb_dim\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        dims = [combined_input_dim] + self.hidden_dims\n",
        "\n",
        "        for i in range(len(dims) - 1):\n",
        "            hidden_layer = nn.Linear(dims[i], dims[i + 1])\n",
        "            nn.init.kaiming_normal_(\n",
        "                hidden_layer.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
        "            )\n",
        "\n",
        "            layers.append(hidden_layer)\n",
        "            layers.append(nn.BatchNorm1d(dims[i + 1]))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        # output layer (logit)\n",
        "        layers.append(nn.Linear(dims[-1], 1))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        continuous_features = []\n",
        "        embedded_features = []\n",
        "        cat_idx = 0\n",
        "\n",
        "        for i in range(self.input_dim):\n",
        "            if i in self.cat_features:\n",
        "                cat_values = x[:, i].long()\n",
        "                embedded = self.embeddings[cat_idx](cat_values)\n",
        "                embedded_features.append(embedded)\n",
        "                cat_idx += 1\n",
        "            else:\n",
        "                continuous_features.append(x[:, i : i + 1])\n",
        "\n",
        "        if continuous_features:\n",
        "            continuous_features = torch.cat(continuous_features, dim=1)\n",
        "        else:\n",
        "            continuous_features = None\n",
        "\n",
        "        if embedded_features:\n",
        "            embedded_features = torch.cat(embedded_features, dim=1)\n",
        "        else:\n",
        "            embedded_features = None\n",
        "\n",
        "        if continuous_features is not None and embedded_features is not None:\n",
        "            combined_features = torch.cat(\n",
        "                [continuous_features, embedded_features], dim=1\n",
        "            )\n",
        "        elif embedded_features is not None:\n",
        "            combined_features = embedded_features\n",
        "        elif continuous_features is not None:\n",
        "            combined_features = continuous_features\n",
        "        else:\n",
        "            raise ValueError(\"No features found for forward pass.\")\n",
        "\n",
        "        logits = self.network(combined_features)\n",
        "        return logits  # (B, 1)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# TabTransformer Model (단일 클래스, BaseNNModel 상속)\n",
        "# -----------------------------------------------------------\n",
        "class TabTransformerModel(BaseNNModel):\n",
        "    \"\"\"\n",
        "    DeepLearningBinaryClassifier 에서 사용할 단일 클래스 TabTransformer.\n",
        "    - input_dim: 전체 feature 개수\n",
        "    - cat_features: 범주형 feature 인덱스 리스트 (X의 column index)\n",
        "    - cat_dims: 각 범주형 feature의 cardinality (고유값 개수)\n",
        "    - 나머지 파라미터는 TabTransformer 구조 하이퍼파라미터\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        cat_features: list[int] | None = None,\n",
        "        cat_dims: list[int] | None = None,\n",
        "        d_token: int = 32,\n",
        "        n_heads: int = 4,\n",
        "        n_layers: int = 2,\n",
        "        dim_feedforward: int = 128,\n",
        "        attn_dropout: float = 0.1,\n",
        "        embedding_dropout: float = 0.1,\n",
        "        add_cls: bool = False,\n",
        "        pooling: str = \"concat\",  # \"concat\" or \"cls\"\n",
        "        cont_proj: str = \"linear\",  # \"none\" or \"linear\"\n",
        "        mlp_hidden_dims: tuple[int, ...] = (128, 64),\n",
        "        mlp_dropout: float = 0.2,\n",
        "        padding_idx: int = 0,  # 0 을 OOV/pad로 예약한 경우\n",
        "    ):\n",
        "        super(TabTransformerModel, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.cat_features = cat_features or []\n",
        "        self.cat_dims = cat_dims or []\n",
        "        self.d_token = d_token\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.dim_feedforward = dim_feedforward\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.embedding_dropout = embedding_dropout\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "        self.cont_proj = cont_proj\n",
        "        self.mlp_hidden_dims = mlp_hidden_dims\n",
        "        self.mlp_dropout = mlp_dropout\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "        assert len(self.cat_features) == len(\n",
        "            self.cat_dims\n",
        "        ), \"cat_features 개수와 cat_dims 개수가 다릅니다.\"\n",
        "\n",
        "        # 연속형 feature 개수\n",
        "        self.n_cat = len(self.cat_features)\n",
        "        self.n_cont = self.input_dim - self.n_cat\n",
        "\n",
        "        # ====== 모듈 구성 (Categorical path) ======\n",
        "        if self.n_cat == 0:\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "            self.col_embedding = None\n",
        "        else:\n",
        "            self.cat_embeddings = nn.ModuleList(\n",
        "                [\n",
        "                    nn.Embedding(\n",
        "                        num_embeddings=c + (1 if padding_idx is not None else 0),\n",
        "                        embedding_dim=d_token,\n",
        "                        padding_idx=padding_idx if padding_idx is not None else None,\n",
        "                    )\n",
        "                    for c in self.cat_dims\n",
        "                ]\n",
        "            )\n",
        "            self.col_embedding = nn.Embedding(self.n_cat, d_token)\n",
        "\n",
        "        if self.add_cls:\n",
        "            self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n",
        "            nn.init.normal_(self.cls_token, std=0.02)\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout(self.embedding_dropout)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_token,\n",
        "            nhead=self.n_heads,\n",
        "            dim_feedforward=self.dim_feedforward,\n",
        "            dropout=self.attn_dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True,\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=self.n_layers)\n",
        "\n",
        "        # cat embedding 초기화\n",
        "        for emb in self.cat_embeddings:\n",
        "            nn.init.normal_(emb.weight, std=0.02)\n",
        "        if self.col_embedding is not None:\n",
        "            nn.init.normal_(self.col_embedding.weight, std=0.02)\n",
        "\n",
        "        # ====== 연속형 path ======\n",
        "        if self.n_cont > 0:\n",
        "            self.cont_bn = nn.BatchNorm1d(self.n_cont)\n",
        "            if self.cont_proj == \"linear\":\n",
        "                self.cont_linear = nn.Linear(self.n_cont, d_token)\n",
        "                nn.init.kaiming_uniform_(\n",
        "                    self.cont_linear.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
        "                )\n",
        "                cont_out_dim = d_token\n",
        "            else:\n",
        "                self.cont_linear = nn.Identity()\n",
        "                cont_out_dim = self.n_cont\n",
        "        else:\n",
        "            self.cont_bn = None\n",
        "            self.cont_linear = None\n",
        "            cont_out_dim = 0\n",
        "\n",
        "        # ====== Head (logit 출력) ======\n",
        "        backbone_out = (\n",
        "            self.d_token if self.pooling == \"cls\" else self.n_cat * self.d_token\n",
        "        )\n",
        "        in_dim = backbone_out + cont_out_dim\n",
        "\n",
        "        layers = []\n",
        "        prev = in_dim\n",
        "        for h in self.mlp_hidden_dims:\n",
        "            lin = nn.Linear(prev, h)\n",
        "            nn.init.kaiming_uniform_(\n",
        "                lin.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
        "            )\n",
        "            layers.extend(\n",
        "                [lin, nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(self.mlp_dropout)]\n",
        "            )\n",
        "            prev = h\n",
        "        layers.append(nn.Linear(prev, 1))  # ★ Sigmoid 없음: BCEWithLogitsLoss와 맞춤\n",
        "        self.head = nn.Sequential(*layers)\n",
        "\n",
        "    # BaseNNModel 요구사항: build_network 구현 (여기서는 self 반환)\n",
        "    def build_network(self) -> nn.Module:\n",
        "        return self\n",
        "\n",
        "    def _encode_categoricals(self, x_cat: torch.LongTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x_cat: (B, n_cat) -> contextualized embedding\n",
        "        \"\"\"\n",
        "        B = x_cat.size(0)\n",
        "        if self.n_cat == 0:\n",
        "            return torch.zeros(\n",
        "                B,\n",
        "                self.d_token if self.pooling == \"cls\" else 0,\n",
        "                device=x_cat.device,\n",
        "                dtype=torch.float32,\n",
        "            )\n",
        "\n",
        "        tok_list = []\n",
        "        for j, emb in enumerate(self.cat_embeddings):\n",
        "            tok = emb(x_cat[:, j])  # (B, d)\n",
        "            if self.col_embedding is not None:\n",
        "                tok = tok + self.col_embedding.weight[j]  # (d,)\n",
        "            tok_list.append(tok.unsqueeze(1))  # (B, 1, d)\n",
        "\n",
        "        x_tok = torch.cat(tok_list, dim=1)  # (B, n_cat, d)\n",
        "\n",
        "        if self.add_cls:\n",
        "            cls = self.cls_token.expand(B, -1, -1)  # (B, 1, d)\n",
        "            x_tok = torch.cat([cls, x_tok], dim=1)  # (B, 1 + n_cat, d)\n",
        "\n",
        "        x_tok = self.embedding_dropout(x_tok)\n",
        "        z = self.transformer(x_tok)  # (B, T, d)\n",
        "\n",
        "        if self.pooling == \"cls\" and self.add_cls:\n",
        "            out = z[:, 0, :]  # (B, d)\n",
        "        elif self.pooling == \"cls\":\n",
        "            out = z.mean(dim=1)  # (B, d)\n",
        "        else:\n",
        "            if self.add_cls:\n",
        "                z = z[:, 1:, :]  # CLS 제거\n",
        "            out = z.reshape(B, -1)  # (B, n_cat*d)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, input_dim)\n",
        "        - self.cat_features 인덱스에 해당하는 column은 categorical, 나머지는 continuous로 사용.\n",
        "        - 출력: (B, 1) logits (sigmoid 전)\n",
        "        \"\"\"\n",
        "        B, D = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        # ----- categorical split -----\n",
        "        if self.n_cat > 0:\n",
        "            cat_idx_tensor = torch.tensor(self.cat_features, device=device)\n",
        "            x_cat = x[:, cat_idx_tensor].long()\n",
        "        else:\n",
        "            x_cat = torch.zeros(B, 0, dtype=torch.long, device=device)\n",
        "\n",
        "        # ----- continuous split -----\n",
        "        if self.n_cont > 0:\n",
        "            cont_idx = [i for i in range(self.input_dim) if i not in self.cat_features]\n",
        "            cont_idx_tensor = torch.tensor(cont_idx, device=device)\n",
        "            x_cont = x[:, cont_idx_tensor].float()\n",
        "        else:\n",
        "            x_cont = None\n",
        "\n",
        "        # categorical path\n",
        "        z_cat = self._encode_categoricals(x_cat)\n",
        "\n",
        "        # continuous path\n",
        "        if (x_cont is not None) and (self.n_cont > 0):\n",
        "            if x_cont.ndim == 1:\n",
        "                x_cont = x_cont.unsqueeze(1)\n",
        "            x_cont = self.cont_bn(x_cont)\n",
        "            x_cont = self.cont_linear(x_cont)\n",
        "            z = torch.cat([z_cat, x_cont], dim=1)\n",
        "        else:\n",
        "            z = z_cat\n",
        "\n",
        "        logits = self.head(z)  # (B, 1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Deep Learning Binary Classifier\n",
        "# -----------------------------------------------------------\n",
        "class DeepLearningBinaryClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_type: str = \"mlp\",\n",
        "        model_params: dict | None = None,\n",
        "    ):\n",
        "        self.model_type = model_type\n",
        "        self.model_params = model_params or {}\n",
        "        self.model = None\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def _build_model(self) -> BaseNNModel:\n",
        "        model_registry = {\n",
        "            \"mlp\": MLPModel,\n",
        "            \"tabtransformer\": TabTransformerModel,\n",
        "        }\n",
        "\n",
        "        if self.model_type not in model_registry:\n",
        "            raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
        "\n",
        "        valid_params = {\n",
        "            k: v for k, v in self.model_params.items() if k not in [\"loss_fn\", \"lr\"]\n",
        "        }\n",
        "\n",
        "        model_class = model_registry[self.model_type](**valid_params)\n",
        "        return model_class\n",
        "\n",
        "    def _get_loss_fn(self) -> nn.Module:\n",
        "        loss_name = self.model_params.get(\"loss_fn\", \"logloss\")\n",
        "        if loss_name == \"logloss\":\n",
        "            return nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss function: {loss_name}\")\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        X: np.ndarray,\n",
        "        y: np.ndarray,\n",
        "        sample_weight: np.ndarray | None = None,\n",
        "        eval_set: list[tuple[np.ndarray, np.ndarray]] | None = None,\n",
        "        eval_metric: list[str] | None = None,\n",
        "        max_epochs: int = 10,\n",
        "        patience: int | None = None,\n",
        "        batch_size: int = 128,\n",
        "        verbose: bool = True,\n",
        "    ) -> \"DeepLearningBinaryClassifier\":\n",
        "\n",
        "        lr = self.model_params.get(\"lr\", 0.001)\n",
        "        eval_metric = eval_metric or [\"logloss\"]\n",
        "\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(self.device)\n",
        "\n",
        "        if eval_set is not None:\n",
        "            x_eval_tensor = torch.tensor(\n",
        "                eval_set[0][0], dtype=torch.float32\n",
        "            ).to(self.device)\n",
        "            y_eval_true = eval_set[0][1]\n",
        "        else:\n",
        "            x_eval_tensor = None\n",
        "            y_eval_true = None\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight_tensor = torch.tensor(\n",
        "                sample_weight, dtype=torch.float32\n",
        "            ).to(self.device)\n",
        "        else:\n",
        "            sample_weight_tensor = torch.ones_like(y_tensor, dtype=torch.float32)\n",
        "\n",
        "        train_dataset = TensorDataset(X_tensor, y_tensor, sample_weight_tensor)\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True\n",
        "        )\n",
        "\n",
        "        if self.model is None:\n",
        "            self.model = self._build_model().to(self.device)\n",
        "\n",
        "        loss_fn = self._get_loss_fn()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "        patience_counter = 0\n",
        "        best_metric = float(\"inf\")\n",
        "        best_model_weights = None\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            self.model.train()\n",
        "            epoch_loss = 0.0\n",
        "            n_batches = 0\n",
        "\n",
        "            for x_batch, y_batch, weight_batch in train_dataloader:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                y_pred_logits = self.model(x_batch)\n",
        "                loss = loss_fn(y_pred_logits, y_batch)\n",
        "                weighted_loss = (loss * weight_batch).sum() / weight_batch.sum()\n",
        "\n",
        "                weighted_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += weighted_loss.item()\n",
        "                n_batches += 1\n",
        "\n",
        "            if verbose:\n",
        "                print(\n",
        "                    f\"Epoch {epoch + 1}/{max_epochs} \"\n",
        "                    f\"- [train] loss: {epoch_loss / max(1, n_batches):.6f}\"\n",
        "                )\n",
        "\n",
        "            # evaluation\n",
        "            if eval_set is not None:\n",
        "                self.model.eval()\n",
        "                with torch.no_grad():\n",
        "                    y_eval_logits = self.model(x_eval_tensor)\n",
        "                    y_eval_pred = torch.sigmoid(y_eval_logits).cpu().numpy().ravel()\n",
        "\n",
        "                eval_metrics = {}\n",
        "                for metric in eval_metric:\n",
        "                    if metric == \"logloss\":\n",
        "                        eval_metrics[\"logloss\"] = log_loss(y_eval_true, y_eval_pred)\n",
        "                    elif metric == \"average_precision\":\n",
        "                        eval_metrics[\"average_precision\"] = -average_precision_score(\n",
        "                            y_eval_true, y_eval_pred\n",
        "                        )\n",
        "                    elif metric == \"auc\":\n",
        "                        eval_metrics[\"auc\"] = -roc_auc_score(y_eval_true, y_eval_pred)\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unknown metric: {metric}\")\n",
        "\n",
        "                if verbose:\n",
        "                    metrics_str = \", \".join(\n",
        "                        [f\"{k}: {v:.4f}\" for k, v in eval_metrics.items()]\n",
        "                    )\n",
        "                    print(f\"  - [eval] {metrics_str}\")\n",
        "\n",
        "                # early stopping (기준 metric은 리스트의 첫 번째)\n",
        "                main_metric_name = eval_metric[0]\n",
        "                current_metric = eval_metrics.get(\n",
        "                    main_metric_name, eval_metrics[\"logloss\"]\n",
        "                )\n",
        "\n",
        "                if verbose:\n",
        "                    print(\n",
        "                        f\"    -- (early_stopping) current_metric: {current_metric:.6f}, \"\n",
        "                        f\"best_metric: {best_metric:.6f}\"\n",
        "                    )\n",
        "\n",
        "                if current_metric < best_metric:\n",
        "                    best_metric = current_metric\n",
        "                    patience_counter = 0\n",
        "                    best_model_weights = self.model.state_dict()\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience is not None and patience_counter >= patience:\n",
        "                        if verbose:\n",
        "                            print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                        break\n",
        "\n",
        "        if best_model_weights is not None:\n",
        "            self.model.load_state_dict(best_model_weights)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        self.model = self.model.to(self.device)\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            logits = self.model(X_tensor)\n",
        "            probs1 = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "        if probs1.shape[1] == 1:\n",
        "            probs1 = probs1.reshape(-1, 1)\n",
        "\n",
        "        probs0 = 1.0 - probs1\n",
        "        probs = np.hstack((probs0, probs1))\n",
        "        return probs.astype(\"float\")\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        probs = self.predict_proba(X)\n",
        "        return probs.argmax(axis=1)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 더미 데이터셋 생성 (범주형 + 연속형 섞인 이진분류)\n",
        "# -----------------------------------------------------------\n",
        "def make_dummy_tabular_data(\n",
        "    n_samples: int = 5000,\n",
        "    n_cont: int = 10,\n",
        "    seed: int = 42,\n",
        "):\n",
        "    rng = np.random.RandomState(seed)\n",
        "\n",
        "    # 범주형 feature 3개\n",
        "    cat_dims = [3, 5, 4]  # 각 컬럼의 cardinality\n",
        "    n_cat = len(cat_dims)\n",
        "\n",
        "    X_cat = np.zeros((n_samples, n_cat), dtype=np.int64)\n",
        "    for j, k in enumerate(cat_dims):\n",
        "        X_cat[:, j] = rng.randint(0, k, size=n_samples)\n",
        "\n",
        "    # 연속형 feature\n",
        "    X_cont = rng.randn(n_samples, n_cont).astype(\"float32\")\n",
        "\n",
        "    # latent score 생성 (cat + cont 섞어서)\n",
        "    w_cat_cols = [rng.uniform(-1.0, 1.0, size=k) for k in cat_dims]\n",
        "    w_cont = rng.randn(n_cont).astype(\"float32\")\n",
        "\n",
        "    score_cat = np.zeros(n_samples, dtype=\"float32\")\n",
        "    for j in range(n_cat):\n",
        "        score_cat += w_cat_cols[j][X_cat[:, j]]\n",
        "\n",
        "    score_cont = (X_cont * w_cont).sum(axis=1)\n",
        "\n",
        "    bias = 0.1\n",
        "    noise = rng.normal(scale=0.5, size=n_samples).astype(\"float32\")\n",
        "    logit = 0.7 * score_cat + 0.8 * score_cont + bias + noise\n",
        "    prob = 1.0 / (1.0 + np.exp(-logit))\n",
        "    y = (prob > 0.5).astype(\"int64\")\n",
        "\n",
        "    # 최종 feature: [cat | cont] 형태\n",
        "    X = np.concatenate(\n",
        "        [X_cat.astype(\"float32\"), X_cont.astype(\"float32\")], axis=1\n",
        "    ).astype(\"float32\")\n",
        "\n",
        "    cat_feature_indices = list(range(n_cat))  # 0,1,2\n",
        "    return X, y, cat_feature_indices, cat_dims\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 간단한 학습/평가 데모\n",
        "# -----------------------------------------------------------\n",
        "def demo_train_tabtransformer():\n",
        "    print(\"\\n===== TabTransformer Demo =====\")\n",
        "\n",
        "    X, y, cat_features, cat_dims = make_dummy_tabular_data(\n",
        "        n_samples=6000, n_cont=10, seed=123\n",
        "    )\n",
        "\n",
        "    # train / val / test split\n",
        "    N = X.shape[0]\n",
        "    idx = np.arange(N)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    tr_end = int(N * 0.7)\n",
        "    va_end = int(N * 0.85)\n",
        "    tr_idx, va_idx, te_idx = idx[:tr_end], idx[tr_end:va_end], idx[va_end:]\n",
        "\n",
        "    X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
        "    X_va, y_va = X[va_idx], y[va_idx]\n",
        "    X_te, y_te = X[te_idx], y[te_idx]\n",
        "\n",
        "    # 간단히 균일한 sample_weight\n",
        "    sample_weight = np.ones_like(y_tr, dtype=\"float32\")\n",
        "\n",
        "    input_dim = X.shape[1]\n",
        "\n",
        "    model_params = {\n",
        "        \"input_dim\": input_dim,\n",
        "        \"cat_features\": cat_features,  # [0,1,2]\n",
        "        \"cat_dims\": cat_dims,          # [3,5,4]\n",
        "        \"d_token\": 32,\n",
        "        \"n_heads\": 4,\n",
        "        \"n_layers\": 2,\n",
        "        \"dim_feedforward\": 128,\n",
        "        \"attn_dropout\": 0.1,\n",
        "        \"embedding_dropout\": 0.05,\n",
        "        \"add_cls\": False,\n",
        "        \"pooling\": \"concat\",\n",
        "        \"cont_proj\": \"linear\",\n",
        "        \"mlp_hidden_dims\": (128, 64),\n",
        "        \"mlp_dropout\": 0.2,\n",
        "        \"padding_idx\": 0,\n",
        "        \"lr\": 1e-3,\n",
        "        \"loss_fn\": \"logloss\",\n",
        "    }\n",
        "\n",
        "    clf = DeepLearningBinaryClassifier(\n",
        "        model_type=\"tabtransformer\",\n",
        "        model_params=model_params,\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_tr,\n",
        "        y_tr,\n",
        "        sample_weight=sample_weight,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric=[\"logloss\"],\n",
        "        max_epochs=15,\n",
        "        patience=3,\n",
        "        batch_size=256,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    # 평가\n",
        "    probs_te = clf.predict_proba(X_te)[:, 1]\n",
        "    preds_te = (probs_te >= 0.5).astype(\"int64\")\n",
        "\n",
        "    acc = (preds_te == y_te).mean()\n",
        "    auc = roc_auc_score(y_te, probs_te)\n",
        "    ll = log_loss(y_te, probs_te)\n",
        "\n",
        "    print(\"\\n===== Test Metrics =====\")\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"ROC-AUC  : {auc:.4f}\")\n",
        "    print(f\"Logloss  : {ll:.4f}\")\n",
        "    print(\"Sample probs (first 10):\", np.round(probs_te[:10], 4))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo_train_tabtransformer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSaFLDz1Pa1I",
        "outputId": "f6a48f38-2e45-462d-eccd-17893460da40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== TabTransformer Demo =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 - [train] loss: 156.340005\n",
            "  - [eval] logloss: 0.5136\n",
            "    -- (early_stopping) current_metric: 0.513568, best_metric: inf\n",
            "Epoch 2/15 - [train] loss: 115.444524\n",
            "  - [eval] logloss: 0.3385\n",
            "    -- (early_stopping) current_metric: 0.338491, best_metric: 0.513568\n",
            "Epoch 3/15 - [train] loss: 83.781380\n",
            "  - [eval] logloss: 0.2459\n",
            "    -- (early_stopping) current_metric: 0.245882, best_metric: 0.338491\n",
            "Epoch 4/15 - [train] loss: 66.346304\n",
            "  - [eval] logloss: 0.2065\n",
            "    -- (early_stopping) current_metric: 0.206536, best_metric: 0.245882\n",
            "Epoch 5/15 - [train] loss: 56.908177\n",
            "  - [eval] logloss: 0.1844\n",
            "    -- (early_stopping) current_metric: 0.184432, best_metric: 0.206536\n",
            "Epoch 6/15 - [train] loss: 49.811581\n",
            "  - [eval] logloss: 0.1709\n",
            "    -- (early_stopping) current_metric: 0.170867, best_metric: 0.184432\n",
            "Epoch 7/15 - [train] loss: 46.979477\n",
            "  - [eval] logloss: 0.1658\n",
            "    -- (early_stopping) current_metric: 0.165841, best_metric: 0.170867\n",
            "Epoch 8/15 - [train] loss: 43.672781\n",
            "  - [eval] logloss: 0.1559\n",
            "    -- (early_stopping) current_metric: 0.155911, best_metric: 0.165841\n",
            "Epoch 9/15 - [train] loss: 43.587748\n",
            "  - [eval] logloss: 0.1534\n",
            "    -- (early_stopping) current_metric: 0.153368, best_metric: 0.155911\n",
            "Epoch 10/15 - [train] loss: 41.254089\n",
            "  - [eval] logloss: 0.1464\n",
            "    -- (early_stopping) current_metric: 0.146400, best_metric: 0.153368\n",
            "Epoch 11/15 - [train] loss: 40.732533\n",
            "  - [eval] logloss: 0.1439\n",
            "    -- (early_stopping) current_metric: 0.143869, best_metric: 0.146400\n",
            "Epoch 12/15 - [train] loss: 39.365403\n",
            "  - [eval] logloss: 0.1423\n",
            "    -- (early_stopping) current_metric: 0.142313, best_metric: 0.143869\n",
            "Epoch 13/15 - [train] loss: 38.758486\n",
            "  - [eval] logloss: 0.1450\n",
            "    -- (early_stopping) current_metric: 0.144975, best_metric: 0.142313\n",
            "Epoch 14/15 - [train] loss: 37.681143\n",
            "  - [eval] logloss: 0.1551\n",
            "    -- (early_stopping) current_metric: 0.155132, best_metric: 0.142313\n",
            "Epoch 15/15 - [train] loss: 39.267556\n",
            "  - [eval] logloss: 0.1516\n",
            "    -- (early_stopping) current_metric: 0.151599, best_metric: 0.142313\n",
            "Early stopping at epoch 15\n",
            "\n",
            "===== Test Metrics =====\n",
            "Accuracy : 0.9356\n",
            "ROC-AUC  : 0.9876\n",
            "Logloss  : 0.1500\n",
            "Sample probs (first 10): [2.000e-04 9.901e-01 9.827e-01 9.999e-01 2.200e-03 5.000e-03 9.995e-01\n",
            " 1.801e-01 1.490e-02 9.980e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. TabTransformerNet 추가 (logit 출력 버전)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# ... BaseNNModel, MLPModel 는 그대로 두고, 그 아래에 추가 ...\n",
        "\n",
        "\n",
        "class TabTransformerNet(nn.Module):\n",
        "    \"\"\"\n",
        "    TabTransformer backbone + head (binary, logit 출력).\n",
        "    - cat_cardinalities: 각 범주형 컬럼의 unique 개수 (OOV 제외)\n",
        "    - n_continuous: 연속형 컬럼 개수\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        cat_cardinalities,          # List[int] (exclude OOV)\n",
        "        n_continuous=0,\n",
        "        d_token=32,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,\n",
        "        embedding_dropout=0.1,\n",
        "        add_cls=False,\n",
        "        pooling=\"concat\",           # \"concat\" or \"cls\"\n",
        "        cont_proj=\"linear\",         # \"none\" or \"linear\"\n",
        "        mlp_hidden_dims=(128, 64),\n",
        "        mlp_dropout=0.2,\n",
        "        padding_idx=0,              # reserve 0 for OOV/pad if not None\n",
        "        norm_first=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert pooling in (\"concat\", \"cls\")\n",
        "        self.n_cat = len(cat_cardinalities)\n",
        "        self.n_cont = n_continuous\n",
        "        self.d_token = d_token\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "        self.cont_proj = cont_proj\n",
        "\n",
        "        # ---- Categorical path ----\n",
        "        if self.n_cat == 0:\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "            self.col_embedding = None\n",
        "        else:\n",
        "            # +1 slot if padding_idx is used (OOV/pad)\n",
        "            self.cat_embeddings = nn.ModuleList([\n",
        "                nn.Embedding(\n",
        "                    num_embeddings=c + (1 if (padding_idx is not None) else 0),\n",
        "                    embedding_dim=d_token,\n",
        "                    padding_idx=padding_idx,\n",
        "                )\n",
        "                for c in cat_cardinalities\n",
        "            ])\n",
        "            self.col_embedding = nn.Embedding(self.n_cat, d_token)\n",
        "\n",
        "        if self.add_cls:\n",
        "            self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n",
        "            nn.init.normal_(self.cls_token, std=0.02)\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout(embedding_dropout)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_token,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=attn_dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=norm_first,\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "\n",
        "        # init categorical embeddings\n",
        "        for emb in self.cat_embeddings:\n",
        "            nn.init.normal_(emb.weight, std=0.02)\n",
        "        if self.col_embedding is not None:\n",
        "            nn.init.normal_(self.col_embedding.weight, std=0.02)\n",
        "\n",
        "        # ---- Continuous path ----\n",
        "        if n_continuous > 0:\n",
        "            self.cont_bn = nn.BatchNorm1d(n_continuous)\n",
        "            if cont_proj == \"linear\":\n",
        "                self.cont_linear = nn.Linear(n_continuous, d_token)\n",
        "                nn.init.kaiming_uniform_(\n",
        "                    self.cont_linear.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
        "                )\n",
        "                cont_out_dim = d_token\n",
        "            else:\n",
        "                self.cont_linear = nn.Identity()\n",
        "                cont_out_dim = n_continuous\n",
        "        else:\n",
        "            self.cont_bn = None\n",
        "            self.cont_linear = None\n",
        "            cont_out_dim = 0\n",
        "\n",
        "        # ---- Head (마지막에 Sigmoid 없음: logit 출력) ----\n",
        "        backbone_out = (d_token if pooling == \"cls\" else self.n_cat * d_token)\n",
        "        in_dim = backbone_out + cont_out_dim\n",
        "\n",
        "        layers = []\n",
        "        prev = in_dim\n",
        "        for h in mlp_hidden_dims:\n",
        "            lin = nn.Linear(prev, h)\n",
        "            nn.init.kaiming_uniform_(lin.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "            layers.extend([lin, nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(mlp_dropout)])\n",
        "            prev = h\n",
        "        layers.append(nn.Linear(prev, 1))  # logit\n",
        "        self.head = nn.Sequential(*layers)\n",
        "\n",
        "    def _encode_categoricals(self, x_cat: torch.LongTensor):\n",
        "        \"\"\"\n",
        "        x_cat: (B, n_cat) -> contextualized representation\n",
        "        returns:\n",
        "          pooling='concat' -> (B, n_cat*d)\n",
        "          pooling='cls'    -> (B, d)\n",
        "        \"\"\"\n",
        "        B = x_cat.size(0)\n",
        "        if self.n_cat == 0:\n",
        "            return torch.zeros(\n",
        "                B,\n",
        "                self.d_token if self.pooling == \"cls\" else 0,\n",
        "                device=x_cat.device,\n",
        "                dtype=torch.float32,\n",
        "            )\n",
        "\n",
        "        tok_list = []\n",
        "        for j, emb in enumerate(self.cat_embeddings):\n",
        "            tok = emb(x_cat[:, j])  # (B, d)\n",
        "            if self.col_embedding is not None:\n",
        "                tok = tok + self.col_embedding.weight[j]  # (d,)\n",
        "            tok_list.append(tok.unsqueeze(1))  # (B, 1, d)\n",
        "\n",
        "        x_tok = torch.cat(tok_list, dim=1)  # (B, n_cat, d)\n",
        "\n",
        "        if self.add_cls:\n",
        "            cls = self.cls_token.expand(B, -1, -1)  # (B, 1, d)\n",
        "            x_tok = torch.cat([cls, x_tok], dim=1)  # (B, 1+n_cat, d)\n",
        "\n",
        "        x_tok = self.embedding_dropout(x_tok)\n",
        "        z = self.transformer(x_tok)  # (B, T, d)\n",
        "\n",
        "        if self.pooling == \"cls\" and self.add_cls:\n",
        "            out = z[:, 0, :]  # (B, d)\n",
        "        elif self.pooling == \"cls\":\n",
        "            out = z.mean(dim=1)  # (B, d)\n",
        "        else:\n",
        "            if self.add_cls:\n",
        "                z = z[:, 1:, :]  # drop CLS\n",
        "            out = z.reshape(B, -1)  # (B, n_cat*d)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x_cat: torch.LongTensor, x_cont: torch.FloatTensor | None = None):\n",
        "        z_cat = self._encode_categoricals(x_cat)\n",
        "        if (x_cont is not None) and (self.n_cont > 0):\n",
        "            if x_cont.ndim == 1:\n",
        "                x_cont = x_cont.unsqueeze(1)\n",
        "            x_cont = self.cont_bn(x_cont)\n",
        "            x_cont = self.cont_linear(x_cont)\n",
        "            z = torch.cat([z_cat, x_cont], dim=1)\n",
        "        else:\n",
        "            z = z_cat\n",
        "        out = self.head(z)  # (B, 1) logits\n",
        "        return out\n",
        "\n",
        "\n",
        "# 2. BaseNNModel 용 TabTransformer 래퍼 추가\n",
        "\n",
        "class TabTransformerModel(BaseNNModel):\n",
        "    \"\"\"\n",
        "    DeepLearningBinaryClassifier 에서 사용할 TabTransformer 래퍼.\n",
        "    - input_dim: 전체 feature 개수\n",
        "    - cat_features: 범주형 feature 인덱스 (X의 column index)\n",
        "    - cat_dims: 각 범주형 feature의 cardinality (고유 값 개수)\n",
        "    - 나머지 파라미터는 TabTransformerNet 에 그대로 전달\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        cat_features: list[int] | None = None,\n",
        "        cat_dims: list[int] | None = None,\n",
        "        d_token: int = 32,\n",
        "        n_heads: int = 4,\n",
        "        n_layers: int = 2,\n",
        "        dim_feedforward: int = 128,\n",
        "        attn_dropout: float = 0.1,\n",
        "        embedding_dropout: float = 0.1,\n",
        "        add_cls: bool = False,\n",
        "        pooling: str = \"concat\",\n",
        "        cont_proj: str = \"linear\",\n",
        "        mlp_hidden_dims: tuple[int, ...] = (128, 64),\n",
        "        mlp_dropout: float = 0.2,\n",
        "        padding_idx: int = 0,\n",
        "    ):\n",
        "        super(TabTransformerModel, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.cat_features = cat_features or []\n",
        "        self.cat_dims = cat_dims or []\n",
        "        self.n_cont = self.input_dim - len(self.cat_features)\n",
        "\n",
        "        assert len(self.cat_features) == len(\n",
        "            self.cat_dims\n",
        "        ), \"cat_features 개수와 cat_dims 개수가 다릅니다.\"\n",
        "\n",
        "        self.network = TabTransformerNet(\n",
        "            cat_cardinalities=self.cat_dims,\n",
        "            n_continuous=self.n_cont,\n",
        "            d_token=d_token,\n",
        "            n_heads=n_heads,\n",
        "            n_layers=n_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            attn_dropout=attn_dropout,\n",
        "            embedding_dropout=embedding_dropout,\n",
        "            add_cls=add_cls,\n",
        "            pooling=pooling,\n",
        "            cont_proj=cont_proj,\n",
        "            mlp_hidden_dims=mlp_hidden_dims,\n",
        "            mlp_dropout=mlp_dropout,\n",
        "            padding_idx=padding_idx,\n",
        "            norm_first=True,\n",
        "        )\n",
        "\n",
        "    def build_network(self) -> nn.Module:\n",
        "        # DeepLearningBinaryClassifier 에서는 __init__ 시점에 network 를 이미 만든 상태이므로\n",
        "        return self.network\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, input_dim)\n",
        "        - cat_features 인덱스에 해당하는 column 은 LongTensor 로 cast해서 categorical로 사용\n",
        "        - 나머지는 continuous 로 사용\n",
        "        \"\"\"\n",
        "        B, D = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        if len(self.cat_features) > 0:\n",
        "            cat_idx_tensor = torch.tensor(self.cat_features, device=device)\n",
        "            x_cat = x[:, cat_idx_tensor].long()\n",
        "        else:\n",
        "            x_cat = torch.zeros(B, 0, dtype=torch.long, device=device)\n",
        "\n",
        "        if self.n_cont > 0:\n",
        "            cont_idx = [i for i in range(self.input_dim) if i not in self.cat_features]\n",
        "            cont_idx_tensor = torch.tensor(cont_idx, device=device)\n",
        "            x_cont = x[:, cont_idx_tensor].float()\n",
        "        else:\n",
        "            x_cont = None\n",
        "\n",
        "        logits = self.network(x_cat, x_cont)  # (B, 1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# 3. DeepLearningBinaryClassifier 에 TabTransformer 등록\n",
        "class DeepLearningBinaryClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_type: str = \"mlp\",\n",
        "        model_params: dict | None = None,\n",
        "    ):\n",
        "        self.model_type = model_type\n",
        "        self.model_params = model_params or {}\n",
        "        self.model = None\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def _build_model(self) -> BaseNNModel:\n",
        "        model_registry = {\n",
        "            \"mlp\": MLPModel,\n",
        "            \"tabtransformer\": TabTransformerModel,   # ★ 여기 추가\n",
        "        }\n",
        "\n",
        "        if self.model_type not in model_registry:\n",
        "            raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
        "\n",
        "        valid_params = {\n",
        "            k: v\n",
        "            for k, v in self.model_params.items()\n",
        "            if k not in [\"loss_fn\", \"lr\"]\n",
        "        }\n",
        "\n",
        "        model_class = model_registry[self.model_type](**valid_params)\n",
        "        return model_class\n",
        "\n",
        "\n",
        "# 사용\n",
        "from custom_model import DeepLearningBinaryClassifier\n",
        "\n",
        "# 예: 전체 feature 100개, 그 중 10, 11, 15번 column이 범주형이라고 가정\n",
        "input_dim = X_train.shape[1]\n",
        "cat_features = [10, 11, 15]\n",
        "cat_dims = [20, 5, 12]  # 각 컬럼의 cardinality\n",
        "\n",
        "tabtrans_params = {\n",
        "    \"input_dim\": input_dim,\n",
        "    \"cat_features\": cat_features,\n",
        "    \"cat_dims\": cat_dims,\n",
        "    \"d_token\": 32,\n",
        "    \"n_heads\": 4,\n",
        "    \"n_layers\": 2,\n",
        "    \"dim_feedforward\": 128,\n",
        "    \"attn_dropout\": 0.1,\n",
        "    \"embedding_dropout\": 0.05,\n",
        "    \"pooling\": \"concat\",\n",
        "    \"add_cls\": False,\n",
        "    \"cont_proj\": \"linear\",\n",
        "    \"mlp_hidden_dims\": (128, 64),\n",
        "    \"mlp_dropout\": 0.2,\n",
        "    \"padding_idx\": 0,\n",
        "    \"lr\": 1e-3,           # ★ DeepLearningBinaryClassifier 의 학습용\n",
        "    \"loss_fn\": \"logloss\", # ★ BCEWithLogitsLoss 사용\n",
        "}\n",
        "\n",
        "clf = DeepLearningBinaryClassifier(\n",
        "    model_type=\"tabtransformer\",\n",
        "    model_params=tabtrans_params,\n",
        ")\n",
        "\n",
        "clf.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    sample_weight=sample_weight_train,             # 있으면\n",
        "    eval_set=[(X_valid, y_valid)],\n",
        "    eval_metric=[\"logloss\"],\n",
        "    max_epochs=10,\n",
        "    patience=2,\n",
        "    batch_size=1024,\n",
        ")\n",
        "\n",
        "proba_test = clf.predict_proba(X_test)[:, 1]\n",
        "pred_test = clf.predict(X_test)\n"
      ],
      "metadata": {
        "id": "3OAeSyoGMchR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from custom_model import DeepLearningBinaryClassifier\n",
        "\n",
        "# 예: 전체 feature 100개, 그 중 10, 11, 15번 column이 범주형이라고 가정\n",
        "input_dim = X_train.shape[1]\n",
        "cat_features = [10, 11, 15]\n",
        "cat_dims = [20, 5, 12]  # 각 컬럼의 cardinality\n",
        "\n",
        "tabtrans_params = {\n",
        "    \"input_dim\": input_dim,\n",
        "    \"cat_features\": cat_features,\n",
        "    \"cat_dims\": cat_dims,\n",
        "    \"d_token\": 32,\n",
        "    \"n_heads\": 4,\n",
        "    \"n_layers\": 2,\n",
        "    \"dim_feedforward\": 128,\n",
        "    \"attn_dropout\": 0.1,\n",
        "    \"embedding_dropout\": 0.05,\n",
        "    \"pooling\": \"concat\",\n",
        "    \"add_cls\": False,\n",
        "    \"cont_proj\": \"linear\",\n",
        "    \"mlp_hidden_dims\": (128, 64),\n",
        "    \"mlp_dropout\": 0.2,\n",
        "    \"padding_idx\": 0,\n",
        "    \"lr\": 1e-3,           # ★ DeepLearningBinaryClassifier 의 학습용\n",
        "    \"loss_fn\": \"logloss\", # ★ BCEWithLogitsLoss 사용\n",
        "}\n",
        "\n",
        "clf = DeepLearningBinaryClassifier(\n",
        "    model_type=\"tabtransformer\",\n",
        "    model_params=tabtrans_params,\n",
        ")\n",
        "\n",
        "clf.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    sample_weight=sample_weight_train,             # 있으면\n",
        "    eval_set=[(X_valid, y_valid)],\n",
        "    eval_metric=[\"logloss\"],\n",
        "    max_epochs=10,\n",
        "    patience=2,\n",
        "    batch_size=1024,\n",
        ")\n",
        "\n",
        "proba_test = clf.predict_proba(X_test)[:, 1]\n",
        "pred_test = clf.predict(X_test)\n"
      ],
      "metadata": {
        "id": "HS7mOjP9Mbmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deep_tab_transformer_demo.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# Standard Library\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Third Party\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import (\n",
        "    average_precision_score,\n",
        "    log_loss,\n",
        "    roc_auc_score,\n",
        ")\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "# ===========================================================\n",
        "# Base Neural Network Model\n",
        "# ===========================================================\n",
        "class BaseNNModel(nn.Module, ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self, **kwargs):\n",
        "        super(BaseNNModel, self).__init__()\n",
        "\n",
        "    @abstractmethod\n",
        "    def build_network(self) -> nn.Module:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        pass\n",
        "\n",
        "\n",
        "# ===========================================================\n",
        "# MLP Model (기존 MLP)\n",
        "# ===========================================================\n",
        "class MLPModel(BaseNNModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dims: list[int],\n",
        "        cat_features: list[int] | None = None,\n",
        "        cat_dims: list[int] | None = None,\n",
        "        emb_dim: int = 8,\n",
        "    ):\n",
        "        super(MLPModel, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.cat_features = cat_features or []\n",
        "        self.cat_dims = cat_dims or []\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.embeddings: nn.ModuleList | None = None\n",
        "        self.network = self.build_network()\n",
        "\n",
        "    def build_network(self) -> nn.Sequential:\n",
        "        # categorical embedding layer\n",
        "        if len(self.cat_dims) > 0:\n",
        "            self.embeddings = nn.ModuleList(\n",
        "                [nn.Embedding(cat_dim, self.emb_dim) for cat_dim in self.cat_dims]\n",
        "            )\n",
        "        else:\n",
        "            self.embeddings = None\n",
        "\n",
        "        combined_input_dim = (\n",
        "            self.input_dim - len(self.cat_features)\n",
        "            + len(self.cat_features) * self.emb_dim\n",
        "        )\n",
        "\n",
        "        layers: list[nn.Module] = []\n",
        "        dims = [combined_input_dim] + self.hidden_dims\n",
        "\n",
        "        for i in range(len(dims) - 1):\n",
        "            hidden_layer = nn.Linear(dims[i], dims[i + 1])\n",
        "            nn.init.kaiming_normal_(\n",
        "                hidden_layer.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
        "            )\n",
        "            layers.append(hidden_layer)\n",
        "            layers.append(nn.BatchNorm1d(dims[i + 1]))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        # output layer: logit 1개\n",
        "        layers.append(nn.Linear(dims[-1], 1))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        continuous_features = []\n",
        "        embedded_features = []\n",
        "        cat_idx = 0\n",
        "\n",
        "        for i in range(self.input_dim):\n",
        "            if i in self.cat_features and self.embeddings is not None:\n",
        "                cat_values = x[:, i].long()\n",
        "                embedded = self.embeddings[cat_idx](cat_values)\n",
        "                embedded_features.append(embedded)\n",
        "                cat_idx += 1\n",
        "            else:\n",
        "                continuous_features.append(x[:, i : i + 1])\n",
        "\n",
        "        if continuous_features:\n",
        "            continuous_features = torch.cat(continuous_features, dim=1)\n",
        "        else:\n",
        "            continuous_features = None\n",
        "\n",
        "        if embedded_features:\n",
        "            embedded_features = torch.cat(embedded_features, dim=1)\n",
        "        else:\n",
        "            embedded_features = None\n",
        "\n",
        "        if continuous_features is not None and embedded_features is not None:\n",
        "            combined_features = torch.cat(\n",
        "                [continuous_features, embedded_features], dim=1\n",
        "            )\n",
        "        elif embedded_features is not None:\n",
        "            combined_features = embedded_features\n",
        "        elif continuous_features is not None:\n",
        "            combined_features = continuous_features\n",
        "        else:\n",
        "            raise ValueError(\"No features found for forward pass.\")\n",
        "\n",
        "        return self.network(combined_features)  # logits\n",
        "\n",
        "\n",
        "# ===========================================================\n",
        "# TabTransformer Model (BaseNNModel 하나로 통합)\n",
        "# ===========================================================\n",
        "class TabTransformerModel(BaseNNModel):\n",
        "    \"\"\"\n",
        "    BaseNNModel을 직접 상속하는 단일 TabTransformer 모델.\n",
        "\n",
        "    - 입력: x (batch, input_dim)\n",
        "      * cat_features 위치: 정수 인덱스(0,1,2,...) — StringIndexer 등으로 변환된 값\n",
        "      * 나머지: 연속형 특징 (float)\n",
        "    - 출력: (batch, 1) 로짓 (sigmoid 전 값)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        cat_features: list[int] | None = None,\n",
        "        cat_dims: list[int] | None = None,\n",
        "        d_token: int = 32,\n",
        "        n_heads: int = 4,\n",
        "        n_layers: int = 2,\n",
        "        dim_feedforward: int = 128,\n",
        "        attn_dropout: float = 0.1,\n",
        "        embedding_dropout: float = 0.1,\n",
        "        add_cls: bool = False,\n",
        "        pooling: str = \"concat\",      # \"concat\" or \"cls\"\n",
        "        cont_proj: str = \"linear\",    # \"none\" or \"linear\"\n",
        "        hidden_dims: list[int] | tuple[int, ...] = (128, 64),\n",
        "        mlp_dropout: float = 0.2,\n",
        "        padding_idx: int | None = 0,\n",
        "    ):\n",
        "        super(TabTransformerModel, self).__init__()\n",
        "\n",
        "        # 기본 설정\n",
        "        self.input_dim = input_dim\n",
        "        self.cat_features = cat_features or []\n",
        "        self.cat_dims = cat_dims or []\n",
        "        self.cont_features = [\n",
        "            i for i in range(self.input_dim) if i not in self.cat_features\n",
        "        ]\n",
        "\n",
        "        self.d_token = d_token\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.dim_feedforward = dim_feedforward\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.embedding_dropout = embedding_dropout\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "        self.cont_proj = cont_proj\n",
        "        self.hidden_dims = list(hidden_dims)\n",
        "        self.mlp_dropout = mlp_dropout\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "        # ---- Categorical path ----\n",
        "        self.n_cat = len(self.cat_features)\n",
        "        self.n_cont = len(self.cont_features)\n",
        "\n",
        "        if self.n_cat > 0 and len(self.cat_dims) != self.n_cat:\n",
        "            raise ValueError(\n",
        "                f\"len(cat_dims) ({len(self.cat_dims)}) must match len(cat_features) ({self.n_cat})\"\n",
        "            )\n",
        "\n",
        "        if self.n_cat > 0:\n",
        "            # 각 범주형 컬럼별 embedding\n",
        "            self.cat_embeddings = nn.ModuleList(\n",
        "                [\n",
        "                    nn.Embedding(\n",
        "                        num_embeddings=c + (1 if padding_idx is not None else 0),\n",
        "                        embedding_dim=self.d_token,\n",
        "                        padding_idx=(\n",
        "                            padding_idx if padding_idx is not None else None\n",
        "                        ),\n",
        "                    )\n",
        "                    for c in self.cat_dims\n",
        "                ]\n",
        "            )\n",
        "            # column embedding\n",
        "            self.col_embedding = nn.Embedding(self.n_cat, self.d_token)\n",
        "        else:\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "            self.col_embedding = None\n",
        "\n",
        "        if self.add_cls:\n",
        "            self.cls_token = nn.Parameter(torch.zeros(1, 1, self.d_token))\n",
        "            nn.init.normal_(self.cls_token, std=0.02)\n",
        "        else:\n",
        "            self.cls_token = None\n",
        "\n",
        "        self.embedding_dropout_layer = nn.Dropout(self.embedding_dropout)\n",
        "\n",
        "        # Transformer encoder\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.d_token,\n",
        "            nhead=self.n_heads,\n",
        "            dim_feedforward=self.dim_feedforward,\n",
        "            dropout=self.attn_dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True,\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=self.n_layers)\n",
        "\n",
        "        # init categorical embeddings\n",
        "        for emb in self.cat_embeddings:\n",
        "            nn.init.normal_(emb.weight, std=0.02)\n",
        "        if self.col_embedding is not None:\n",
        "            nn.init.normal_(self.col_embedding.weight, std=0.02)\n",
        "\n",
        "        # ---- Continuous path ----\n",
        "        if self.n_cont > 0:\n",
        "            self.cont_bn = nn.BatchNorm1d(self.n_cont)\n",
        "            if self.cont_proj == \"linear\":\n",
        "                self.cont_linear = nn.Linear(self.n_cont, self.d_token)\n",
        "                nn.init.kaiming_uniform_(\n",
        "                    self.cont_linear.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
        "                )\n",
        "                cont_out_dim = self.d_token\n",
        "            else:\n",
        "                self.cont_linear = nn.Identity()\n",
        "                cont_out_dim = self.n_cont\n",
        "        else:\n",
        "            self.cont_bn = None\n",
        "            self.cont_linear = None\n",
        "            cont_out_dim = 0\n",
        "\n",
        "        # ---- Head (MLP, 마지막엔 sigmoid 없이 logit만 출력) ----\n",
        "        if self.pooling == \"cls\":\n",
        "            backbone_out_dim = self.d_token\n",
        "        else:  # concat\n",
        "            backbone_out_dim = self.n_cat * self.d_token if self.n_cat > 0 else 0\n",
        "\n",
        "        in_dim = backbone_out_dim + cont_out_dim\n",
        "\n",
        "        layers: list[nn.Module] = []\n",
        "        prev_dim = in_dim if in_dim > 0 else self.d_token\n",
        "        if in_dim == 0:\n",
        "            # 극단적으로 cat/cont 둘 다 없는 경우 방어 (실제론 안 쓰이겠지만)\n",
        "            layers.append(nn.Linear(self.d_token, self.d_token))\n",
        "            prev_dim = self.d_token\n",
        "\n",
        "        for h in self.hidden_dims:\n",
        "            lin = nn.Linear(prev_dim, h)\n",
        "            nn.init.kaiming_uniform_(lin.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "            layers.extend(\n",
        "                [\n",
        "                    lin,\n",
        "                    nn.BatchNorm1d(h),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(self.mlp_dropout),\n",
        "                ]\n",
        "            )\n",
        "            prev_dim = h\n",
        "\n",
        "        # 출력: 로짓 1개\n",
        "        layers.append(nn.Linear(prev_dim, 1))\n",
        "        self.head = nn.Sequential(*layers)\n",
        "\n",
        "    def build_network(self) -> nn.Module:\n",
        "        # 네트워크 전체가 self 이므로 별도 모듈은 필요 없음\n",
        "        return self\n",
        "\n",
        "    # 내부: categorical 인코딩\n",
        "    def _encode_categoricals(self, x_cat: torch.LongTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x_cat: (B, n_cat) -> (B, n_cat*d) or (B, d) depending on pooling\n",
        "        \"\"\"\n",
        "        B = x_cat.size(0)\n",
        "\n",
        "        if self.n_cat == 0:\n",
        "            if self.pooling == \"cls\":\n",
        "                return torch.zeros(\n",
        "                    B, self.d_token, device=x_cat.device, dtype=torch.float32\n",
        "                )\n",
        "            else:\n",
        "                return torch.zeros(\n",
        "                    B, 0, device=x_cat.device, dtype=torch.float32\n",
        "                )\n",
        "\n",
        "        tok_list = []\n",
        "        for j, emb in enumerate(self.cat_embeddings):\n",
        "            tok = emb(x_cat[:, j])  # (B, d)\n",
        "            if self.col_embedding is not None:\n",
        "                tok = tok + self.col_embedding.weight[j]  # column embedding 더하기\n",
        "            tok_list.append(tok.unsqueeze(1))  # (B,1,d)\n",
        "\n",
        "        x_tok = torch.cat(tok_list, dim=1)  # (B, n_cat, d)\n",
        "\n",
        "        if self.add_cls and self.cls_token is not None:\n",
        "            cls = self.cls_token.expand(B, 1, -1)  # (B,1,d)\n",
        "            x_tok = torch.cat([cls, x_tok], dim=1)  # (B,1+n_cat,d)\n",
        "\n",
        "        x_tok = self.embedding_dropout_layer(x_tok)\n",
        "        z = self.transformer(x_tok)  # (B,T,d)\n",
        "\n",
        "        if self.pooling == \"cls\":\n",
        "            if self.add_cls and self.cls_token is not None:\n",
        "                out = z[:, 0, :]\n",
        "            else:\n",
        "                out = z.mean(dim=1)\n",
        "        else:  # \"concat\"\n",
        "            if self.add_cls and self.cls_token is not None:\n",
        "                z = z[:, 1:, :]  # CLS 제외\n",
        "            out = z.reshape(B, -1)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, input_dim)\n",
        "        - self.cat_features 위치: 정수 인덱스 (Long)\n",
        "        - self.cont_features 위치: float\n",
        "        반환: (B, 1) — 로짓\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "\n",
        "        # 범주형 피처\n",
        "        if self.cat_features:\n",
        "            x_cat = x[:, self.cat_features].long().to(device)  # (B, n_cat)\n",
        "        else:\n",
        "            x_cat = torch.zeros(\n",
        "                (x.size(0), 0), dtype=torch.long, device=device\n",
        "            )\n",
        "\n",
        "        # 연속형 피처\n",
        "        if self.cont_features:\n",
        "            x_cont = x[:, self.cont_features].float().to(device)  # (B, n_cont)\n",
        "        else:\n",
        "            x_cont = None\n",
        "\n",
        "        # categorical encode\n",
        "        z_cat = self._encode_categoricals(x_cat)  # (B, d*)\n",
        "\n",
        "        # continuous path\n",
        "        if x_cont is not None and self.n_cont > 0:\n",
        "            if x_cont.ndim == 1:\n",
        "                x_cont = x_cont.unsqueeze(1)\n",
        "            x_cont = self.cont_bn(x_cont)\n",
        "            x_cont = self.cont_linear(x_cont)\n",
        "            z = torch.cat([z_cat, x_cont], dim=1)\n",
        "        else:\n",
        "            z = z_cat\n",
        "\n",
        "        logit = self.head(z)\n",
        "        return logit\n",
        "\n",
        "\n",
        "# ===========================================================\n",
        "# Deep Learning Binary Classifier (공통 Wrapper)\n",
        "# ===========================================================\n",
        "class DeepLearningBinaryClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_type: str = \"mlp\",\n",
        "        model_params: dict | None = None,\n",
        "    ):\n",
        "        self.model_type = model_type\n",
        "        self.model_params = model_params or {}\n",
        "        self.model: BaseNNModel | None = None\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def _build_model(self) -> BaseNNModel:\n",
        "        model_registry: dict[str, type[BaseNNModel]] = {\n",
        "            \"mlp\": MLPModel,\n",
        "            \"tabtransformer\": TabTransformerModel,\n",
        "        }\n",
        "\n",
        "        if self.model_type not in model_registry:\n",
        "            raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
        "\n",
        "        valid_params = {\n",
        "            k: v\n",
        "            for k, v in self.model_params.items()\n",
        "            if k not in [\"loss_fn\", \"lr\"]\n",
        "        }\n",
        "\n",
        "        model_class = model_registry[self.model_type](**valid_params)\n",
        "        return model_class\n",
        "\n",
        "    def _get_loss_fn(self) -> nn.Module:\n",
        "        loss_name = self.model_params.get(\"loss_fn\", \"logloss\")\n",
        "        if loss_name == \"logloss\":\n",
        "            # logits + BCEWithLogitsLoss\n",
        "            return nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss function: {loss_name}\")\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        X: np.ndarray,\n",
        "        y: np.ndarray,\n",
        "        sample_weight: np.ndarray | None = None,\n",
        "        eval_set: list[tuple[np.ndarray, np.ndarray]] | None = None,\n",
        "        eval_metric: list[str] | None = None,\n",
        "        max_epochs: int = 10,\n",
        "        patience: int | None = None,\n",
        "        batch_size: int = 128,\n",
        "    ) -> \"DeepLearningBinaryClassifier\":\n",
        "\n",
        "        lr = self.model_params.get(\"lr\", 0.001)\n",
        "        eval_metric = eval_metric or [\"logloss\"]\n",
        "\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(self.device)\n",
        "\n",
        "        if eval_set is not None:\n",
        "            x_eval_tensor = torch.tensor(\n",
        "                eval_set[0][0], dtype=torch.float32\n",
        "            ).to(self.device)\n",
        "            y_eval_true = eval_set[0][1]\n",
        "        else:\n",
        "            x_eval_tensor = None\n",
        "            y_eval_true = None\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight_tensor = torch.tensor(\n",
        "                sample_weight, dtype=torch.float32\n",
        "            ).view(-1, 1).to(self.device)\n",
        "        else:\n",
        "            sample_weight_tensor = torch.ones_like(y_tensor, dtype=torch.float32)\n",
        "\n",
        "        train_dataset = TensorDataset(X_tensor, y_tensor, sample_weight_tensor)\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True\n",
        "        )\n",
        "\n",
        "        if self.model is None:\n",
        "            self.model = self._build_model().to(self.device)\n",
        "\n",
        "        loss_fn = self._get_loss_fn()\n",
        "        optimizer = optim.Adam(\n",
        "            self.model.parameters(), lr=lr, weight_decay=1e-4\n",
        "        )\n",
        "\n",
        "        patience_counter = 0\n",
        "        best_metric = float(\"inf\")\n",
        "        best_model_weights = None\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            self.model.train()\n",
        "            epoch_loss = 0.0\n",
        "            n_batches = 0\n",
        "\n",
        "            for x_batch, y_batch, weight_batch in train_dataloader:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                y_pred = self.model(x_batch)  # logits\n",
        "                loss = loss_fn(y_pred, y_batch)\n",
        "                weighted_loss = (loss * weight_batch).sum() / weight_batch.sum()\n",
        "\n",
        "                weighted_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += weighted_loss.item()\n",
        "                n_batches += 1\n",
        "\n",
        "            avg_train_loss = epoch_loss / max(1, n_batches)\n",
        "            print(f\"Epoch {epoch + 1}/{max_epochs}\")\n",
        "            print(f\"- [train] loss: {avg_train_loss:.6f}\")\n",
        "\n",
        "            # evaluation\n",
        "            if eval_set is not None and x_eval_tensor is not None:\n",
        "                self.model.eval()\n",
        "                with torch.no_grad():\n",
        "                    logits_eval = self.model(x_eval_tensor)\n",
        "                    y_eval_pred = (\n",
        "                        torch.sigmoid(logits_eval).cpu().numpy().ravel()\n",
        "                    )\n",
        "\n",
        "                eval_metrics: dict[str, float] = {}\n",
        "                for metric in eval_metric:\n",
        "                    if metric == \"logloss\":\n",
        "                        eval_metrics[\"logloss\"] = log_loss(\n",
        "                            y_eval_true, y_eval_pred, eps=1e-7\n",
        "                        )\n",
        "                    elif metric == \"average_precision\":\n",
        "                        # 최대화 → 부호를 바꿔 최소화 기준에 맞추기\n",
        "                        eval_metrics[\"average_precision\"] = (\n",
        "                            -average_precision_score(y_eval_true, y_eval_pred)\n",
        "                        )\n",
        "                    elif metric == \"auc\":\n",
        "                        eval_metrics[\"auc\"] = -roc_auc_score(\n",
        "                            y_eval_true, y_eval_pred\n",
        "                        )\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unknown metric: {metric}\")\n",
        "\n",
        "                metrics_str = \", \".join(\n",
        "                    [f\"{k}: {v:.4f}\" for k, v in eval_metrics.items()]\n",
        "                )\n",
        "                print(f\"- [eval] {metrics_str}\")\n",
        "\n",
        "                # early stopping: 첫 번째 metric 기준\n",
        "                primary_metric_name = eval_metric[0]\n",
        "                current_metric = eval_metrics.get(\n",
        "                    primary_metric_name, eval_metrics.get(\"logloss\")\n",
        "                )\n",
        "                print(\n",
        "                    f\"  -- (early_stopping) current_metric: {current_metric:.6f}, best_metric: {best_metric:.6f}\"\n",
        "                )\n",
        "\n",
        "                if current_metric < best_metric:\n",
        "                    best_metric = current_metric\n",
        "                    patience_counter = 0\n",
        "                    best_model_weights = {\n",
        "                        k: v.detach().cpu().clone()\n",
        "                        for k, v in self.model.state_dict().items()\n",
        "                    }\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience is not None and patience_counter >= patience:\n",
        "                        print(f\"early stopping at epoch {epoch + 1}\")\n",
        "                        break\n",
        "\n",
        "        if best_model_weights is not None:\n",
        "            self.model.load_state_dict(best_model_weights)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        if self.model is None:\n",
        "            raise RuntimeError(\"Model is not trained yet.\")\n",
        "\n",
        "        self.model = self.model.to(self.device)\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            logits = self.model(X_tensor)\n",
        "            probs1 = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "        if probs1.shape[1] == 1:\n",
        "            probs = np.hstack((1 - probs1, probs1))\n",
        "        else:\n",
        "            probs = probs1\n",
        "\n",
        "        return probs.astype(\"float\")\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        probs = self.predict_proba(X)\n",
        "        return probs.argmax(axis=1)\n",
        "\n",
        "\n",
        "# ===========================================================\n",
        "# 간단 테스트용 데이터 생성 & 실행 예제\n",
        "# ===========================================================\n",
        "def make_dummy_tabular(\n",
        "    n_samples: int = 5000,\n",
        "    n_cont: int = 10,\n",
        "    random_state: int = 42,\n",
        "):\n",
        "    rng = np.random.RandomState(random_state)\n",
        "\n",
        "    # 세 개의 범주형 피처 (0,1,2열)\n",
        "    cat1 = rng.randint(0, 5, size=n_samples)   # cardinality 5\n",
        "    cat2 = rng.randint(0, 4, size=n_samples)   # cardinality 4\n",
        "    cat3 = rng.randint(0, 3, size=n_samples)   # cardinality 3\n",
        "\n",
        "    # 연속형 피처 (3~)\n",
        "    X_cont = rng.randn(n_samples, n_cont).astype(\"float32\")\n",
        "\n",
        "    # 간단한 로짓 생성 (cat + cont 조합)\n",
        "    w_cat1 = rng.uniform(-0.5, 0.8, size=5)\n",
        "    w_cat2 = rng.uniform(-0.3, 0.6, size=4)\n",
        "    w_cat3 = rng.uniform(-1.0, 0.4, size=3)\n",
        "    w_cont = rng.randn(n_cont)\n",
        "\n",
        "    score_cat = (\n",
        "        w_cat1[cat1] + w_cat2[cat2] + w_cat3[cat3]\n",
        "    ).astype(\"float32\")\n",
        "    score_cont = (X_cont * w_cont).sum(axis=1).astype(\"float32\")\n",
        "\n",
        "    logit = 0.7 * score_cat + 0.5 * score_cont + rng.normal(\n",
        "        scale=0.5, size=n_samples\n",
        "    ).astype(\"float32\")\n",
        "    prob = 1.0 / (1.0 + np.exp(-logit))\n",
        "    y = (prob > 0.5).astype(\"int64\")\n",
        "\n",
        "    # 최종 X: (cat1, cat2, cat3, cont...)\n",
        "    X = np.zeros((n_samples, 3 + n_cont), dtype=\"float32\")\n",
        "    X[:, 0] = cat1\n",
        "    X[:, 1] = cat2\n",
        "    X[:, 2] = cat3\n",
        "    X[:, 3:] = X_cont\n",
        "\n",
        "    cat_features = [0, 1, 2]\n",
        "    cat_dims = [5, 4, 3]\n",
        "\n",
        "    return X, y, cat_features, cat_dims\n",
        "\n",
        "\n",
        "def train_demo():\n",
        "    X, y, cat_features, cat_dims = make_dummy_tabular(\n",
        "        n_samples=8000, n_cont=10, random_state=123\n",
        "    )\n",
        "\n",
        "    # train / valid split\n",
        "    N = X.shape[0]\n",
        "    idx = np.arange(N)\n",
        "    np.random.shuffle(idx)\n",
        "    tr_end = int(N * 0.8)\n",
        "\n",
        "    tr_idx = idx[:tr_end]\n",
        "    va_idx = idx[tr_end:]\n",
        "\n",
        "    X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
        "    X_va, y_va = X[va_idx], y[va_idx]\n",
        "\n",
        "    print(\"=== Shape ===\")\n",
        "    print(\"X_tr:\", X_tr.shape, \"y_tr:\", y_tr.shape)\n",
        "    print(\"X_va:\", X_va.shape, \"y_va:\", y_va.shape)\n",
        "    print(\"cat_features:\", cat_features)\n",
        "    print(\"cat_dims:\", cat_dims)\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 1) TabTransformer\n",
        "    # --------------------------------------------------\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"Training TabTransformer model\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    tab_clf = DeepLearningBinaryClassifier(\n",
        "        model_type=\"tabtransformer\",\n",
        "        model_params={\n",
        "            \"input_dim\": X_tr.shape[1],\n",
        "            \"cat_features\": cat_features,\n",
        "            \"cat_dims\": cat_dims,\n",
        "            \"hidden_dims\": [128, 64],\n",
        "            \"d_token\": 32,\n",
        "            \"n_heads\": 4,\n",
        "            \"n_layers\": 2,\n",
        "            \"dim_feedforward\": 128,\n",
        "            \"attn_dropout\": 0.1,\n",
        "            \"embedding_dropout\": 0.05,\n",
        "            \"pooling\": \"concat\",\n",
        "            \"add_cls\": False,\n",
        "            \"cont_proj\": \"linear\",\n",
        "            \"mlp_dropout\": 0.2,\n",
        "            \"lr\": 1e-3,\n",
        "            \"loss_fn\": \"logloss\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "    tab_clf.fit(\n",
        "        X_tr,\n",
        "        y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric=[\"logloss\"],\n",
        "        max_epochs=10,\n",
        "        patience=3,\n",
        "        batch_size=512,\n",
        "    )\n",
        "\n",
        "    proba_tab = tab_clf.predict_proba(X_va)[:, 1]\n",
        "    pred_tab = (proba_tab >= 0.5).astype(int)\n",
        "\n",
        "    auc_tab = roc_auc_score(y_va, proba_tab)\n",
        "    ap_tab = average_precision_score(y_va, proba_tab)\n",
        "    ll_tab = log_loss(y_va, proba_tab)\n",
        "\n",
        "    print(\"\\n[TabTransformer] Metrics on valid:\")\n",
        "    print(f\"AUC: {auc_tab:.4f} | AP: {ap_tab:.4f} | Logloss: {ll_tab:.4f}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 2) MLP (비교용)\n",
        "    # --------------------------------------------------\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"Training MLP model\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    mlp_clf = DeepLearningBinaryClassifier(\n",
        "        model_type=\"mlp\",\n",
        "        model_params={\n",
        "            \"input_dim\": X_tr.shape[1],\n",
        "            \"cat_features\": cat_features,\n",
        "            \"cat_dims\": cat_dims,\n",
        "            \"hidden_dims\": [128, 64],\n",
        "            \"emb_dim\": 8,\n",
        "            \"lr\": 1e-3,\n",
        "            \"loss_fn\": \"logloss\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "    mlp_clf.fit(\n",
        "        X_tr,\n",
        "        y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric=[\"logloss\"],\n",
        "        max_epochs=10,\n",
        "        patience=3,\n",
        "        batch_size=512,\n",
        "    )\n",
        "\n",
        "    proba_mlp = mlp_clf.predict_proba(X_va)[:, 1]\n",
        "    pred_mlp = (proba_mlp >= 0.5).astype(int)\n",
        "\n",
        "    auc_mlp = roc_auc_score(y_va, proba_mlp)\n",
        "    ap_mlp = average_precision_score(y_va, proba_mlp)\n",
        "    ll_mlp = log_loss(y_va, proba_mlp)\n",
        "\n",
        "    print(\"\\n[MLP] Metrics on valid:\")\n",
        "    print(f\"AUC: {auc_mlp:.4f} | AP: {ap_mlp:.4f} | Logloss: {ll_mlp:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_demo()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "hj7oLN6JyuPh",
        "outputId": "595ebea3-d412-48d2-b066-614b881a73b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Shape ===\n",
            "X_tr: (6400, 13) y_tr: (6400,)\n",
            "X_va: (1600, 13) y_va: (1600,)\n",
            "cat_features: [0, 1, 2]\n",
            "cat_dims: [5, 4, 3]\n",
            "\n",
            "==============================\n",
            "Training TabTransformer model\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "- [train] loss: 0.610459\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "got an unexpected keyword argument 'eps'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2607655579.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m     \u001b[0mtrain_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2607655579.py\u001b[0m in \u001b[0;36mtrain_demo\u001b[0;34m()\u001b[0m\n\u001b[1;32m    674\u001b[0m     )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m     tab_clf.fit(\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2607655579.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, max_epochs, patience, batch_size)\u001b[0m\n\u001b[1;32m    503\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"logloss\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m                         eval_metrics[\"logloss\"] = log_loss(\n\u001b[0m\u001b[1;32m    506\u001b[0m                             \u001b[0my_eval_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_eval_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m                         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;31m# Map *args/**kwargs to the function signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/inspect.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mcan\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m         \"\"\"\n\u001b[0;32m-> 3280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbind_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/inspect.py\u001b[0m in \u001b[0;36m_bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3267\u001b[0m                 )\n\u001b[1;32m   3268\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3269\u001b[0;31m                 raise TypeError(\n\u001b[0m\u001b[1;32m   3270\u001b[0m                     'got an unexpected keyword argument {arg!r}'.format(\n\u001b[1;32m   3271\u001b[0m                         arg=next(iter(kwargs))))\n",
            "\u001b[0;31mTypeError\u001b[0m: got an unexpected keyword argument 'eps'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE8VgrM0XtXv",
        "outputId": "1a0b3caf-b310-4ff7-ec5a-8ea4941e4bf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "- [train] loss: 0.553935\n",
            "- [eval] logloss: 0.4053\n",
            " -- (early_stopping) current_metric: 0.405295, best_metric: inf\n",
            "Epoch 2/10\n",
            "- [train] loss: 0.333160\n",
            "- [eval] logloss: 0.2482\n",
            " -- (early_stopping) current_metric: 0.248219, best_metric: 0.405295\n",
            "Epoch 3/10\n",
            "- [train] loss: 0.230129\n",
            "- [eval] logloss: 0.1967\n",
            " -- (early_stopping) current_metric: 0.196677, best_metric: 0.248219\n",
            "Epoch 4/10\n",
            "- [train] loss: 0.181525\n",
            "- [eval] logloss: 0.1572\n",
            " -- (early_stopping) current_metric: 0.157211, best_metric: 0.196677\n",
            "Epoch 5/10\n",
            "- [train] loss: 0.157481\n",
            "- [eval] logloss: 0.1420\n",
            " -- (early_stopping) current_metric: 0.141978, best_metric: 0.157211\n",
            "Epoch 6/10\n",
            "- [train] loss: 0.144002\n",
            "- [eval] logloss: 0.1331\n",
            " -- (early_stopping) current_metric: 0.133109, best_metric: 0.141978\n",
            "Epoch 7/10\n",
            "- [train] loss: 0.135624\n",
            "- [eval] logloss: 0.1270\n",
            " -- (early_stopping) current_metric: 0.127006, best_metric: 0.133109\n",
            "Epoch 8/10\n",
            "- [train] loss: 0.131817\n",
            "- [eval] logloss: 0.1242\n",
            " -- (early_stopping) current_metric: 0.124186, best_metric: 0.127006\n",
            "Epoch 9/10\n",
            "- [train] loss: 0.130688\n",
            "- [eval] logloss: 0.1260\n",
            " -- (early_stopping) current_metric: 0.126020, best_metric: 0.124186\n",
            "Epoch 10/10\n",
            "- [train] loss: 0.127362\n",
            "- [eval] logloss: 0.1244\n",
            " -- (early_stopping) current_metric: 0.124395, best_metric: 0.124186\n",
            "early stopping at epoch 10\n",
            "\n",
            "===== Test Metrics (TabTransformer via DeepLearningBinaryClassifier) =====\n",
            "Accuracy : 0.9420\n",
            "ROC-AUC  : 0.9895\n",
            "Logloss  : 0.1335\n",
            "\n",
            "Sample predictions (first 10):\n",
            "[8.490e-02 3.900e-03 9.864e-01 9.873e-01 2.000e-04 5.320e-02 7.000e-04\n",
            " 9.812e-01 9.809e-01 9.998e-01]\n",
            "\n",
            "[Info] cat_features indices: [0, 1, 2]\n",
            "[Info] cat cardinalities   : [3, 6, 3]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# =========================================================\n",
        "# 통합 코드:\n",
        "#  - TabularPreprocessor + TabTransformerNet\n",
        "#  - BaseNNModel / MLPModel / TabTransformerModel\n",
        "#  - DeepLearningBinaryClassifier (mlp + tabtransformer 지원)\n",
        "#  - 샘플 데이터 생성 + TabTransformer 실행 데모\n",
        "# =========================================================\n",
        "\n",
        "# Standard Library\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Third Party\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    average_precision_score,\n",
        "    log_loss,\n",
        "    roc_auc_score,\n",
        ")\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 0) Preprocessor: string categorical -> integer IDs (with OOV=0)\n",
        "#    + auto cat_idx/cont_idx + cardinalities\n",
        "# ---------------------------------------------------------\n",
        "class TabularPreprocessor:\n",
        "    def __init__(self, categorical_indices=None, use_oov=True, oov_token=0, add_na_token=True):\n",
        "        self.categorical_indices = None if categorical_indices is None else list(categorical_indices)\n",
        "        self.use_oov = use_oov\n",
        "        self.oov_token = int(oov_token)  # 0 recommended\n",
        "        self.add_na_token = add_na_token\n",
        "        self.cat_maps = {}          # col_idx -> {category_value: int_id}\n",
        "        self.cardinalities = []     # per-categorical-column unique count (K, excluding OOV)\n",
        "        self.cat_idx = []\n",
        "        self.cont_idx = []\n",
        "        self.fitted_ = False\n",
        "\n",
        "    def _ensure_ndarray(self, X):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values\n",
        "        return np.asarray(X, dtype=object)  # safe for mixed types\n",
        "\n",
        "    def fit(self, X, categorical_indices=None):\n",
        "        X = self._ensure_ndarray(X)\n",
        "        n_cols = X.shape[1]\n",
        "\n",
        "        if categorical_indices is not None:\n",
        "            self.categorical_indices = list(categorical_indices)\n",
        "\n",
        "        if self.categorical_indices is None:\n",
        "            # fallback: infer by dtype==object\n",
        "            self.categorical_indices = [j for j in range(n_cols) if X[:, j].dtype == object]\n",
        "\n",
        "        cat_set = set(self.categorical_indices)\n",
        "        self.cat_idx = sorted(list(cat_set))\n",
        "        self.cont_idx = [j for j in range(n_cols) if j not in cat_set]\n",
        "\n",
        "        # build per-column maps: real categories 1..K (0 reserved for OOV)\n",
        "        self.cat_maps = {}\n",
        "        self.cardinalities = []\n",
        "        for j in self.cat_idx:\n",
        "            col = X[:, j]\n",
        "            if self.add_na_token:\n",
        "                col = np.where(pd.isna(col), \"<NA>\", col)\n",
        "            uniques = pd.unique(col)\n",
        "            id_map = {val: i + 1 for i, val in enumerate(uniques)}  # 1..K\n",
        "            self.cat_maps[j] = id_map\n",
        "            self.cardinalities.append(len(uniques))  # exclude OOV\n",
        "\n",
        "        self.fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        assert self.fitted_, \"Call fit() before transform().\"\n",
        "        X = self._ensure_ndarray(X)\n",
        "\n",
        "        # categorical\n",
        "        if len(self.cat_idx) > 0:\n",
        "            x_cat = np.zeros((X.shape[0], len(self.cat_idx)), dtype=\"int64\")\n",
        "            for ti, j in enumerate(self.cat_idx):\n",
        "                col = X[:, j]\n",
        "                if self.add_na_token:\n",
        "                    col = np.where(pd.isna(col), \"<NA>\", col)\n",
        "                id_map = self.cat_maps[j]\n",
        "                x_cat[:, ti] = np.array([id_map.get(v, self.oov_token) for v in col], dtype=\"int64\")\n",
        "        else:\n",
        "            x_cat = np.zeros((X.shape[0], 0), dtype=\"int64\")\n",
        "\n",
        "        # continuous\n",
        "        if len(self.cont_idx) > 0:\n",
        "            x_cont = X[:, self.cont_idx].astype(\"float32\")\n",
        "        else:\n",
        "            x_cont = None\n",
        "\n",
        "        return x_cat, x_cont\n",
        "\n",
        "    def fit_transform(self, X, categorical_indices=None):\n",
        "        self.fit(X, categorical_indices)\n",
        "        return self.transform(X)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1) Single-class TabTransformer (Backbone + Continuous + Head)\n",
        "#    (원본과 동일, Sigmoid까지 포함)\n",
        "# ---------------------------------------------------------\n",
        "class TabTransformerNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Single-class TabTransformer:\n",
        "    - Categorical: per-column embedding (+ OOV/pad), column embedding, Transformer encoder, pooling\n",
        "    - Continuous: BatchNorm + (optional) Linear projection to d_token\n",
        "    - Head: MLP -> sigmoid\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        cat_cardinalities,          # List[int] (exclude OOV)\n",
        "        n_continuous=0,\n",
        "        d_token=32,\n",
        "        n_heads=4,\n",
        "        n_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        attn_dropout=0.1,\n",
        "        embedding_dropout=0.1,\n",
        "        add_cls=False,\n",
        "        pooling=\"concat\",           # \"concat\" or \"cls\"\n",
        "        cont_proj=\"linear\",         # \"none\" or \"linear\"\n",
        "        mlp_hidden_dims=(128, 64),\n",
        "        mlp_dropout=0.2,\n",
        "        padding_idx=0,              # reserve 0 for OOV/pad if not None\n",
        "        norm_first=True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert pooling in (\"concat\", \"cls\")\n",
        "        self.n_cat = len(cat_cardinalities)\n",
        "        self.n_cont = n_continuous\n",
        "        self.d_token = d_token\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "        self.cont_proj = cont_proj\n",
        "\n",
        "        # ---- Categorical path ----\n",
        "        if self.n_cat == 0:\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "            self.col_embedding = None\n",
        "        else:\n",
        "            # +1 slot if padding_idx is used (OOV/pad)\n",
        "            self.cat_embeddings = nn.ModuleList([\n",
        "                nn.Embedding(\n",
        "                    num_embeddings=c + (1 if (padding_idx is not None) else 0),\n",
        "                    embedding_dim=d_token,\n",
        "                    padding_idx=padding_idx\n",
        "                )\n",
        "                for c in cat_cardinalities\n",
        "            ])\n",
        "            self.col_embedding = nn.Embedding(self.n_cat, d_token)\n",
        "\n",
        "        if self.add_cls:\n",
        "            self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n",
        "            nn.init.normal_(self.cls_token, std=0.02)\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout(embedding_dropout)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_token,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=attn_dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=norm_first\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "\n",
        "        # init categorical embeddings\n",
        "        for emb in self.cat_embeddings:\n",
        "            nn.init.normal_(emb.weight, std=0.02)\n",
        "        if self.col_embedding is not None:\n",
        "            nn.init.normal_(self.col_embedding.weight, std=0.02)\n",
        "\n",
        "        # ---- Continuous path ----\n",
        "        if n_continuous > 0:\n",
        "            self.cont_bn = nn.BatchNorm1d(n_continuous)\n",
        "            if cont_proj == \"linear\":\n",
        "                self.cont_linear = nn.Linear(n_continuous, d_token)\n",
        "                nn.init.kaiming_uniform_(self.cont_linear.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "                cont_out_dim = d_token\n",
        "            else:\n",
        "                self.cont_linear = nn.Identity()\n",
        "                cont_out_dim = n_continuous\n",
        "        else:\n",
        "            self.cont_bn = None\n",
        "            self.cont_linear = None\n",
        "            cont_out_dim = 0\n",
        "\n",
        "        # ---- Head ----\n",
        "        backbone_out = (d_token if pooling == \"cls\" else self.n_cat * d_token)\n",
        "        in_dim = backbone_out + cont_out_dim\n",
        "\n",
        "        layers = []\n",
        "        prev = in_dim\n",
        "        for h in mlp_hidden_dims:\n",
        "            lin = nn.Linear(prev, h)\n",
        "            nn.init.kaiming_uniform_(lin.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "            layers.extend([lin, nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(mlp_dropout)])\n",
        "            prev = h\n",
        "        layers.append(nn.Linear(prev, 1))\n",
        "        layers.append(nn.Sigmoid())\n",
        "        self.head = nn.Sequential(*layers)\n",
        "\n",
        "    def _encode_categoricals(self, x_cat: torch.LongTensor):\n",
        "        \"\"\"\n",
        "        x_cat: (B, n_cat) -> contextualized representation\n",
        "        returns:\n",
        "          pooling='concat' -> (B, n_cat*d)\n",
        "          pooling='cls'    -> (B, d)\n",
        "        \"\"\"\n",
        "        B = x_cat.size(0)\n",
        "        if self.n_cat == 0:\n",
        "            return torch.zeros(B, self.d_token if self.pooling == \"cls\" else 0,\n",
        "                               device=x_cat.device, dtype=torch.float32)\n",
        "\n",
        "        tok_list = []\n",
        "        for j, emb in enumerate(self.cat_embeddings):\n",
        "            tok = emb(x_cat[:, j])                         # (B, d)\n",
        "            if self.col_embedding is not None:\n",
        "                tok = tok + self.col_embedding.weight[j]   # (d,)\n",
        "            tok_list.append(tok.unsqueeze(1))              # (B, 1, d)\n",
        "\n",
        "        x_tok = torch.cat(tok_list, dim=1)                 # (B, n_cat, d)\n",
        "\n",
        "        if self.add_cls:\n",
        "            cls = self.cls_token.expand(B, -1, -1)         # (B, 1, d)\n",
        "            x_tok = torch.cat([cls, x_tok], dim=1)         # (B, 1+n_cat, d)\n",
        "\n",
        "        x_tok = self.embedding_dropout(x_tok)\n",
        "        z = self.transformer(x_tok)                        # (B, T, d)\n",
        "\n",
        "        if self.pooling == \"cls\" and self.add_cls:\n",
        "            out = z[:, 0, :]                               # (B, d)\n",
        "        elif self.pooling == \"cls\":\n",
        "            out = z.mean(dim=1)                            # (B, d)\n",
        "        else:\n",
        "            if self.add_cls:\n",
        "                z = z[:, 1:, :]                            # drop CLS\n",
        "            out = z.reshape(B, -1)                         # (B, n_cat*d)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x_cat: torch.LongTensor, x_cont: torch.FloatTensor = None):\n",
        "        z_cat = self._encode_categoricals(x_cat)\n",
        "        if (x_cont is not None) and (self.n_cont > 0):\n",
        "            if x_cont.ndim == 1:\n",
        "                x_cont = x_cont.unsqueeze(1)\n",
        "            x_cont = self.cont_bn(x_cont)\n",
        "            x_cont = self.cont_linear(x_cont)\n",
        "            z = torch.cat([z_cat, x_cont], dim=1)\n",
        "        else:\n",
        "            z = z_cat\n",
        "        out = self.head(z)\n",
        "        return out  # (B, 1), sigmoid된 확률\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2) 샘플 mixed-type dataset generator (문자열 포함)\n",
        "# ---------------------------------------------------------\n",
        "def make_mixed_sample(\n",
        "    n_samples=40000,\n",
        "    seed=7\n",
        "):\n",
        "    rng = np.random.RandomState(seed)\n",
        "\n",
        "    genders = np.array([\"남성\", \"여성\", \"기타\"], dtype=object)\n",
        "    cities = np.array([\"서울\", \"부산\", \"대구\", \"인천\", \"수원\", \"고양\"], dtype=object)\n",
        "    devices = np.array([\"ios\", \"android\", \"web\"], dtype=object)\n",
        "\n",
        "    # categorical columns\n",
        "    gender_col = rng.choice(genders, size=n_samples, p=[0.48, 0.48, 0.04])\n",
        "    city_col   = rng.choice(cities, size=n_samples)\n",
        "    device_col = rng.choice(devices, size=n_samples, p=[0.35, 0.55, 0.10])\n",
        "\n",
        "    # continuous columns\n",
        "    n_cont = 10\n",
        "    X_cont = rng.randn(n_samples, n_cont).astype(\"float32\")\n",
        "\n",
        "    # latent score\n",
        "    w_gender = {g: w for g, w in zip(genders, rng.uniform(-0.8, 0.8, size=len(genders)))}\n",
        "    w_city   = {c: w for c, w in zip(cities, rng.uniform(-0.6, 1.0, size=len(cities)))}\n",
        "    w_device = {d: w for d, w in zip(devices, rng.uniform(-0.5, 0.9, size=len(devices)))}\n",
        "    w_cont   = rng.randn(n_cont).astype(\"float32\")\n",
        "\n",
        "    score_cat = (np.vectorize(lambda v: w_gender[v])(gender_col) +\n",
        "                 np.vectorize(lambda v: w_city[v])(city_col) +\n",
        "                 np.vectorize(lambda v: w_device[v])(device_col)).astype(\"float32\")\n",
        "    score_cont = (X_cont * w_cont).sum(axis=1).astype(\"float32\")\n",
        "\n",
        "    bias = 0.1\n",
        "    noise = rng.normal(scale=0.5, size=n_samples).astype(\"float32\")\n",
        "    logit = 0.7 * score_cat + 0.8 * score_cont + bias + noise\n",
        "    prob = 1.0 / (1.0 + np.exp(-logit))\n",
        "    y = (prob > 0.5).astype(\"int64\")\n",
        "\n",
        "    # combine into feature array (object dtype)\n",
        "    X = np.empty((n_samples, 3 + n_cont), dtype=object)\n",
        "    X[:, 0] = gender_col\n",
        "    X[:, 1] = city_col\n",
        "    X[:, 2] = device_col\n",
        "    X[:, 3:] = X_cont\n",
        "\n",
        "    categorical_feature_indices = [0, 1, 2]\n",
        "    return X, y, categorical_feature_indices\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Base Neural Network Model\n",
        "# -----------------------------------------------------------\n",
        "class BaseNNModel(nn.Module, ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self, **kwargs):\n",
        "        super(BaseNNModel, self).__init__()\n",
        "\n",
        "    @abstractmethod\n",
        "    def build_network(self) -> nn.Module:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        pass\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# MLP Model\n",
        "# -----------------------------------------------------------\n",
        "class MLPModel(BaseNNModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dims: list[int],\n",
        "        cat_features: list[int] | None = None,\n",
        "        cat_dims: list[int] | None = None,\n",
        "        emb_dim: int = 8,\n",
        "    ):\n",
        "        super(MLPModel, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.cat_features = cat_features or []\n",
        "        self.cat_dims = cat_dims or []\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.embeddings = None\n",
        "        self.network = self.build_network()\n",
        "\n",
        "    def build_network(self) -> nn.Sequential:\n",
        "        # categorical embedding layer\n",
        "        if len(self.cat_dims) > 0:\n",
        "            self.embeddings = nn.ModuleList([\n",
        "                nn.Embedding(cat_dim, self.emb_dim) for cat_dim in self.cat_dims\n",
        "            ])\n",
        "\n",
        "        combined_input_dim = (\n",
        "            self.input_dim - len(self.cat_features)\n",
        "            + len(self.cat_features) * self.emb_dim\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        dims = [combined_input_dim] + self.hidden_dims\n",
        "\n",
        "        for i in range(len(dims) - 1):\n",
        "            hidden_layer = nn.Linear(dims[i], dims[i + 1])\n",
        "            nn.init.kaiming_normal_(hidden_layer.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "\n",
        "            layers.append(hidden_layer)\n",
        "            layers.append(nn.BatchNorm1d(dims[i + 1]))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        # output layer (logit)\n",
        "        layers.append(nn.Linear(dims[-1], 1))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        continuous_features = []\n",
        "        embedded_features = []\n",
        "        cat_idx = 0\n",
        "\n",
        "        for i in range(self.input_dim):\n",
        "            if i in self.cat_features:\n",
        "                cat_values = x[:, i].long()\n",
        "                embedded = self.embeddings[cat_idx](cat_values)\n",
        "                embedded_features.append(embedded)\n",
        "                cat_idx += 1\n",
        "            else:\n",
        "                continuous_features.append(x[:, i:i+1])\n",
        "\n",
        "        if continuous_features:\n",
        "            continuous_features = torch.cat(continuous_features, dim=1)\n",
        "        else:\n",
        "            continuous_features = None\n",
        "\n",
        "        if embedded_features:\n",
        "            embedded_features = torch.cat(embedded_features, dim=1)\n",
        "        else:\n",
        "            embedded_features = None\n",
        "\n",
        "        if continuous_features is not None and embedded_features is not None:\n",
        "            combined_features = torch.cat([continuous_features, embedded_features], dim=1)\n",
        "        elif embedded_features is not None:\n",
        "            combined_features = embedded_features\n",
        "        elif continuous_features is not None:\n",
        "            combined_features = continuous_features\n",
        "        else:\n",
        "            raise ValueError(\"No features found for forward pass.\")\n",
        "\n",
        "        # logit 출력\n",
        "        return self.network(combined_features)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# TabTransformer Model (DeepLearningBinaryClassifier용 래퍼)\n",
        "# -----------------------------------------------------------\n",
        "class TabTransformerModel(BaseNNModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        cat_features: list[int],\n",
        "        cat_cardinalities: list[int],\n",
        "        d_token: int = 32,\n",
        "        n_heads: int = 4,\n",
        "        n_layers: int = 2,\n",
        "        dim_feedforward: int = 128,\n",
        "        attn_dropout: float = 0.1,\n",
        "        embedding_dropout: float = 0.1,\n",
        "        add_cls: bool = False,\n",
        "        pooling: str = \"concat\",\n",
        "        cont_proj: str = \"linear\",\n",
        "        hidden_dims: tuple[int, ...] = (128, 64),\n",
        "        mlp_dropout: float = 0.2,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        input_dim        : 전체 feature 개수 (cat + cont)\n",
        "        cat_features     : categorical feature의 컬럼 인덱스 리스트\n",
        "        cat_cardinalities: 각 categorical 컬럼의 cardinality (OOV 제외, TabTransformerNet 정의와 동일)\n",
        "        \"\"\"\n",
        "        super(TabTransformerModel, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.cat_features = list(cat_features)\n",
        "        self.cont_features = [i for i in range(input_dim) if i not in self.cat_features]\n",
        "        self.cat_cardinalities = cat_cardinalities\n",
        "\n",
        "        # TabTransformerNet 하이퍼파라미터\n",
        "        self.d_token = d_token\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.dim_feedforward = dim_feedforward\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.embedding_dropout = embedding_dropout\n",
        "        self.add_cls = add_cls\n",
        "        self.pooling = pooling\n",
        "        self.cont_proj = cont_proj\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.mlp_dropout = mlp_dropout\n",
        "\n",
        "        self.network = self.build_network()\n",
        "\n",
        "    def build_network(self) -> nn.Module:\n",
        "        # n_continuous = 전체 - categorical 개수\n",
        "        n_cont = len(self.cont_features)\n",
        "\n",
        "        model = TabTransformerNet(\n",
        "            cat_cardinalities=self.cat_cardinalities,\n",
        "            n_continuous=n_cont,\n",
        "            d_token=self.d_token,\n",
        "            n_heads=self.n_heads,\n",
        "            n_layers=self.n_layers,\n",
        "            dim_feedforward=self.dim_feedforward,\n",
        "            attn_dropout=self.attn_dropout,\n",
        "            embedding_dropout=self.embedding_dropout,\n",
        "            add_cls=self.add_cls,\n",
        "            pooling=self.pooling,\n",
        "            cont_proj=self.cont_proj,\n",
        "            mlp_hidden_dims=self.hidden_dims,\n",
        "            mlp_dropout=self.mlp_dropout,\n",
        "            padding_idx=0,       # OOV=0\n",
        "            norm_first=True,\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, input_dim)\n",
        "          - cat_features 위치의 값들은 이미 integer ID라고 가정\n",
        "          - 나머지는 float형 연속 변수\n",
        "        \"\"\"\n",
        "        # categorical\n",
        "        if len(self.cat_features) > 0:\n",
        "            x_cat = x[:, self.cat_features].long()\n",
        "        else:\n",
        "            x_cat = torch.zeros(x.size(0), 0, dtype=torch.long, device=x.device)\n",
        "\n",
        "        # continuous\n",
        "        if len(self.cont_features) > 0:\n",
        "            x_cont = x[:, self.cont_features].float()\n",
        "        else:\n",
        "            x_cont = None\n",
        "\n",
        "        # TabTransformerNet은 Sigmoid까지 포함된 확률(p) (B,1) 반환\n",
        "        out = self.network(x_cat, x_cont)\n",
        "        return out  # 확률\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Deep Learning Binary Classifier\n",
        "#   - model_type: \"mlp\" / \"tabtransformer\"\n",
        "# -----------------------------------------------------------\n",
        "class DeepLearningBinaryClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_type: str = \"mlp\",\n",
        "        model_params: dict | None = None,\n",
        "    ):\n",
        "        self.model_type = model_type\n",
        "        self.model_params = model_params or {}\n",
        "        self.model = None\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def _build_model(self) -> BaseNNModel:\n",
        "        model_registry = {\n",
        "            \"mlp\": MLPModel,\n",
        "            \"tabtransformer\": TabTransformerModel,\n",
        "        }\n",
        "\n",
        "        if self.model_type not in model_registry:\n",
        "            raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
        "\n",
        "        # lr, loss_fn 은 모델 __init__에 넘기지 않음\n",
        "        valid_params = {\n",
        "            k: v for k, v in self.model_params.items()\n",
        "            if k not in [\"loss_fn\", \"lr\"]\n",
        "        }\n",
        "\n",
        "        model_class = model_registry[self.model_type](**valid_params)\n",
        "        return model_class\n",
        "\n",
        "    def _get_loss_fn(self) -> nn.Module:\n",
        "        loss_name = self.model_params.get(\"loss_fn\", \"logloss\")\n",
        "\n",
        "        if loss_name == \"logloss\":\n",
        "            # TabTransformer는 이미 Sigmoid 된 확률을 출력하므로 BCELoss,\n",
        "            # MLP는 logit 출력이므로 BCEWithLogitsLoss\n",
        "            if self.model_type == \"tabtransformer\":\n",
        "                return nn.BCELoss(reduction=\"none\")\n",
        "            else:\n",
        "                return nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss function: {loss_name}\")\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        X: np.ndarray,\n",
        "        y: np.ndarray,\n",
        "        sample_weight: np.ndarray | None = None,\n",
        "        eval_set: list[tuple[np.ndarray, np.ndarray]] | None = None,\n",
        "        eval_metric: list[str] | None = None,\n",
        "        max_epochs: int = 10,\n",
        "        patience: int | None = None,\n",
        "        batch_size: int = 128,\n",
        "    ) -> None:\n",
        "\n",
        "        lr = self.model_params.get(\"lr\", 0.001)\n",
        "        eval_metric = eval_metric or [\"logloss\"]\n",
        "\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(self.device)\n",
        "\n",
        "        if eval_set is not None:\n",
        "            x_eval_tensor = torch.tensor(eval_set[0][0], dtype=torch.float32).to(self.device)\n",
        "            y_eval_true = eval_set[0][1]\n",
        "        else:\n",
        "            x_eval_tensor = None\n",
        "            y_eval_true = None\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight_tensor = torch.tensor(sample_weight, dtype=torch.float32).view(-1, 1).to(self.device)\n",
        "        else:\n",
        "            sample_weight_tensor = torch.ones_like(y_tensor, dtype=torch.float32)\n",
        "\n",
        "        train_dataset = TensorDataset(X_tensor, y_tensor, sample_weight_tensor)\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        if self.model is None:\n",
        "            self.model = self._build_model().to(self.device)\n",
        "\n",
        "        loss_fn = self._get_loss_fn()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "        patience_counter = 0\n",
        "        best_metric = float(\"inf\")\n",
        "        best_model_weights = None\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            epoch_loss = 0.0\n",
        "\n",
        "            for x_batch, y_batch, weight_batch in train_dataloader:\n",
        "                self.model.train()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                y_pred = self.model(x_batch)\n",
        "\n",
        "                # MLP: y_pred = logits, loss_fn = BCEWithLogitsLoss\n",
        "                # TabTransformer: y_pred = prob,   loss_fn = BCELoss\n",
        "                loss = loss_fn(y_pred, y_batch)\n",
        "                weighted_loss = (loss * weight_batch).sum() / weight_batch.sum()\n",
        "\n",
        "                weighted_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += weighted_loss.item()\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{max_epochs}\")\n",
        "            print(f\"- [train] loss: {epoch_loss / len(train_dataloader):.6f}\")\n",
        "\n",
        "            # evaluation\n",
        "            if eval_set is not None and x_eval_tensor is not None:\n",
        "                with torch.no_grad():\n",
        "                    self.model.eval()\n",
        "                    raw_eval_pred = self.model(x_eval_tensor)\n",
        "\n",
        "                    if self.model_type == \"tabtransformer\":\n",
        "                        y_eval_pred = raw_eval_pred.cpu().numpy().ravel()\n",
        "                    else:\n",
        "                        y_eval_pred = torch.sigmoid(raw_eval_pred).cpu().numpy().ravel()\n",
        "\n",
        "                eval_metrics = {}\n",
        "                for metric in eval_metric:\n",
        "                    if metric == \"logloss\":\n",
        "                        eval_metrics[\"logloss\"] = log_loss(y_eval_true, y_eval_pred)\n",
        "                    elif metric == \"average_precision\":\n",
        "                        eval_metrics[\"average_precision\"] = -average_precision_score(\n",
        "                            y_eval_true, y_eval_pred\n",
        "                        )  # early stopping을 \"minimize\" 기준으로 쓰기 위해 -\n",
        "                    elif metric == \"auc\":\n",
        "                        eval_metrics[\"auc\"] = -roc_auc_score(y_eval_true, y_eval_pred)\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unknown metric: {metric}\")\n",
        "\n",
        "                metrics_str = \", \".join(\n",
        "                    [f\"{k}: {v:.4f}\" for k, v in eval_metrics.items()]\n",
        "                )\n",
        "                print(f\"- [eval] {metrics_str}\")\n",
        "\n",
        "                # early stopping 기준 metric\n",
        "                key_for_es = eval_metric[0] if eval_metric[0] in eval_metrics else \"logloss\"\n",
        "                current_metric = eval_metrics[key_for_es]\n",
        "                print(f\" -- (early_stopping) current_metric: {current_metric:.6f}, best_metric: {best_metric:.6f}\")\n",
        "\n",
        "                if current_metric < best_metric:\n",
        "                    best_metric = current_metric\n",
        "                    patience_counter = 0\n",
        "                    best_model_weights = self.model.state_dict()\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience is not None and patience_counter >= patience:\n",
        "                        print(f\"early stopping at epoch {epoch + 1}\")\n",
        "                        break\n",
        "\n",
        "        if best_model_weights is not None:\n",
        "            self.model.load_state_dict(best_model_weights)\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        self.model = self.model.to(self.device)\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            raw_out = self.model(X_tensor)\n",
        "\n",
        "            if self.model_type == \"tabtransformer\":\n",
        "                # 이미 확률\n",
        "                probs1 = raw_out\n",
        "            else:\n",
        "                # logits -> sigmoid\n",
        "                probs1 = torch.sigmoid(raw_out)\n",
        "\n",
        "            probs = probs1.cpu().numpy()\n",
        "\n",
        "        if probs.shape[1] == 1:\n",
        "            probs = np.hstack((1 - probs, probs))\n",
        "\n",
        "        return probs.astype(\"float\")\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        probs = self.predict_proba(X)\n",
        "        return probs.argmax(axis=1)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# (옵션) 나머지 Rule Classifier들 – 원래 인프라 유지용\n",
        "# -----------------------------------------------------------\n",
        "class WeightedRuleClassifier:\n",
        "    def __init__(self, rule_weights: list[float]):\n",
        "        self.rule_weights = rule_weights\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        probs = np.zeros((X.shape[0], 2))\n",
        "        for i, rule_weight in enumerate(self.rule_weights):\n",
        "            probs[:, 1] += rule_weight * X[:, i]\n",
        "        probs[:, 0] = (np.ones(X.shape[0],) - probs[:, 1])\n",
        "        return probs.astype(\"float\")\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        return self.predict_proba(X).argmax(axis=1)\n",
        "\n",
        "\n",
        "class CustomRuleClassifier:\n",
        "    def __init__(self, rule: str):\n",
        "        self.rule = rule\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 4) TabTransformer + DeepLearningBinaryClassifier 실행 데모\n",
        "# -----------------------------------------------------------\n",
        "def tabtransformer_demo():\n",
        "    np.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # 1) 문자열 섞인 샘플 데이터 생성\n",
        "    X_raw, y, categorical_feature_indices = make_mixed_sample(\n",
        "        n_samples=30000,\n",
        "        seed=123\n",
        "    )\n",
        "\n",
        "    # 2) 전처리: 문자열 카테고리 -> 정수 ID\n",
        "    preproc = TabularPreprocessor(\n",
        "        categorical_indices=categorical_feature_indices,\n",
        "        use_oov=True,\n",
        "        oov_token=0,\n",
        "        add_na_token=True,\n",
        "    )\n",
        "    x_cat, x_cont = preproc.fit_transform(X_raw)\n",
        "\n",
        "    # TabTransformerModel은 X 전체(np.float32)를 받되,\n",
        "    # cat_features 인덱스에 카테고리 ID가 들어있다고 가정\n",
        "    if x_cont is not None:\n",
        "        X_all = np.concatenate([x_cat, x_cont], axis=1)\n",
        "    else:\n",
        "        X_all = x_cat\n",
        "\n",
        "    # categorical 위치: 앞쪽부터 x_cat.shape[1]개\n",
        "    cat_features = list(range(x_cat.shape[1]))\n",
        "    cat_cardinalities = preproc.cardinalities  # 각 cat column의 cardinality (OOV 제외)\n",
        "\n",
        "    # 3) train/val/test split\n",
        "    N = X_all.shape[0]\n",
        "    idx = np.arange(N)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    tr_end = int(N * 0.7)\n",
        "    va_end = int(N * 0.85)\n",
        "    tr_idx, va_idx, te_idx = idx[:tr_end], idx[tr_end:va_end], idx[va_end:]\n",
        "\n",
        "    X_tr, y_tr = X_all[tr_idx], y[tr_idx]\n",
        "    X_va, y_va = X_all[va_idx], y[va_idx]\n",
        "    X_te, y_te = X_all[te_idx], y[te_idx]\n",
        "\n",
        "    # 4) DeepLearningBinaryClassifier + TabTransformer 설정\n",
        "    clf = DeepLearningBinaryClassifier(\n",
        "        model_type=\"tabtransformer\",\n",
        "        model_params={\n",
        "            \"input_dim\": X_all.shape[1],\n",
        "            \"cat_features\": cat_features,\n",
        "            \"cat_cardinalities\": cat_cardinalities,\n",
        "            # TabTransformerNet hyperparams\n",
        "            \"d_token\": 32,\n",
        "            \"n_heads\": 4,\n",
        "            \"n_layers\": 2,\n",
        "            \"dim_feedforward\": 128,\n",
        "            \"attn_dropout\": 0.1,\n",
        "            \"embedding_dropout\": 0.05,\n",
        "            \"pooling\": \"concat\",\n",
        "            \"add_cls\": False,\n",
        "            \"cont_proj\": \"linear\",\n",
        "            \"hidden_dims\": (128, 64),\n",
        "            \"mlp_dropout\": 0.2,\n",
        "            # DL classifier options\n",
        "            \"lr\": 1e-3,\n",
        "            \"loss_fn\": \"logloss\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_tr,\n",
        "        y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric=[\"logloss\"],\n",
        "        max_epochs=10,\n",
        "        patience=2,\n",
        "        batch_size=1024,\n",
        "    )\n",
        "\n",
        "    # 5) 평가\n",
        "    proba_te = clf.predict_proba(X_te)[:, 1]\n",
        "    pred_te = (proba_te >= 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_te, pred_te)\n",
        "    auc = roc_auc_score(y_te, proba_te)\n",
        "    ll = log_loss(y_te, proba_te)\n",
        "\n",
        "    print(\"\\n===== Test Metrics (TabTransformer via DeepLearningBinaryClassifier) =====\")\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"ROC-AUC  : {auc:.4f}\")\n",
        "    print(f\"Logloss  : {ll:.4f}\")\n",
        "\n",
        "    print(\"\\nSample predictions (first 10):\")\n",
        "    print(np.round(proba_te[:10], 4))\n",
        "\n",
        "    print(\"\\n[Info] cat_features indices:\", cat_features)\n",
        "    print(\"[Info] cat cardinalities   :\", cat_cardinalities)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tabtransformer_demo()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UZUUMdxIZtpd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}